{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoZMmaS-RHVA"
   },
   "source": [
    "# Fine-tuning Wav2Vec2 for Amharic Legal ASR with LoRA\n",
    "\n",
    "This notebook fine-tunes `agkphysics/wav2vec2-large-xlsr-53-amharic` for legal domain Amharic speech recognition using LoRA (Low-Rank Adaptation) for efficient training on Colab free tier.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `agkphysics/wav2vec2-large-xlsr-53-amharic`\n",
    "- **Fine-tuning Method**: LoRA (via PEFT library)\n",
    "- **Task**: Automatic Speech Recognition (ASR)\n",
    "- **Domain**: Legal Amharic text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DBs0OtVRHVN"
   },
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oA0TIzXYRHVO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0233ec43-7a25-4f59-b5a5-81c43716b54a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa jiwer soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiAJxozFRHVQ",
    "outputId": "1d7a9bc9-1af4-4baf-9596-a3fec00bc10e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "CUDA memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import jiwer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfSjXrKbSAY3",
    "outputId": "7f7f577c-8523-448c-e4a2-0f6925bd02dc"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtdlWzztRHVS"
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HRLhSmZRHVT",
    "outputId": "c72e267d-0ea5-410f-9bea-b3beb7f97be1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration:\n",
      "  Model: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "  Output directory: wav2vec2_lora_amharic_legal\n",
      "  LoRA rank (r): 8\n",
      "  LoRA alpha: 32\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"agkphysics/wav2vec2-large-xlsr-53-amharic\"\n",
    "\n",
    "AUDIO_DIR = \"/content/drive/MyDrive/Dataset_4.0h/audio\"\n",
    "TRAIN_CSV = \"/content/drive/MyDrive/Dataset_4.0h/train.csv\"\n",
    "VAL_CSV = \"/content/drive/MyDrive/Dataset_4.0h/val.csv\"\n",
    "TEST_CSV = \"/content/drive/MyDrive/Dataset_4.0h/test.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"wav2vec2_lora_amharic_legal\"\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,  # Lowered from 3e-4 for more stable training\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 2000,  # Increased steps for larger dataset\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"wer\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"none\",\n",
    "    \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  LoRA rank (r): {LORA_CONFIG['r']}\")\n",
    "print(f\"  LoRA alpha: {LORA_CONFIG['lora_alpha']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SEkOVMoRHVU"
   },
   "source": [
    "## 3. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk1eD9YSRHVV",
    "outputId": "2f440748-c623-456a-ec6c-c339d877bad0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train samples: 302\n",
      "Validation samples: 37\n",
      "Test samples: 39\n",
      "\n",
      "Total samples: 378\n"
     ]
    }
   ],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX4M4CVLRHVW"
   },
   "source": [
    "## 4. Create Vocabulary and Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qW2cL0xRRHVY",
    "outputId": "a45d2ad8-00ff-41ba-d3a2-daf21abba8de"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 170\n",
      "First 20 characters: [' ', 'ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ']\n"
     ]
    }
   ],
   "source": [
    "# Use the original model's processor instead of creating a new vocabulary\n",
    "# This preserves the pre-trained CTC head weights\n",
    "print(\"Loading original model processor to preserve vocabulary and CTC head...\")\n",
    "original_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Original vocabulary size: {len(original_processor.tokenizer)}\")\n",
    "print(f\"First 20 characters: {list(original_processor.tokenizer.get_vocab().keys())[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hga5SKM4RHVZ",
    "outputId": "1a718f5e-fe27-4a5e-bccd-2e2cc98c06a1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processor created successfully\n"
     ]
    }
   ],
   "source": [
    "# Use the original processor to maintain vocabulary compatibility\n",
    "processor = original_processor\n",
    "print(\"Using original model processor to preserve CTC head weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8XvLtU4RHVa"
   },
   "source": [
    "## 5. Load Model and Apply LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE9J73VPRHVb",
    "outputId": "21410b9c-0ad8-4e1a-80d3-ee5714884c03"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at agkphysics/wav2vec2-large-xlsr-53-amharic and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([234]) in the checkpoint and torch.Size([172]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([234, 1024]) in the checkpoint and torch.Size([172, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Base model loaded: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "Vocabulary size: 172\n",
      "Model parameters: 315.62M\n",
      "Model moved to CUDA\n"
     ]
    }
   ],
   "source": [
    "# Load model WITHOUT reinitializing CTC head - use original vocabulary\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    "    # No vocab_size or ignore_mismatched_sizes - preserves original CTC head\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {len(processor.tokenizer)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Ensure CTC head (lm_head) is trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "print(f\"CTC head (lm_head) parameters: {sum(p.numel() for p in model.lm_head.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"CTC head trainable: {all(p.requires_grad for p in model.lm_head.parameters())}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APDNpsMwRHVb",
    "outputId": "f55d1e21-a612-448d-d3f6-38a049e9744a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 1,572,864 || all params: 317,187,884 || trainable%: 0.4959\n",
      "\n",
      "LoRA adapters applied successfully\n",
      "Model base forward method wrapped to filter input_ids and inputs_embeds\n",
      "Applied comprehensive PEFT compatibility patches for Wav2Vec2\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_CONFIG[\"r\"],\n",
    "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "    bias=LORA_CONFIG[\"bias\"],\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure CTC head remains trainable after LoRA application\n",
    "for param in model.base_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Count trainable parameters including CTC head\n",
    "lm_head_params = sum(p.numel() for p in model.base_model.lm_head.parameters() if p.requires_grad)\n",
    "print(f\"\\nCTC head (lm_head) trainable parameters: {lm_head_params / 1e6:.2f}M\")\n",
    "\n",
    "print(\"\\nLoRA adapters applied successfully\")\n",
    "\n",
    "# Wrap forward method to filter out input_ids and inputs_embeds (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "original_base_forward = model.base_model.forward\n",
    "\n",
    "def filtered_base_forward(*args, **kwargs):\n",
    "    # Remove input_ids and inputs_embeds if present (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "    # Wav2Vec2ForCTC only expects input_values, not input_ids or inputs_embeds\n",
    "    if 'input_ids' in kwargs:\n",
    "        del kwargs['input_ids']\n",
    "    if 'inputs_embeds' in kwargs:\n",
    "        del kwargs['inputs_embeds']\n",
    "    return original_base_forward(*args, **kwargs)\n",
    "\n",
    "model.base_model.forward = filtered_base_forward\n",
    "print(\"Model base forward method wrapped to filter input_ids and inputs_embeds\")\n",
    "\n",
    "# RIGHT AFTER applying LoRA and before creating Trainer\n",
    "# Solution 1: Patch PEFT's save function\n",
    "from peft.utils.save_and_load import get_peft_model_state_dict\n",
    "import peft.utils.save_and_load\n",
    "\n",
    "_original_get_peft_state_dict = get_peft_model_state_dict\n",
    "\n",
    "def safe_get_peft_state_dict(model, state_dict=None, adapter_name=\"default\",\n",
    "                              unwrap_compiled=False, save_embedding_layers=False, **kwargs):\n",
    "    \"\"\"Skip embedding layers for Wav2Vec2 models\"\"\"\n",
    "    if hasattr(model, 'base_model'):\n",
    "        base_class = model.base_model.__class__.__name__\n",
    "        if 'Wav2Vec2' in base_class:\n",
    "            save_embedding_layers = False\n",
    "    elif 'Wav2Vec2' in model.__class__.__name__:\n",
    "        save_embedding_layers = False\n",
    "\n",
    "    return _original_get_peft_state_dict(\n",
    "        model, state_dict=state_dict, adapter_name=adapter_name,\n",
    "        unwrap_compiled=unwrap_compiled,\n",
    "        save_embedding_layers=save_embedding_layers, **kwargs\n",
    "    )\n",
    "\n",
    "peft.utils.save_and_load.get_peft_model_state_dict = safe_get_peft_state_dict\n",
    "\n",
    "# Solution 2: Patch the Wav2Vec2Model class\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2ForCTC\n",
    "\n",
    "Wav2Vec2Model.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2Model.get_output_embeddings = lambda self: None\n",
    "Wav2Vec2Model.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2Model.set_output_embeddings = lambda self, x: None\n",
    "\n",
    "Wav2Vec2ForCTC.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2ForCTC.get_output_embeddings = lambda self: (self.lm_head if hasattr(self, 'lm_head') else None)\n",
    "Wav2Vec2ForCTC.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2ForCTC.set_output_embeddings = lambda self, x: (setattr(self, 'lm_head', x) if hasattr(self, 'lm_head') else None)\n",
    "\n",
    "print(\"Applied comprehensive PEFT compatibility patches for Wav2Vec2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGjPQteGRHVb"
   },
   "source": [
    "## 6. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "42649ca8bd0648db8966485a57f81531",
      "517048ac9b714570b114156b67e7f3ba",
      "ed9c593de4b946859093bcefec492105",
      "6def75c803d5403e8c2410633655d31d",
      "49dd8d55eaa44953aab2cb28ccd114e4",
      "76d5deb9356047dc9a4a8e83ec9b1d1a",
      "2c0bdc188df146c1aa93dc2693e9a94a",
      "e083923a7c854c2d90026366abc3112d",
      "c2b90a4443314f66aca7e8ab8e3e5c1e",
      "d30049acfc7a42c8a52ff4a35d255f82",
      "eb20e5586f8146e5b0f2acb4cc1df38e",
      "c5c90634bce04d66bc6d3c1b5edd3eda",
      "5c4329e8541b49fb84a80c28b6390c7a",
      "4c30ca48d6e2409eb660aa319bec31f7",
      "8992eb0fa5a74dc4b75d4649336ff774",
      "2696ada16df2448baea07f7061442f23",
      "62349c8a80bc407fb675ba8e0880198d",
      "d4fde5f445b94a3a9f953c3d4c824e48",
      "617b78b2ff594f1c8d071955947f5848",
      "6ad7ccdc65bb4561b44d0b6e17c011a2",
      "71daaa4fe75e432e9b2997ea0c324c91",
      "70bbbe6d03ee4944bbba199fff1e97f2",
      "3066992bc6a042b9a9d3774d0a49afe9",
      "5522323fea774fc3aa639de383bbc6a6",
      "3fb8e3ed5d474ef48c40bb5ef1b32c53",
      "c7b6988b67084060ad526aac426b783a",
      "51372394c01a4abc9fef41872b11f556",
      "b4e6b622c0d148908090150d8bec7a15",
      "100cbfeb0f9d4987befae78553e3bad4",
      "d746c0494a3c4247b4de3813e32ded61",
      "6b7d4f3fd3f7479aacb62aaf7cea690c",
      "64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "49c1b3c35cd7405ebfc838a237a75a58"
     ]
    },
    "id": "49LM_ofyRHVb",
    "outputId": "9c5aedca-a34b-430f-bc1a-310a3223c622"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/302 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42649ca8bd0648db8966485a57f81531"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5c90634bce04d66bc6d3c1b5edd3eda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3066992bc6a042b9a9d3774d0a49afe9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets prepared:\n",
      "  Train: 302 samples\n",
      "  Validation: 37 samples\n",
      "  Test: 39 samples\n"
     ]
    }
   ],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    \"\"\"Load audio file and resample to 16kHz\"\"\"\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=16000)\n",
    "    return speech_array\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Process a batch of audio and transcriptions\"\"\"\n",
    "    audio = [speech_file_to_array_fn(path) for path in batch[\"audio_path\"]]\n",
    "\n",
    "    # Process audio - return as lists (no return_tensors)\n",
    "    audio_features = processor.feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000\n",
    "        # No padding, no return_tensors here - collator handles it\n",
    "    )\n",
    "    # Only store input_values, not attention_mask\n",
    "    batch[\"input_values\"] = audio_features.input_values  # This will be a list of arrays\n",
    "\n",
    "    # Process text using tokenizer directly - return as lists\n",
    "    batch[\"labels\"] = [tokenizer(transcription, add_special_tokens=False)[\"input_ids\"] for transcription in batch[\"transcription\"]]\n",
    "\n",
    "    # Make sure we're not accidentally storing attention_mask\n",
    "    if \"attention_mask\" in batch:\n",
    "        del batch[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePMnBg7URHVc"
   },
   "source": [
    "## 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do_OeZxlRHVc",
    "outputId": "772f883b-207b-4a60-c030-9017da9d9696"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data collator created\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        import torch\n",
    "\n",
    "        # Extract input_values and labels - explicitly ignore attention_mask if present\n",
    "        input_values_list = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # Convert audio to tensors and pad manually\n",
    "        input_values_tensors = []\n",
    "        for iv in input_values_list:\n",
    "            if isinstance(iv, torch.Tensor):\n",
    "                input_values_tensors.append(iv)\n",
    "            elif isinstance(iv, np.ndarray):\n",
    "                input_values_tensors.append(torch.tensor(iv, dtype=torch.float32))\n",
    "            else:\n",
    "                input_values_tensors.append(torch.tensor(np.array(iv), dtype=torch.float32))\n",
    "\n",
    "        # Pad audio sequences\n",
    "        input_values = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_values_tensors,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0\n",
    "        )\n",
    "\n",
    "        # Pad labels manually\n",
    "        max_label_len = max(len(labels) for labels in label_features)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        padded_labels = []\n",
    "        for labels in label_features:\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                labels = labels.tolist()\n",
    "\n",
    "            padding_length = max_label_len - len(labels)\n",
    "            padded = labels + [pad_token_id] * padding_length\n",
    "            padded_labels.append(padded)\n",
    "\n",
    "        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        # Mask padded labels with -100\n",
    "        attention_mask_labels = (labels_tensor != pad_token_id).long()\n",
    "        labels_tensor = labels_tensor.masked_fill(attention_mask_labels.ne(1), -100)\n",
    "\n",
    "        # Return ONLY input_values and labels - explicitly create a new dict\n",
    "        batch = {}\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = labels_tensor\n",
    "\n",
    "        # Explicitly ensure no other keys are present\n",
    "        return batch\n",
    "# Add these lines:\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "print(\"Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8Lx1gm2RHVd"
   },
   "source": [
    "## 8. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C05gDw0RRHVe",
    "outputId": "853faab0-693b-4813-cf20-67df5acc3ab2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation metrics function created\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER (Word Error Rate) and CER (Character Error Rate)\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9h39rbJRHVe"
   },
   "source": [
    "## 9. Training Arguments and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIsrmgrVRHVf",
    "outputId": "c75f944c-4e41-4992-aead-a528012643ca"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainer initialized\n",
      "\n",
      "Training configuration:\n",
      "  Max steps: 1000\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0003\n",
      "  Evaluation steps: 500\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(**TRAINING_ARGS)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Evaluation steps: {training_args.eval_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o4oggy-RHVf"
   },
   "source": [
    "## 10. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "-EAFlSv6RHVg",
    "outputId": "a7cd9a1e-dc9d-4088-ac3d-0e51d800df65"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 2:07:43, Epoch 52/53]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.693900</td>\n",
       "      <td>5.121708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.792800</td>\n",
       "      <td>4.608199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.924718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 10.1412\n",
      "Final model saved to: wav2vec2_lora_amharic_legal_final\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQUJNm5eRHVh"
   },
   "source": [
    "## 11. Zip and Copy Model to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qAwyb1zARHVi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "faae84ff-5eea-4886-ced9-afee305bad9e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating zip file: wav2vec2_lora_amharic_legal_final.zip...\n",
      "  Added: added_tokens.json\n",
      "  Added: preprocessor_config.json\n",
      "  Added: special_tokens_map.json\n",
      "  Added: tokenizer_config.json\n",
      "  Added: adapter_config.json\n",
      "  Added: training_args.bin\n",
      "  Added: adapter_model.safetensors\n",
      "  Added: README.md\n",
      "  Added: vocab.json\n",
      "\n",
      "Zip file created: wav2vec2_lora_amharic_legal_final.zip\n",
      "Model zip file copied to Google Drive: /content/drive/MyDrive/wav2vec2_lora_amharic_legal_final.zip\n",
      "\n",
      "File size: 6.21 MB\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory\n",
    "zip_filename = f\"{final_model_path}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            # Get relative path for archive\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"  Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "drive_dest = f\"/content/drive/MyDrive/{zip_filename}\"\n",
    "shutil.copy2(zip_filename, drive_dest)\n",
    "\n",
    "print(f\"Model zip file copied to Google Drive: {drive_dest}\")\n",
    "print(f\"\\nFile size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrA-9_3CRHVj"
   },
   "source": [
    "## 12. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pZAuL4OgRHVk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4bf62d8b-38ff-4d74-f942-87dc288c582b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final model saved to: wav2vec2_lora_amharic_legal_final\n",
      "\n",
      "Files saved:\n",
      "  - LoRA adapters (adapter_model.bin, adapter_config.json)\n",
      "  - Processor (tokenizer, feature_extractor)\n",
      "  - Training configuration\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - LoRA adapters (adapter_model.bin, adapter_config.json)\")\n",
    "print(\"  - Processor (tokenizer, feature_extractor)\")\n",
    "print(\"  - Training configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbojTaxzRHVl"
   },
   "source": [
    "## 13. Inference Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WNTgA1skRHVl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c0eb1b89-eabf-4cb3-ddb7-8b56066741c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference function created\n",
      "\n",
      "Example usage:\n",
      "  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\n",
      "  print(transcription)\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(model, processor, audio_path):\n",
    "    \"\"\"Transcribe a single audio file\"\"\"\n",
    "    speech, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "print(\"Inference function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\")\n",
    "print(\"  print(transcription)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB2IActjRHVl"
   },
   "source": [
    "## 14. Load Model for Inference (After Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wuLQLT2URHVn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3cd4b42c-748a-4dd7-8567-42c72852f5da"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loading function created\n",
      "\n",
      "Example usage:\n",
      "  model, processor = load_trained_model(\n",
      "      MODEL_NAME,\n",
      "      'wav2vec2_lora_amharic_legal_final',\n",
      "      'wav2vec2_lora_amharic_legal_final'\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_trained_model(base_model_name, adapter_path, processor_path):\n",
    "    \"\"\"Load base model and LoRA adapters\"\"\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "\n",
    "    base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        base_model_name,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "print(\"Model loading function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  model, processor = load_trained_model(\")\n",
    "print(\"      MODEL_NAME,\")\n",
    "print(f\"      '{OUTPUT_DIR}_final',\")\n",
    "print(f\"      '{OUTPUT_DIR}_final'\")\n",
    "print(\"  )\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "42649ca8bd0648db8966485a57f81531": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_517048ac9b714570b114156b67e7f3ba",
       "IPY_MODEL_ed9c593de4b946859093bcefec492105",
       "IPY_MODEL_6def75c803d5403e8c2410633655d31d"
      ],
      "layout": "IPY_MODEL_49dd8d55eaa44953aab2cb28ccd114e4"
     }
    },
    "517048ac9b714570b114156b67e7f3ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76d5deb9356047dc9a4a8e83ec9b1d1a",
      "placeholder": "​",
      "style": "IPY_MODEL_2c0bdc188df146c1aa93dc2693e9a94a",
      "value": "Map: 100%"
     }
    },
    "ed9c593de4b946859093bcefec492105": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e083923a7c854c2d90026366abc3112d",
      "max": 302,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b90a4443314f66aca7e8ab8e3e5c1e",
      "value": 302
     }
    },
    "6def75c803d5403e8c2410633655d31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d30049acfc7a42c8a52ff4a35d255f82",
      "placeholder": "​",
      "style": "IPY_MODEL_eb20e5586f8146e5b0f2acb4cc1df38e",
      "value": " 302/302 [00:10&lt;00:00, 34.12 examples/s]"
     }
    },
    "49dd8d55eaa44953aab2cb28ccd114e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76d5deb9356047dc9a4a8e83ec9b1d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0bdc188df146c1aa93dc2693e9a94a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e083923a7c854c2d90026366abc3112d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b90a4443314f66aca7e8ab8e3e5c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d30049acfc7a42c8a52ff4a35d255f82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb20e5586f8146e5b0f2acb4cc1df38e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5c90634bce04d66bc6d3c1b5edd3eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c4329e8541b49fb84a80c28b6390c7a",
       "IPY_MODEL_4c30ca48d6e2409eb660aa319bec31f7",
       "IPY_MODEL_8992eb0fa5a74dc4b75d4649336ff774"
      ],
      "layout": "IPY_MODEL_2696ada16df2448baea07f7061442f23"
     }
    },
    "5c4329e8541b49fb84a80c28b6390c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62349c8a80bc407fb675ba8e0880198d",
      "placeholder": "​",
      "style": "IPY_MODEL_d4fde5f445b94a3a9f953c3d4c824e48",
      "value": "Map: 100%"
     }
    },
    "4c30ca48d6e2409eb660aa319bec31f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617b78b2ff594f1c8d071955947f5848",
      "max": 37,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ad7ccdc65bb4561b44d0b6e17c011a2",
      "value": 37
     }
    },
    "8992eb0fa5a74dc4b75d4649336ff774": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71daaa4fe75e432e9b2997ea0c324c91",
      "placeholder": "​",
      "style": "IPY_MODEL_70bbbe6d03ee4944bbba199fff1e97f2",
      "value": " 37/37 [00:00&lt;00:00, 54.77 examples/s]"
     }
    },
    "2696ada16df2448baea07f7061442f23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62349c8a80bc407fb675ba8e0880198d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4fde5f445b94a3a9f953c3d4c824e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "617b78b2ff594f1c8d071955947f5848": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ad7ccdc65bb4561b44d0b6e17c011a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71daaa4fe75e432e9b2997ea0c324c91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70bbbe6d03ee4944bbba199fff1e97f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3066992bc6a042b9a9d3774d0a49afe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5522323fea774fc3aa639de383bbc6a6",
       "IPY_MODEL_3fb8e3ed5d474ef48c40bb5ef1b32c53",
       "IPY_MODEL_c7b6988b67084060ad526aac426b783a"
      ],
      "layout": "IPY_MODEL_51372394c01a4abc9fef41872b11f556"
     }
    },
    "5522323fea774fc3aa639de383bbc6a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e6b622c0d148908090150d8bec7a15",
      "placeholder": "​",
      "style": "IPY_MODEL_100cbfeb0f9d4987befae78553e3bad4",
      "value": "Map: 100%"
     }
    },
    "3fb8e3ed5d474ef48c40bb5ef1b32c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d746c0494a3c4247b4de3813e32ded61",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b7d4f3fd3f7479aacb62aaf7cea690c",
      "value": 39
     }
    },
    "c7b6988b67084060ad526aac426b783a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "placeholder": "​",
      "style": "IPY_MODEL_49c1b3c35cd7405ebfc838a237a75a58",
      "value": " 39/39 [00:00&lt;00:00, 41.89 examples/s]"
     }
    },
    "51372394c01a4abc9fef41872b11f556": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e6b622c0d148908090150d8bec7a15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "100cbfeb0f9d4987befae78553e3bad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d746c0494a3c4247b4de3813e32ded61": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b7d4f3fd3f7479aacb62aaf7cea690c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64a07e9bc4b64bedb4b3d9b78aedc0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49c1b3c35cd7405ebfc838a237a75a58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}