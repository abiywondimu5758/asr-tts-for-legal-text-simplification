{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoZMmaS-RHVA"
   },
   "source": [
    "# Ablation V0: CTC-Only Baseline (No LoRA)\n",
    "\n",
    "This notebook is part of an ablation study to compare different fine-tuning approaches.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `agkphysics/wav2vec2-large-xlsr-53-amharic`\n",
    "- **Fine-tuning Method**: CTC head only (no LoRA)\n",
    "- **Task**: Automatic Speech Recognition (ASR)\n",
    "- **Domain**: Legal Amharic text\n",
    "- **Dataset**: Dataset_4.0h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DBs0OtVRHVN"
   },
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU usage to avoid DataParallel issues on Kaggle\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA0TIzXYRHVO",
    "outputId": "0233ec43-7a25-4f59-b5a5-81c43716b54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa jiwer soundfile matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiAJxozFRHVQ",
    "outputId": "1d7a9bc9-1af4-4baf-9596-a3fec00bc10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import jiwer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "seed_train_wav2vec2_ctc_only"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42  # Change to 123, 456 for different seeds\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfSjXrKbSAY3",
    "outputId": "7f7f577c-8523-448c-e4a2-0f6925bd02dc"
   },
   "outputs": [],
   "source": [
    "# Kaggle: Dataset is uploaded as input dataset\n",
    "# No need to mount Google Drive\n",
    "print(\"Running on Kaggle - using input dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtdlWzztRHVS"
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HRLhSmZRHVT",
    "outputId": "c72e267d-0ea5-410f-9bea-b3beb7f97be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Running locally - using path: /Users/blank/Documents/Audio/Dataset_4.0h\n",
      "\n",
      "Dataset paths:\n",
      "  Audio directory: /Users/blank/Documents/Audio/Dataset_4.0h/audio\n",
      "  Train CSV: /Users/blank/Documents/Audio/Dataset_4.0h/train.csv\n",
      "  Val CSV: /Users/blank/Documents/Audio/Dataset_4.0h/val.csv\n",
      "  Test CSV: /Users/blank/Documents/Audio/Dataset_4.0h/test.csv\n",
      "‚ö†Ô∏è  TEST MODE ENABLED - Running quick validation (< 1 minute)\n",
      "  üì± Detected Apple Silicon (MPS)\n",
      "  ‚ö†Ô∏è  Using CPU for test mode to avoid MPS memory issues\n",
      "  üí° Note: Full training on Kaggle will use CUDA with more memory\n",
      "Configuration:\n",
      "  Model: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "  Output directory: wav2vec2_ctc_only_baseline\n",
      "  LoRA rank (r): 8\n",
      "  LoRA alpha: 32\n",
      "\n",
      "============================================================\n",
      "  ‚ö†Ô∏è  TEST MODE: Quick validation run (< 1 minute)\n",
      "  ‚ö†Ô∏è  Set TEST_MODE = False for full training!\n",
      "  Max steps: 2\n",
      "  Batch size: 2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"agkphysics/wav2vec2-large-xlsr-53-amharic\"\n",
    "\n",
    "# Dataset paths - Kaggle Input\n",
    "# Assume Dataset_4.0h is uploaded to Kaggle as input dataset\n",
    "# Update the path below to match your actual Kaggle input dataset name\n",
    "# Kaggle input path format: /kaggle/input/your-dataset-name\n",
    "# If your dataset is named \"dataset-40h\" in Kaggle, use: \"/kaggle/input/dataset-40h\"\n",
    "# If your dataset is named \"Dataset_4.0h\", use: \"/kaggle/input/Dataset_4.0h\" (or whatever Kaggle shows)\n",
    "BASE_DATASET_DIR = \"/kaggle/input/dataset-4-0h/Dataset_4.0h\"  # UPDATE THIS to match your Kaggle dataset name\n",
    "# The dataset should contain: audio/, train.csv, val.csv, test.csv\n",
    "AUDIO_DIR = f\"{BASE_DATASET_DIR}/audio\"\n",
    "TRAIN_CSV = f\"{BASE_DATASET_DIR}/train.csv\"\n",
    "VAL_CSV = f\"{BASE_DATASET_DIR}/val.csv\"\n",
    "TEST_CSV = f\"{BASE_DATASET_DIR}/test.csv\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Dataset paths (Kaggle):\")\n",
    "print(f\"  Audio directory: {AUDIO_DIR}\")\n",
    "print(f\"  Train CSV: {TRAIN_CSV}\")\n",
    "print(f\"  Val CSV: {VAL_CSV}\")\n",
    "print(f\"  Test CSV: {TEST_CSV}\")\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(AUDIO_DIR):\n",
    "    print(f\"\\nWARNING: Audio directory not found: {AUDIO_DIR}\")\n",
    "    print(\"  Please ensure dataset is uploaded to Kaggle as input dataset\")\n",
    "if not os.path.exists(TRAIN_CSV):\n",
    "    print(f\"WARNING: Train CSV not found: {TRAIN_CSV}\")\n",
    "if not os.path.exists(VAL_CSV):\n",
    "    print(f\"WARNING: Val CSV not found: {VAL_CSV}\")\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    print(f\"WARNING: Test CSV not found: {TEST_CSV}\")\n",
    "\n",
    "# Output directory - Kaggle working directory (persistent)\n",
    "OUTPUT_DIR = \"/kaggle/working/wav2vec2_ctc_only_baseline\"\n",
    "\n",
    "# Training configuration for Kaggle GPU\n",
    "# Parameters match other ablation study notebooks\n",
    "# Checkpoints will be saved every 500 steps to /kaggle/working/ (persistent)\n",
    "TRAINING_ARGS = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 2000,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"wer\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"none\",\n",
    "    \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Max steps: {TRAINING_ARGS['max_steps']}\")\n",
    "print(f\"  Batch size: {TRAINING_ARGS['per_device_train_batch_size']}\")\n",
    "print(f\"  Gradient accumulation: {TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective batch size: {TRAINING_ARGS['per_device_train_batch_size'] * TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
    "print(f\"  FP16: {TRAINING_ARGS['fp16']}\")\n",
    "print(f\"  Gradient checkpointing: {TRAINING_ARGS['gradient_checkpointing']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SEkOVMoRHVU"
   },
   "source": [
    "## 3. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk1eD9YSRHVV",
    "outputId": "2f440748-c623-456a-ec6c-c339d877bad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 824\n",
      "Validation samples: 103\n",
      "Test samples: 103\n",
      "\n",
      "Total samples: 1030\n"
     ]
    }
   ],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX4M4CVLRHVW"
   },
   "source": [
    "## 4. Create Vocabulary and Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qW2cL0xRRHVY",
    "outputId": "a45d2ad8-00ff-41ba-d3a2-daf21abba8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model processor to preserve vocabulary and CTC head...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 234\n",
      "First 20 characters: [' ', '</s>', '<pad>', '<s>', '<unk>', '·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ', '·àà', '·àâ', '·àä', '·àã', '·àå', '·àç', '·àé', '·àè']\n"
     ]
    }
   ],
   "source": [
    "# Use the original model's processor instead of creating a new vocabulary\n",
    "# This preserves the pre-trained CTC head weights\n",
    "print(\"Loading original model processor to preserve vocabulary and CTC head...\")\n",
    "original_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Original vocabulary size: {len(original_processor.tokenizer)}\")\n",
    "print(f\"First 20 characters: {list(original_processor.tokenizer.get_vocab().keys())[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hga5SKM4RHVZ",
    "outputId": "1a718f5e-fe27-4a5e-bccd-2e2cc98c06a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using original model processor to preserve CTC head weights\n"
     ]
    }
   ],
   "source": [
    "# Use the original processor to maintain vocabulary compatibility\n",
    "processor = original_processor\n",
    "print(\"Using original model processor to preserve CTC head weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  TEST MODE: Using CPU to avoid MPS memory issues\n",
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device selection for Kaggle\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU (CUDA not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8XvLtU4RHVa"
   },
   "source": [
    "## 5. Load Model and Apply LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE9J73VPRHVb",
    "outputId": "21410b9c-0ad8-4e1a-80d3-ee5714884c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "Vocabulary size: 234\n",
      "Model parameters: 315.68M\n",
      "CTC head (lm_head) parameters: 0.24M\n",
      "CTC head trainable: True\n"
     ]
    }
   ],
   "source": [
    "# Load model WITHOUT reinitializing CTC head - use original vocabulary\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    "    # No vocab_size or ignore_mismatched_sizes - preserves original CTC head\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {len(processor.tokenizer)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Ensure CTC head (lm_head) is trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "print(f\"CTC head (lm_head) parameters: {sum(p.numel() for p in model.lm_head.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"CTC head trainable: {all(p.requires_grad for p in model.lm_head.parameters())}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APDNpsMwRHVb",
    "outputId": "f55d1e21-a612-448d-d3f6-38a049e9744a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC-only baseline: Training only the CTC head (no LoRA)\n",
      "CTC head (lm_head) parameters: 0.24M\n",
      "CTC head trainable: True\n",
      "\n",
      "Model ready for training (CTC head only)\n"
     ]
    }
   ],
   "source": [
    "# CTC-ONLY BASELINE: No LoRA adapters, only CTC head is trainable\n",
    "# This baseline tests if fine-tuning just the CTC head is sufficient\n",
    "\n",
    "print(\"CTC-only baseline: Training only the CTC head (no LoRA)\")\n",
    "print(f\"CTC head (lm_head) parameters: {sum(p.numel() for p in model.lm_head.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"CTC head trainable: {all(p.requires_grad for p in model.lm_head.parameters())}\")\n",
    "\n",
    "# Skip LoRA application - model is ready for training\n",
    "print(\"\\nModel ready for training (CTC head only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGjPQteGRHVb"
   },
   "source": [
    "## 6. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "42649ca8bd0648db8966485a57f81531",
      "517048ac9b714570b114156b67e7f3ba",
      "ed9c593de4b946859093bcefec492105",
      "6def75c803d5403e8c2410633655d31d",
      "49dd8d55eaa44953aab2cb28ccd114e4",
      "76d5deb9356047dc9a4a8e83ec9b1d1a",
      "2c0bdc188df146c1aa93dc2693e9a94a",
      "e083923a7c854c2d90026366abc3112d",
      "c2b90a4443314f66aca7e8ab8e3e5c1e",
      "d30049acfc7a42c8a52ff4a35d255f82",
      "eb20e5586f8146e5b0f2acb4cc1df38e",
      "c5c90634bce04d66bc6d3c1b5edd3eda",
      "5c4329e8541b49fb84a80c28b6390c7a",
      "4c30ca48d6e2409eb660aa319bec31f7",
      "8992eb0fa5a74dc4b75d4649336ff774",
      "2696ada16df2448baea07f7061442f23",
      "62349c8a80bc407fb675ba8e0880198d",
      "d4fde5f445b94a3a9f953c3d4c824e48",
      "617b78b2ff594f1c8d071955947f5848",
      "6ad7ccdc65bb4561b44d0b6e17c011a2",
      "71daaa4fe75e432e9b2997ea0c324c91",
      "70bbbe6d03ee4944bbba199fff1e97f2",
      "3066992bc6a042b9a9d3774d0a49afe9",
      "5522323fea774fc3aa639de383bbc6a6",
      "3fb8e3ed5d474ef48c40bb5ef1b32c53",
      "c7b6988b67084060ad526aac426b783a",
      "51372394c01a4abc9fef41872b11f556",
      "b4e6b622c0d148908090150d8bec7a15",
      "100cbfeb0f9d4987befae78553e3bad4",
      "d746c0494a3c4247b4de3813e32ded61",
      "6b7d4f3fd3f7479aacb62aaf7cea690c",
      "64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "49c1b3c35cd7405ebfc838a237a75a58"
     ]
    },
    "id": "49LM_ofyRHVb",
    "outputId": "9c5aedca-a34b-430f-bc1a-310a3223c622"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/824 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 824/824 [00:03<00:00, 225.06 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 275.49 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 306.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared:\n",
      "  Train: 824 samples\n",
      "  Validation: 103 samples\n",
      "  Test: 103 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    \"\"\"Load audio file and resample to 16kHz\"\"\"\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=16000)\n",
    "    return speech_array\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Process a batch of audio and transcriptions\"\"\"\n",
    "    audio = [speech_file_to_array_fn(path) for path in batch[\"audio_path\"]]\n",
    "\n",
    "    # Process audio - return as lists (no return_tensors)\n",
    "    audio_features = processor.feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000\n",
    "        # No padding, no return_tensors here - collator handles it\n",
    "    )\n",
    "    # Only store input_values, not attention_mask\n",
    "    batch[\"input_values\"] = audio_features.input_values  # This will be a list of arrays\n",
    "\n",
    "    # Process text using tokenizer directly - return as lists\n",
    "    batch[\"labels\"] = [processor.tokenizer(transcription, add_special_tokens=False)[\"input_ids\"] for transcription in batch[\"transcription\"]]\n",
    "\n",
    "    # Make sure we're not accidentally storing attention_mask\n",
    "    if \"attention_mask\" in batch:\n",
    "        del batch[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Moving model to CPU for test mode (MPS memory constraints)\n",
      "‚úÖ Model moved to CPU\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA\")\n",
    "else:\n",
    "    model = model.to(\"cpu\")\n",
    "    print(\"Model moved to CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePMnBg7URHVc"
   },
   "source": [
    "## 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  TEST MODE ENABLED - Quick Validation Run\n",
      "======================================================================\n",
      "This will run only 2 training steps to verify setup.\n",
      "Expected runtime: < 1 minute\n",
      "\n",
      "To run FULL training:\n",
      "  1. Go to Configuration cell (Cell 7)\n",
      "  2. Change: TEST_MODE = False\n",
      "  3. Re-run from Configuration cell onwards\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-training summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Max steps: {TRAINING_ARGS['max_steps']}\")\n",
    "print(f\"Batch size: {TRAINING_ARGS['per_device_train_batch_size']}\")\n",
    "print(f\"Gradient accumulation: {TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective batch size: {TRAINING_ARGS['per_device_train_batch_size'] * TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
    "print(f\"Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
    "print(f\"FP16: {TRAINING_ARGS['fp16']}\")\n",
    "print(f\"Gradient checkpointing: {TRAINING_ARGS['gradient_checkpointing']}\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do_OeZxlRHVc",
    "outputId": "772f883b-207b-4a60-c030-9017da9d9696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator created\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        import torch\n",
    "\n",
    "        # Extract input_values and labels - explicitly ignore attention_mask if present\n",
    "        input_values_list = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # Convert audio to tensors and pad manually\n",
    "        input_values_tensors = []\n",
    "        for iv in input_values_list:\n",
    "            if isinstance(iv, torch.Tensor):\n",
    "                input_values_tensors.append(iv)\n",
    "            elif isinstance(iv, np.ndarray):\n",
    "                input_values_tensors.append(torch.tensor(iv, dtype=torch.float32))\n",
    "            else:\n",
    "                input_values_tensors.append(torch.tensor(np.array(iv), dtype=torch.float32))\n",
    "\n",
    "        # Pad audio sequences\n",
    "        input_values = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_values_tensors,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0\n",
    "        )\n",
    "\n",
    "        # Pad labels manually\n",
    "        max_label_len = max(len(labels) for labels in label_features)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        padded_labels = []\n",
    "        for labels in label_features:\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                labels = labels.tolist()\n",
    "\n",
    "            padding_length = max_label_len - len(labels)\n",
    "            padded = labels + [pad_token_id] * padding_length\n",
    "            padded_labels.append(padded)\n",
    "\n",
    "        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        # Mask padded labels with -100\n",
    "        attention_mask_labels = (labels_tensor != pad_token_id).long()\n",
    "        labels_tensor = labels_tensor.masked_fill(attention_mask_labels.ne(1), -100)\n",
    "\n",
    "        # Return ONLY input_values and labels - explicitly create a new dict\n",
    "        batch = {}\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = labels_tensor\n",
    "\n",
    "        # Explicitly ensure no other keys are present\n",
    "        return batch\n",
    "# Add these lines:\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "print(\"Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ TEST MODE VALIDATION COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "All components verified:\n",
      "  ‚úì Model loading\n",
      "  ‚úì Data processing\n",
      "  ‚úì Training loop\n",
      "  ‚úì Evaluation metrics\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: This was a QUICK TEST (2 steps only)\n",
      "   For full training on Kaggle:\n",
      "   1. Set TEST_MODE = False in Configuration cell\n",
      "   2. Re-run the notebook\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Post-training summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training completed\")\n",
    "print(\"=\"*70)\n",
    "if 'train_result' in locals():\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "if 'final_model_path' in locals():\n",
    "    print(f\"Model saved to: {final_model_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8Lx1gm2RHVd"
   },
   "source": [
    "## 8. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C05gDw0RRHVe",
    "outputId": "853faab0-693b-4813-cf20-67df5acc3ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics function created\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER (Word Error Rate) and CER (Character Error Rate)\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9h39rbJRHVe"
   },
   "source": [
    "## 9. Training Arguments and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIsrmgrVRHVf",
    "outputId": "c75f944c-4e41-4992-aead-a528012643ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n",
      "\n",
      "Training configuration:\n",
      "  Max steps: 2\n",
      "  Batch size: 2\n",
      "  Gradient accumulation: 1\n",
      "  Effective batch size: 2\n",
      "  Learning rate: 0.0001\n",
      "  Evaluation steps: 2\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(**TRAINING_ARGS)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Evaluation steps: {training_args.eval_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o4oggy-RHVf"
   },
   "source": [
    "## 10. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for Kaggle (persistent)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory created: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoints will be saved every {TRAINING_ARGS['save_steps']} steps\")\n",
    "print(f\"All outputs will be saved to Kaggle working directory (persistent)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is not wrapped in DataParallel and is on single device\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    print(\"Unwrapping model from DataParallel...\")\n",
    "    model = model.module\n",
    "\n",
    "# Move to single device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model on device: {device}\")\n",
    "print(f\"Visible GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "-EAFlSv6RHVg",
    "outputId": "a7cd9a1e-dc9d-4088-ac3d-0e51d800df65"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_final\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1893\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1890\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mflags(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1893\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflattened_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_zero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1904\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[_HIDDEN_STATES_START_POSITION:]\n",
      "File \u001b[0;32m~/Documents/Audio/venv/lib/python3.10/site-packages/torch/nn/functional.py:3071\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3061\u001b[0m         ctc_loss,\n\u001b[1;32m   3062\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity,\n\u001b[1;32m   3070\u001b[0m     )\n\u001b[0;32m-> 3071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_ctc_loss' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz_markdown"
   },
   "source": [
    "## Training Visualizations\n",
    "\n",
    "Visualize training progress, validation metrics, and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Extract training history from trainer state\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in train_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in train_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract data\n",
    "train_steps = [log['step'] for log in train_logs]\n",
    "train_losses = [log['loss'] for log in train_logs]\n",
    "\n",
    "eval_steps = [log['step'] for log in eval_logs]\n",
    "eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "eval_wers = [log.get('eval_wer', 0) for log in eval_logs]\n",
    "eval_cers = [log.get('eval_cer', 0) for log in eval_logs]\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(train_steps, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.legend(fontsize=12, loc='best')\n",
    "ax1.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if eval_losses:\n",
    "    ax2.plot(eval_steps, eval_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Word Error Rate (WER)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    ax3.plot(eval_steps, eval_wers, 'g-', linewidth=2.5, marker='s', markersize=5, label='WER', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Word Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.set_ylim(bottom=0)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No WER data', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Character Error Rate (CER)\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    ax4.plot(eval_steps, eval_cers, 'm-', linewidth=2.5, marker='^', markersize=5, label='CER', alpha=0.8)\n",
    "    ax4.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Character Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.set_ylim(bottom=0)\n",
    "    ax4.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No CER data', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Combined Loss Plot (Training vs Validation)\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "if eval_losses:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    ax5.plot(eval_steps, eval_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING SUMMARY: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "if train_losses:\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\"  Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(train_losses):.4f} (at step {train_steps[train_losses.index(min(train_losses))]})\")\n",
    "    print(f\"  Improvement: {((train_losses[0] - min(train_losses)) / train_losses[0] * 100):.2f}%\")\n",
    "if eval_losses:\n",
    "    print(f\"\\nValidation Loss:\")\n",
    "    print(f\"  Initial: {eval_losses[0]:.4f}\")\n",
    "    print(f\"  Final: {eval_losses[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(eval_losses):.4f} (at step {eval_steps[eval_losses.index(min(eval_losses))]})\")\n",
    "    print(f\"  Improvement: {((eval_losses[0] - min(eval_losses)) / eval_losses[0] * 100):.2f}%\")\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    valid_wers = [w for w in eval_wers if w > 0]\n",
    "    valid_steps = [eval_steps[i] for i, w in enumerate(eval_wers) if w > 0]\n",
    "    print(f\"\\nWord Error Rate (WER):\")\n",
    "    print(f\"  Initial: {valid_wers[0]:.4f}\")\n",
    "    print(f\"  Final: {valid_wers[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(valid_wers):.4f} (at step {valid_steps[valid_wers.index(min(valid_wers))]})\")\n",
    "    print(f\"  Improvement: {((valid_wers[0] - min(valid_wers)) / valid_wers[0] * 100):.2f}%\")\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    valid_cers = [c for c in eval_cers if c > 0]\n",
    "    valid_steps = [eval_steps[i] for i, c in enumerate(eval_cers) if c > 0]\n",
    "    print(f\"\\nCharacter Error Rate (CER):\")\n",
    "    print(f\"  Initial: {valid_cers[0]:.4f}\")\n",
    "    print(f\"  Final: {valid_cers[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(valid_cers):.4f} (at step {valid_steps[valid_cers.index(min(valid_cers))]})\")\n",
    "    print(f\"  Improvement: {((valid_cers[0] - min(valid_cers)) / valid_cers[0] * 100):.2f}%\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract training history from trainer state\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in train_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in train_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract epochs and losses\n",
    "# Group training logs by epoch\n",
    "train_by_epoch = {}\n",
    "for log in train_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in train_by_epoch:\n",
    "        train_by_epoch[epoch] = []\n",
    "    train_by_epoch[epoch].append(log.get('loss', 0))\n",
    "\n",
    "# Average losses per epoch\n",
    "epochs = sorted(train_by_epoch.keys())\n",
    "train_losses = [np.mean(train_by_epoch[epoch]) for epoch in epochs]\n",
    "\n",
    "# Extract validation losses by epoch\n",
    "eval_by_epoch = {}\n",
    "for log in eval_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in eval_by_epoch:\n",
    "        eval_by_epoch[epoch] = []\n",
    "    eval_by_epoch[epoch].append(log.get('eval_loss', 0))\n",
    "\n",
    "# Average validation losses per epoch\n",
    "eval_epochs = sorted(eval_by_epoch.keys())\n",
    "eval_losses = [np.mean(eval_by_epoch[epoch]) for epoch in eval_epochs]\n",
    "\n",
    "# Create the plot matching the image style\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "if eval_epochs:\n",
    "    plt.plot(eval_epochs, eval_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"Epoch-based loss plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQUJNm5eRHVh"
   },
   "source": [
    "## 11. Zip and Copy Model to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAwyb1zARHVi",
    "outputId": "faae84ff-5eea-4886-ced9-afee305bad9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating zip file: wav2vec2_lora_amharic_legal_final.zip...\n",
      "  Added: added_tokens.json\n",
      "  Added: preprocessor_config.json\n",
      "  Added: special_tokens_map.json\n",
      "  Added: tokenizer_config.json\n",
      "  Added: adapter_config.json\n",
      "  Added: training_args.bin\n",
      "  Added: adapter_model.safetensors\n",
      "  Added: README.md\n",
      "  Added: vocab.json\n",
      "\n",
      "Zip file created: wav2vec2_lora_amharic_legal_final.zip\n",
      "Model zip file copied to Google Drive: /content/drive/MyDrive/wav2vec2_lora_amharic_legal_final.zip\n",
      "\n",
      "File size: 6.21 MB\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory (saved to Kaggle working directory)\n",
    "zip_filename = f\"/kaggle/working/{Path(final_model_path).name}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            # Get relative path for archive\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"  Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "print(f\"File size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"\\nModel and zip file saved to Kaggle working directory (persistent)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrA-9_3CRHVj"
   },
   "source": [
    "## 12. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZAuL4OgRHVk",
    "outputId": "4bf62d8b-38ff-4d74-f942-87dc288c582b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to: wav2vec2_lora_amharic_legal_final\n",
      "\n",
      "Files saved:\n",
      "  - LoRA adapters (adapter_model.bin, adapter_config.json)\n",
      "  - Processor (tokenizer, feature_extractor)\n",
      "  - Training configuration\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - LoRA adapters (adapter_model.bin, adapter_config.json)\")\n",
    "print(\"  - Processor (tokenizer, feature_extractor)\")\n",
    "print(\"  - Training configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbojTaxzRHVl"
   },
   "source": [
    "## 13. Inference Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNTgA1skRHVl",
    "outputId": "c0eb1b89-eabf-4cb3-ddb7-8b56066741c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference function created\n",
      "\n",
      "Example usage:\n",
      "  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\n",
      "  print(transcription)\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(model, processor, audio_path):\n",
    "    \"\"\"Transcribe a single audio file\"\"\"\n",
    "    speech, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "print(\"Inference function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\")\n",
    "print(\"  print(transcription)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB2IActjRHVl"
   },
   "source": [
    "## 14. Load Model for Inference (After Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuLQLT2URHVn",
    "outputId": "3cd4b42c-748a-4dd7-8567-42c72852f5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading function created\n",
      "\n",
      "Example usage:\n",
      "  model, processor = load_trained_model(\n",
      "      MODEL_NAME,\n",
      "      'wav2vec2_lora_amharic_legal_final',\n",
      "      'wav2vec2_lora_amharic_legal_final'\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_trained_model(base_model_name, adapter_path, processor_path):\n",
    "    \"\"\"Load base model and LoRA adapters\"\"\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "\n",
    "    base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        base_model_name,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "print(\"Model loading function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  model, processor = load_trained_model(\")\n",
    "print(\"      MODEL_NAME,\")\n",
    "print(f\"      '{OUTPUT_DIR}_final',\")\n",
    "print(f\"      '{OUTPUT_DIR}_final'\")\n",
    "print(\"  )\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "100cbfeb0f9d4987befae78553e3bad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2696ada16df2448baea07f7061442f23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0bdc188df146c1aa93dc2693e9a94a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3066992bc6a042b9a9d3774d0a49afe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5522323fea774fc3aa639de383bbc6a6",
       "IPY_MODEL_3fb8e3ed5d474ef48c40bb5ef1b32c53",
       "IPY_MODEL_c7b6988b67084060ad526aac426b783a"
      ],
      "layout": "IPY_MODEL_51372394c01a4abc9fef41872b11f556"
     }
    },
    "3fb8e3ed5d474ef48c40bb5ef1b32c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d746c0494a3c4247b4de3813e32ded61",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b7d4f3fd3f7479aacb62aaf7cea690c",
      "value": 39
     }
    },
    "42649ca8bd0648db8966485a57f81531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_517048ac9b714570b114156b67e7f3ba",
       "IPY_MODEL_ed9c593de4b946859093bcefec492105",
       "IPY_MODEL_6def75c803d5403e8c2410633655d31d"
      ],
      "layout": "IPY_MODEL_49dd8d55eaa44953aab2cb28ccd114e4"
     }
    },
    "49c1b3c35cd7405ebfc838a237a75a58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49dd8d55eaa44953aab2cb28ccd114e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c30ca48d6e2409eb660aa319bec31f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617b78b2ff594f1c8d071955947f5848",
      "max": 37,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ad7ccdc65bb4561b44d0b6e17c011a2",
      "value": 37
     }
    },
    "51372394c01a4abc9fef41872b11f556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517048ac9b714570b114156b67e7f3ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76d5deb9356047dc9a4a8e83ec9b1d1a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2c0bdc188df146c1aa93dc2693e9a94a",
      "value": "Map:‚Äá100%"
     }
    },
    "5522323fea774fc3aa639de383bbc6a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e6b622c0d148908090150d8bec7a15",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_100cbfeb0f9d4987befae78553e3bad4",
      "value": "Map:‚Äá100%"
     }
    },
    "5c4329e8541b49fb84a80c28b6390c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62349c8a80bc407fb675ba8e0880198d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d4fde5f445b94a3a9f953c3d4c824e48",
      "value": "Map:‚Äá100%"
     }
    },
    "617b78b2ff594f1c8d071955947f5848": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62349c8a80bc407fb675ba8e0880198d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64a07e9bc4b64bedb4b3d9b78aedc0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ad7ccdc65bb4561b44d0b6e17c011a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b7d4f3fd3f7479aacb62aaf7cea690c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6def75c803d5403e8c2410633655d31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d30049acfc7a42c8a52ff4a35d255f82",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eb20e5586f8146e5b0f2acb4cc1df38e",
      "value": "‚Äá302/302‚Äá[00:10&lt;00:00,‚Äá34.12‚Äáexamples/s]"
     }
    },
    "70bbbe6d03ee4944bbba199fff1e97f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71daaa4fe75e432e9b2997ea0c324c91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76d5deb9356047dc9a4a8e83ec9b1d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8992eb0fa5a74dc4b75d4649336ff774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71daaa4fe75e432e9b2997ea0c324c91",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_70bbbe6d03ee4944bbba199fff1e97f2",
      "value": "‚Äá37/37‚Äá[00:00&lt;00:00,‚Äá54.77‚Äáexamples/s]"
     }
    },
    "b4e6b622c0d148908090150d8bec7a15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b90a4443314f66aca7e8ab8e3e5c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5c90634bce04d66bc6d3c1b5edd3eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c4329e8541b49fb84a80c28b6390c7a",
       "IPY_MODEL_4c30ca48d6e2409eb660aa319bec31f7",
       "IPY_MODEL_8992eb0fa5a74dc4b75d4649336ff774"
      ],
      "layout": "IPY_MODEL_2696ada16df2448baea07f7061442f23"
     }
    },
    "c7b6988b67084060ad526aac426b783a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_49c1b3c35cd7405ebfc838a237a75a58",
      "value": "‚Äá39/39‚Äá[00:00&lt;00:00,‚Äá41.89‚Äáexamples/s]"
     }
    },
    "d30049acfc7a42c8a52ff4a35d255f82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4fde5f445b94a3a9f953c3d4c824e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d746c0494a3c4247b4de3813e32ded61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e083923a7c854c2d90026366abc3112d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb20e5586f8146e5b0f2acb4cc1df38e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed9c593de4b946859093bcefec492105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e083923a7c854c2d90026366abc3112d",
      "max": 302,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b90a4443314f66aca7e8ab8e3e5c1e",
      "value": 302
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
