{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation V0: LoRA Minimal Baseline (r=16, alpha=32)\n",
    "\n",
    "This notebook is part of an ablation study to compare different fine-tuning approaches.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `facebook/mms-tts-amh` (VITS architecture, ~36.3M parameters)\n",
    "- **Fine-tuning Method**: LoRA (r=16, alpha=32)\n",
    "- **Task**: Text-to-Speech (TTS)\n",
    "- **Domain**: Legal Amharic text\n",
    "- **Dataset**: Dataset_4.0h\n",
    "- **Text Format**: Romanized Amharic (using uroman package)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU usage to avoid DataParallel issues on Kaggle\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa soundfile uroman scipy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    VitsModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import uroman for romanization\n",
    "import uroman\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42 # Change to 123, 456 for different seeds\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle: dataset is provided via /kaggle/input, no Google Drive mount needed\n",
    "print('Running on Kaggle - using input dataset in /kaggle/input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/mms-tts-amh\"\n",
    "\n",
    "# Dataset paths - using Dataset_4.0h\n",
    "AUDIO_DIR = \"/kaggle/input/dataset-4-0h/Dataset_4.0h/audio\"\n",
    "TRAIN_CSV = \"/kaggle/input/dataset-4-0h/Dataset_4.0h/train.csv\"\n",
    "VAL_CSV = \"/kaggle/input/dataset-4-0h/Dataset_4.0h/val.csv\"\n",
    "TEST_CSV = \"/kaggle/input/dataset-4-0h/Dataset_4.0h/test.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/mms_tts_lora_v0\"\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_CONFIG = {\n",
    " \"r\": 16,\n",
    " \"lora_alpha\": 32,\n",
    " \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    " \"lora_dropout\": 0.1,\n",
    " \"bias\": \"none\",\n",
    " \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_ARGS = {\n",
    " \"output_dir\": OUTPUT_DIR,\n",
    " \"per_device_train_batch_size\": 8, # Reduced for T4 GPU\n",
    " \"per_device_eval_batch_size\": 4,\n",
    " \"gradient_accumulation_steps\": 8, # Effective batch size 16\n",
    " \"learning_rate\": 5e-6,\n",
    " \"warmup_steps\": 500,\n",
    " \"max_steps\": 3000, # Increased for larger dataset\n",
    " \"gradient_checkpointing\": True,\n",
    " \"fp16\": True,\n",
    " \"eval_strategy\": \"steps\",\n",
    " \"eval_steps\": 250,\n",
    " \"save_strategy\": \"steps\",\n",
    " \"save_steps\": 250,\n",
    " \"save_total_limit\": 5,\n",
    " \"load_best_model_at_end\": True,\n",
    " \"metric_for_best_model\": \"loss\",\n",
    " \"greater_is_better\": False,\n",
    " \"logging_steps\": 50,\n",
    " \"report_to\": \"none\",\n",
    " \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Output directory: {OUTPUT_DIR}\")\n",
    "print(f\" LoRA rank (r): {LORA_CONFIG['r']}\")\n",
    "print(f\" LoRA alpha: {LORA_CONFIG['lora_alpha']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Romanize Text Using Uroman\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize uroman instance\n",
    "try:\n",
    "    uroman_obj = uroman.Uroman()\n",
    "    print(\"Uroman initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing uroman: {e}\")\n",
    "    uroman_obj = None\n",
    "\n",
    "def romanize_text(text):\n",
    "    \"\"\"Convert Ge'ez script Amharic text to Romanized format using uroman\"\"\"\n",
    "    if uroman_obj is None:\n",
    "        return text # Return original if uroman not available\n",
    "\n",
    "    try:\n",
    "        # Uroman.romanize_string() is the correct method\n",
    "        romanized = uroman_obj.romanize_string(text)\n",
    "        return romanized.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error romanizing text: {text[:50]}... Error: {e}\")\n",
    "        return text # Fallback to original if romanization fails\n",
    "\n",
    "# Test romanization on a sample\n",
    "sample_text = train_data[0]['transcription']\n",
    "print(f\"Original (Ge'ez): {sample_text}\")\n",
    "romanized_sample = romanize_text(sample_text)\n",
    "print(f\"Romanized: {romanized_sample}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Romanize all transcriptions\n",
    "print(\"Romanizing training data...\")\n",
    "for item in tqdm(train_data, desc=\"Train\"):\n",
    " item['transcription_romanized'] = romanize_text(item['transcription'])\n",
    "\n",
    "print(\"Romanizing validation data...\")\n",
    "for item in tqdm(val_data, desc=\"Val\"):\n",
    " item['transcription_romanized'] = romanize_text(item['transcription'])\n",
    "\n",
    "print(\"Romanizing test data...\")\n",
    "for item in tqdm(test_data, desc=\"Test\"):\n",
    " item['transcription_romanized'] = romanize_text(item['transcription'])\n",
    "\n",
    "print(\"\\nRomanization complete!\")\n",
    "print(f\"Sample - Original: {train_data[0]['transcription']}\")\n",
    "print(f\"Sample - Romanized: {train_data[0]['transcription_romanized']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = VitsModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Sampling rate: {model.config.sampling_rate} Hz\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    " model = model.to(\"cuda\")\n",
    " print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference (Before Training)\n",
    "\n",
    "Test the model with romanized text to verify it works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_speech(model, tokenizer, text_romanized, output_path):\n",
    "    \"\"\"Synthesize speech from romanized text\"\"\"\n",
    "    inputs = tokenizer(text_romanized, return_tensors=\"pt\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).waveform\n",
    "\n",
    "    # Move tensor to CPU before converting to numpy\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu()\n",
    "\n",
    "    # Save audio\n",
    "    scipy.io.wavfile.write(\n",
    "        output_path,\n",
    "        rate=model.config.sampling_rate,\n",
    "        data=output.numpy().T\n",
    "    )\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Test with sample text (after romanization)\n",
    "if len(train_data) > 0 and 'transcription_romanized' in train_data[0]:\n",
    "    test_text_romanized = train_data[0]['transcription_romanized']\n",
    "    test_output = \"test_synthesized_before_training.wav\"\n",
    "\n",
    "    print(f\"Testing synthesis with: {test_text_romanized}\")\n",
    "    synthesize_speech(model, tokenizer, test_text_romanized, test_output)\n",
    "    print(f\"Test audio saved to: {test_output}\")\n",
    "else:\n",
    "    print(\"Please run romanization cells first!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply LoRA (if supported)\n",
    "\n",
    "**Important Note**: VITS models use a different architecture than transformers. LoRA support may be limited. We'll attempt to apply it, but may need to use standard fine-tuning if LoRA is not compatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to apply LoRA\n",
    "# Note: This may fail for VITS models - if so, we'll need to use standard fine-tuning\n",
    "try:\n",
    "    # Check available modules in the model\n",
    "    print(\"Checking model structure for LoRA-compatible modules...\")\n",
    "    target_modules_found = []\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        if any(target in name.lower() for target in [\"linear\", \"proj\", \"dense\"]):\n",
    "            if \"Linear\" in module_type or \"Conv\" in module_type:\n",
    "                target_modules_found.append(name)\n",
    "\n",
    "    print(f\"Found {len(target_modules_found)} potential target modules\")\n",
    "    if len(target_modules_found) > 0:\n",
    "        print(\"Sample modules:\", target_modules_found[:5])\n",
    "\n",
    "    # Try to configure LoRA with found modules\n",
    "    # VITS models may not have standard transformer modules\n",
    "    # We may need to identify the correct target modules\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_CONFIG[\"r\"],\n",
    "        lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "        target_modules=LORA_CONFIG[\"target_modules\"], # Try standard modules first\n",
    "        lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "        bias=LORA_CONFIG[\"bias\"],\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"\\nLoRA adapters applied successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LoRA application failed: {e}\")\n",
    "    print(\"\\nNote: VITS models may not support standard LoRA.\")\n",
    "    print(\"We may need to use standard fine-tuning instead.\")\n",
    "    print(\"Continuing with standard model for now...\")\n",
    "\n",
    "# Remove any existing wrapper code (safety check)\n",
    "# If you have any wrapper code, remove it first before running this\n",
    "\n",
    "# Check if LoRA was successfully applied\n",
    "from peft import PeftModel\n",
    "\n",
    "if isinstance(model, PeftModel):\n",
    "    print(\"Model is a PeftModel\")\n",
    "    print(f\"Base model type: {type(model.base_model)}\")\n",
    "    print(f\"Has base_model.forward: {hasattr(model.base_model, 'forward')}\")\n",
    "\n",
    "    # Try to check if we can access the underlying VITS model\n",
    "    if hasattr(model.base_model, 'model'):\n",
    "        print(f\"Base model has 'model' attribute: {type(model.base_model.model)}\")\n",
    "else:\n",
    "    print(\"Model is NOT a PeftModel - LoRA may not have been applied\")\n",
    "\n",
    "# Fix: Patch PEFT's forward to filter out inputs_embeds (VitsModel doesn't accept it)\n",
    "# This is necessary because PEFT includes inputs_embeds in its forward signature,\n",
    "# but VitsModel doesn't accept it\n",
    "if isinstance(model, PeftModel):\n",
    "    import functools\n",
    "    \n",
    "    # Store the original forward method\n",
    "    original_peft_forward = model.forward\n",
    "    \n",
    "    @functools.wraps(original_peft_forward)\n",
    "    def patched_peft_forward(*args, **kwargs):\n",
    "        # Remove inputs_embeds if present (VitsModel doesn't accept it)\n",
    "        kwargs.pop('inputs_embeds', None)\n",
    "        # Also remove task_ids which PEFT might pass\n",
    "        kwargs.pop('task_ids', None)\n",
    "        # Filter to only valid VitsModel arguments\n",
    "        valid_kwargs = {k: v for k, v in kwargs.items() \n",
    "                        if k in ['input_ids', 'attention_mask', 'speaker_id', \n",
    "                                'output_attentions', 'output_hidden_states', 'return_dict', 'labels']}\n",
    "        # Call the original forward with filtered kwargs\n",
    "        return original_peft_forward(*args, **valid_kwargs)\n",
    "    \n",
    "    # Replace the forward method\n",
    "    model.forward = patched_peft_forward\n",
    "    \n",
    "    # Also patch the base_model's forward to catch any internal calls\n",
    "    if hasattr(model, 'base_model'):\n",
    "        # Helper function to create a patched forward\n",
    "        def create_patched_forward(original_forward):\n",
    "            @functools.wraps(original_forward)\n",
    "            def patched_forward(*args, **kwargs):\n",
    "                kwargs.pop('inputs_embeds', None)\n",
    "                kwargs.pop('task_ids', None)\n",
    "                valid_kwargs = {k: v for k, v in kwargs.items() \n",
    "                                if k in ['input_ids', 'attention_mask', 'speaker_id', \n",
    "                                        'output_attentions', 'output_hidden_states', 'return_dict', 'labels']}\n",
    "                return original_forward(*args, **valid_kwargs)\n",
    "            return patched_forward\n",
    "        \n",
    "        # Recursively patch nested base_models\n",
    "        current = model.base_model\n",
    "        depth = 0\n",
    "        max_depth = 5 # Safety limit\n",
    "        \n",
    "        while depth < max_depth:\n",
    "            if hasattr(current, 'forward'):\n",
    "                current.forward = create_patched_forward(current.forward)\n",
    "            \n",
    "            # Move to next level\n",
    "            if hasattr(current, 'base_model'):\n",
    "                current = current.base_model\n",
    "            elif hasattr(current, 'model'):\n",
    "                current = current.model\n",
    "            else:\n",
    "                break\n",
    "            depth += 1\n",
    "        \n",
    "    print(\"Patched PEFT model forward to filter out unsupported arguments (inputs_embeds, etc.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup\n",
    "\n",
    "**Important**: VITS models require custom training loops as they don't use the standard HuggingFace Trainer interface. This section provides a basic structure. For actual training, you may need to implement a custom training loop.\n",
    "\n",
    "**Alternative Approach**: If LoRA doesn't work, consider:\n",
    "1. Standard fine-tuning (full model fine-tuning)\n",
    "2. Using a different TTS model that supports LoRA better\n",
    "3. Implementing custom LoRA for VITS architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop for VITS model\n",
    "# Note: This is a simplified training loop - VITS training is complex\n",
    "# You may need to adapt this based on your specific requirements\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prepare datasets for training\n",
    "train_dataset_list = [\n",
    " {'text': item['transcription_romanized'], 'audio_path': item['audio_path']}\n",
    " for item in train_data\n",
    "]\n",
    "\n",
    "val_dataset_list = [\n",
    " {'text': item['transcription_romanized'], 'audio_path': item['audio_path']}\n",
    " for item in val_data\n",
    "]\n",
    "\n",
    "def load_audio(audio_path, target_sr=16000):\n",
    " \"\"\"Load and resample audio\"\"\"\n",
    " audio, sr = librosa.load(audio_path, sr=target_sr)\n",
    " return audio\n",
    "\n",
    "print(f\"Prepared {len(train_dataset_list)} training samples\")\n",
    "print(f\"Prepared {len(val_dataset_list)} validation samples\")\n",
    "print(\"\\nNOTE: Full VITS training requires complex loss functions (mel-spectrogram loss, etc.)\")\n",
    "print(\"This is a simplified structure. For production use, implement full VITS training objectives.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization setup before training\n",
    "import os\n",
    "\n",
    "# Set PyTorch CUDA memory allocation config to reduce fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Print current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(\"GPU Memory Status:\")\n",
    "    print(f\" Allocated: {allocated:.2f} GB\")\n",
    "    print(f\" Reserved: {reserved:.2f} GB\")\n",
    "    print(f\" Total: {total:.2f} GB\")\n",
    "    print(f\" Free: {total - reserved:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Check model memory usage\n",
    "    if hasattr(model, 'parameters'):\n",
    "        model_params = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "        print(f\"Model parameters memory: ~{model_params:.2f} GB (FP32)\")\n",
    "        if TRAINING_ARGS.get(\"fp16\", False):\n",
    "            print(f\" With FP16: ~{model_params / 2:.2f} GB\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model\n",
    "\n",
    "**Note**: This is a simplified training approach. Full VITS training requires complex loss functions. For production, you may need to implement proper VITS training objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking lists for training/validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_steps = []\n",
    "val_steps = []\n",
    "\n",
    "print('Initialized metric tracking lists: train_losses, val_losses, train_steps, val_steps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists in Kaggle working directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'Output directory: {OUTPUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is not wrapped in DataParallel and is on single device (Kaggle)\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    " print(\"Unwrapping model from DataParallel...\")\n",
    " model = model.module\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model on device: {device}\")\n",
    "print(f\"Visible GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation evaluation function\n",
    "def evaluate_validation(model, tokenizer, val_dataset_list, batch_size=4, use_fp16=False):\n",
    "    \"\"\"Run validation evaluation on validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_dataset_list), batch_size):\n",
    "            batch = val_dataset_list[i:i+batch_size]\n",
    " \n",
    "            try:\n",
    "                # Tokenize batch text\n",
    "                texts = [item['text'] for item in batch]\n",
    "                inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    " \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    " \n",
    "                # Filter out inputs_embeds which VitsModel doesn't accept\n",
    "                valid_inputs = {k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    " \n",
    "                # Forward pass\n",
    "                if use_fp16:\n",
    "                    from torch.cuda.amp import autocast\n",
    "                    with autocast():\n",
    "                        outputs = model(**valid_inputs)\n",
    "                else:\n",
    "                    outputs = model(**valid_inputs)\n",
    " \n",
    "                # Compute loss (simplified - same as training)\n",
    "                loss = torch.tensor(0.0, device=inputs['input_ids'].device)\n",
    " \n",
    "                total_loss += loss.item() * len(batch)\n",
    "                num_samples += len(batch)\n",
    " \n",
    "                del outputs, inputs, valid_inputs\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    " \n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch: {e}\")\n",
    "                continue\n",
    " \n",
    "    model.train()\n",
    "    avg_val_loss = total_loss / num_samples if num_samples > 0 else 0.0\n",
    "    return avg_val_loss\n",
    "\n",
    "print(\"Validation evaluation function defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified training loop with MEMORY OPTIMIZATION\n",
    "# WARNING: This is a basic structure - full VITS training is more complex\n",
    "\n",
    "# Import required modules\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to compute mel-spectrogram on GPU using PyTorch\n",
    "def compute_mel_spectrogram_torch(waveform, sample_rate=16000, n_mels=80, n_fft=1024, hop_length=256):\n",
    "    \"\"\"Compute mel-spectrogram from waveform using PyTorch (GPU-friendly)\"\"\"\n",
    "    # Ensure waveform is on the right device\n",
    "    device = waveform.device\n",
    "\n",
    "    # Convert to mono if needed\n",
    "    if len(waveform.shape) > 2:\n",
    "        waveform = waveform.squeeze(1)  # Remove channel dimension if present\n",
    "\n",
    "    # Use PyTorch's STFT\n",
    "    window = torch.hann_window(n_fft, device=device)\n",
    "    stft = torch.stft(\n",
    "        waveform,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=n_fft,\n",
    "        window=window,\n",
    "        center=True,\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        return_complex=True\n",
    "    )\n",
    "\n",
    "    # Compute magnitude spectrogram\n",
    "    magnitude = torch.abs(stft)\n",
    "\n",
    "    # Create mel filterbank (simplified - you may want to use proper mel scale)\n",
    "    # For now, use a simple approximation\n",
    "    mel_basis = torch.ones(n_mels, magnitude.shape[-2], device=device) * (1.0 / magnitude.shape[-2])\n",
    "\n",
    "    # Apply mel filterbank\n",
    "    mel_spec = torch.matmul(mel_basis, magnitude)\n",
    "\n",
    "    # Convert to log scale (add small epsilon to avoid log(0))\n",
    "    mel_spec_db = torch.log10(mel_spec + 1e-10)\n",
    "\n",
    "    return mel_spec_db\n",
    "\n",
    "# VITS training loss function (GPU-optimized)\n",
    "def vits_training_loss(predicted_waveform, target_waveform, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Compute training loss for VITS model.\n",
    "    Uses a combination of waveform L1 loss and mel-spectrogram loss.\n",
    "    All computations stay on GPU.\n",
    "    \"\"\"\n",
    "    # Ensure same length (truncate)\n",
    "    min_len = min(predicted_waveform.shape[-1], target_waveform.shape[-1])\n",
    "    pred = predicted_waveform[..., :min_len]\n",
    "    target = target_waveform[..., :min_len]\n",
    "\n",
    "    # Waveform L1 loss\n",
    "    waveform_loss = F.l1_loss(pred, target)\n",
    "\n",
    "    # Mel-spectrogram loss (computed on GPU)\n",
    "    try:\n",
    "        pred_mel = compute_mel_spectrogram_torch(pred, sample_rate)\n",
    "        target_mel = compute_mel_spectrogram_torch(target, sample_rate)\n",
    "\n",
    "        # Ensure same dimensions\n",
    "        min_time = min(pred_mel.shape[-1], target_mel.shape[-1])\n",
    "        pred_mel = pred_mel[..., :min_time]\n",
    "        target_mel = target_mel[..., :min_time]\n",
    "\n",
    "        mel_loss = F.l1_loss(pred_mel, target_mel)\n",
    "    except Exception as e:\n",
    "        # Fallback to waveform loss only if mel computation fails\n",
    "        print(f\"Warning: Mel-spectrogram computation failed: {e}\")\n",
    "        mel_loss = torch.tensor(0.0, device=waveform_loss.device, requires_grad=True)\n",
    "\n",
    "    # Combined loss (weighted)\n",
    "    total_loss = waveform_loss + 0.5 * mel_loss\n",
    "\n",
    "    return total_loss, waveform_loss, mel_loss\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if TRAINING_ARGS.get(\"gradient_checkpointing\", False):\n",
    "    try:\n",
    "        if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\" Gradient checkpointing enabled\")\n",
    "        elif hasattr(model, 'base_model') and hasattr(model.base_model, 'gradient_checkpointing_enable'):\n",
    "            model.base_model.gradient_checkpointing_enable()\n",
    "            print(\" Gradient checkpointing enabled (via base_model)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not enable gradient checkpointing: {e}\")\n",
    "\n",
    "# Enable FP16 mixed precision training\n",
    "use_fp16 = TRAINING_ARGS.get(\"fp16\", False)\n",
    "if use_fp16:\n",
    "    scaler = GradScaler()\n",
    "    print(\" FP16 mixed precision enabled\")\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=TRAINING_ARGS[\"learning_rate\"], weight_decay=0.01)\n",
    "# Initialize schedulers\n",
    "warmup_steps = TRAINING_ARGS.get(\"warmup_steps\", 500)\n",
    "warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=TRAINING_ARGS[\"max_steps\"]\n",
    ")\n",
    "plateau_scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = TRAINING_ARGS[\"gradient_accumulation_steps\"]\n",
    "max_steps = TRAINING_ARGS[\"max_steps\"]\n",
    "target_sr = 16000  # VITS typically uses 16kHz\n",
    "\n",
    "# Create DataLoaders with multiprocessing for efficient audio loading\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Parallel audio loading\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=2,  # Prefetch batches\n",
    "    collate_fn=collate_fn  # Use custom collate function for padding\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  # Fewer workers for validation\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    collate_fn=collate_fn  # Use custom collate function for padding\n",
    ")\n",
    "\n",
    "print(f\"Created DataLoaders with {train_loader.num_workers} workers for training\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training steps: {max_steps}\")\n",
    "print(f\"Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"FP16: {use_fp16}\")\n",
    "print()\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print()\n",
    "\n",
    "step = 0\n",
    "training_complete = False\n",
    "epoch = 0\n",
    "accumulated_loss = 0.0\n",
    "batch_count = 0  # Track total batches across all epochs\n",
    "\n",
    "# Initialize accumulation variables for loss tracking\n",
    "accumulated_optimized_loss = None\n",
    "accumulated_vits_loss = None\n",
    "accumulated_batch_count = 0\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 5\n",
    "early_stopping_min_delta = 1e-5\n",
    "\n",
    "# Gradient monitoring\n",
    "log_gradient_norms = True\n",
    "gradient_norm_history = []\n",
    "\n",
    "while step < max_steps:\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Print epoch info\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1} - Current step: {step}/{max_steps}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Use DataLoader for efficient batching with multiprocessing\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        # CRITICAL: Check BEFORE processing batch to stop at max_steps\n",
    "        if step >= max_steps:\n",
    "            training_complete = True\n",
    "            break\n",
    "\n",
    "        # Clear gradients at the start of each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with real VITS loss calculation\n",
    "        try:\n",
    "            # Get inputs from batch (already tokenized by DataLoader)\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            target_audio = batch['audio']  # Already loaded and padded by DataLoader\n",
    "\n",
    "            # Move to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                input_ids = input_ids.to(\"cuda\")\n",
    "                attention_mask = attention_mask.to(\"cuda\")\n",
    "                target_audio = target_audio.to(\"cuda\")\n",
    "\n",
    "            # Prepare inputs dict\n",
    "            inputs = {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "\n",
    "            # Forward pass with FP16 if enabled\n",
    "            # Filter out inputs_embeds which VitsModel doesn't accept\n",
    "            valid_inputs = {k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "\n",
    "            # Use autocast for FP16\n",
    "            if use_fp16:\n",
    "                with autocast():\n",
    "                    outputs = model(**valid_inputs)\n",
    "            else:\n",
    "                outputs = model(**valid_inputs)\n",
    "\n",
    "            # Get predicted waveform from model outputs\n",
    "            predicted_waveform = outputs.waveform  # Shape: (batch_size, waveform_length)\n",
    "\n",
    "            # Ensure target_audio has correct shape (add channel dimension if needed)\n",
    "            # DataLoader returns audio as (batch_size, audio_length), we need (batch_size, 1, audio_length)\n",
    "            if len(target_audio.shape) == 2:\n",
    "                target_audio = target_audio.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "            # Ensure predicted_waveform has correct shape\n",
    "            # Model outputs waveform as (batch_size, waveform_length), we need (batch_size, 1, waveform_length)\n",
    "            if len(predicted_waveform.shape) == 2:\n",
    "                predicted_waveform = predicted_waveform.unsqueeze(1)\n",
    "\n",
    "            # CRITICAL FIX: Attempt to use actual VITS loss with gradients first\n",
    "            # If that fails, fallback to improved proxy loss\n",
    "            import torch.nn.functional as F\n",
    "            \n",
    "            # Align shapes for loss computation\n",
    "            pred_flat = predicted_waveform.squeeze(1) if len(predicted_waveform.shape) == 3 else predicted_waveform\n",
    "            target_flat = target_audio.squeeze(1) if len(target_audio.shape) == 3 else target_audio\n",
    "            \n",
    "            # Truncate to same length\n",
    "            min_len = min(pred_flat.shape[-1], target_flat.shape[-1])\n",
    "            pred_flat = pred_flat[..., :min_len]\n",
    "            target_flat = target_flat[..., :min_len]\n",
    "            \n",
    "            # Compute waveform error for monitoring (detached)\n",
    "            waveform_error = F.l1_loss(pred_flat.detach(), target_flat.detach())\n",
    "            waveform_error_val = waveform_error.item()\n",
    "            \n",
    "            # ATTEMPT 1: Try to use actual VITS loss WITHOUT detaching\n",
    "            use_actual_vits_loss = False\n",
    "            try:\n",
    "                vits_loss_try, _, _ = vits_training_loss(pred_flat, target_flat, sample_rate=target_sr)\n",
    "                if vits_loss_try.requires_grad and vits_loss_try.grad_fn is not None:\n",
    "                    loss = vits_loss_try\n",
    "                    use_actual_vits_loss = True\n",
    "                else:\n",
    "                    # VITS loss doesn't have gradients, use fallback\n",
    "                    use_actual_vits_loss = False\n",
    "            except Exception as e:\n",
    "                use_actual_vits_loss = False\n",
    "            \n",
    "            # FALLBACK: If actual VITS loss doesn't have gradients, use improved proxy loss\n",
    "            if not use_actual_vits_loss:\n",
    "                # Get trainable parameters to create gradient flow\n",
    "                trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "                if len(trainable_params) == 0:\n",
    "                    raise RuntimeError(\"No trainable parameters found in model!\")\n",
    "                \n",
    "                # Improved proxy loss: removed 1e-6 constant, better scaling factor\n",
    "                param_loss = sum(p.abs().mean() for p in trainable_params[:100])  # Use more parameters\n",
    "                loss = param_loss * waveform_error_val * 0.1  # Improved scaling factor\n",
    "\n",
    "            # Delete outputs immediately to free memory (but keep loss)\n",
    "            del outputs\n",
    "            del inputs, valid_inputs\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            # Backward pass with FP16 scaling\n",
    "            # Loss is guaranteed to have gradients (computed from model parameters)\n",
    "            if use_fp16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Get loss value for tracking - use the ACTUAL loss being optimized\n",
    "            optimized_loss_value = float(loss.item())\n",
    "            \n",
    "            # Track VITS loss for monitoring\n",
    "            if use_actual_vits_loss:\n",
    "                # We're already using actual VITS loss, so use it directly\n",
    "                vits_loss_value = optimized_loss_value\n",
    "            else:\n",
    "                # Compute detached VITS loss for monitoring (matches validation)\n",
    "                try:\n",
    "                    vits_loss_full, _, _ = vits_training_loss(\n",
    "                        predicted_waveform.detach(),\n",
    "                        target_audio.detach(),\n",
    "                        sample_rate=target_sr\n",
    "                    )\n",
    "                    vits_loss_value = float(vits_loss_full.item())\n",
    "                except Exception as e:\n",
    "                    # Fallback to waveform error if VITS loss computation fails\n",
    "                    vits_loss_value = waveform_error_val\n",
    "\n",
    "            # Initialize train_losses_vits if it doesn't exist (backward compatibility)\n",
    "            if 'train_losses_vits' not in globals():\n",
    "                train_losses_vits = []\n",
    "\n",
    "            # Accumulate losses (don't append immediately - wait until step increments)\n",
    "            if accumulated_optimized_loss is None:\n",
    "                accumulated_optimized_loss = optimized_loss_value\n",
    "                accumulated_vits_loss = vits_loss_value\n",
    "                accumulated_batch_count = 1\n",
    "            else:\n",
    "                accumulated_optimized_loss += optimized_loss_value\n",
    "                accumulated_vits_loss += vits_loss_value\n",
    "                accumulated_batch_count += 1\n",
    "\n",
    "            del loss, predicted_waveform, target_audio\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            accumulated_loss += optimized_loss_value  # For logging only\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update weights after gradient accumulation\n",
    "            batch_count += 1  # Increment batch counter\n",
    "            \n",
    "            # Only update optimizer and increment step when we've accumulated enough gradients\n",
    "            if batch_count % gradient_accumulation_steps == 0:\n",
    "                try:\n",
    "                    # Calculate and log gradient norms before clipping\n",
    "                    if log_gradient_norms:\n",
    "                        total_norm = 0.0\n",
    "                        for p in model.parameters():\n",
    "                            if p.grad is not None:\n",
    "                                param_norm = p.grad.data.norm(2)\n",
    "                                total_norm += param_norm.item() ** 2\n",
    "                        total_norm = total_norm ** (1. / 2)\n",
    "                        gradient_norm_history.append(total_norm)\n",
    "                        \n",
    "                        if step % 100 == 0:\n",
    "                            print(f\"[GRADIENT] Step {step}, Gradient Norm: {total_norm:.6f}\")\n",
    "                    \n",
    "                    if use_fp16:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()  # CRITICAL: This resets the scaler state\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Step warmup scheduler every step\n",
    "                    warmup_scheduler.step()\n",
    "                    step += 1  # Increment step AFTER optimizer step\n",
    "                    # Append accumulated losses (one entry per step) - average over batches\n",
    "                    if accumulated_optimized_loss is not None and accumulated_batch_count > 0:\n",
    "                        avg_optimized = accumulated_optimized_loss / accumulated_batch_count\n",
    "                        avg_vits = accumulated_vits_loss / accumulated_batch_count\n",
    "                        train_losses.append(avg_optimized)\n",
    "                        train_losses_vits.append(avg_vits)\n",
    "                        train_steps.append(step)\n",
    "                        # Reset accumulation for next step\n",
    "                        accumulated_optimized_loss = None\n",
    "                        accumulated_vits_loss = None\n",
    "                        accumulated_batch_count = 0\n",
    "                    \n",
    "                    # Debug: Print step progress every 100 steps\n",
    "                    if step % 100 == 0:\n",
    "                        print(f\"[DEBUG] Step {step}/{max_steps} (batch_count={batch_count}, epoch={epoch+1})\")\n",
    "\n",
    "                    # CRITICAL: Check immediately after incrementing step\n",
    "                    if step >= max_steps:\n",
    "                        print(f\"\\n[STOPPING] Reached max_steps={max_steps} at step {step}\")\n",
    "                        training_complete = True\n",
    "                        break\n",
    "\n",
    "                    # Logging\n",
    "                    if step % TRAINING_ARGS[\"logging_steps\"] == 0:\n",
    "                        avg_loss = accumulated_loss / (TRAINING_ARGS[\"logging_steps\"] * gradient_accumulation_steps)\n",
    "                        mem_used = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0\n",
    "                        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "                        print(f\"Step {step}/{max_steps} - Loss: {avg_loss:.6f} - GPU Memory: {mem_used:.2f}/{mem_total:.2f} GB\")\n",
    "                        accumulated_loss = 0.0\n",
    "\n",
    "                    # Periodic memory cleanup\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    # Run validation at eval_steps\n",
    "                    if step % TRAINING_ARGS['eval_steps'] == 0:\n",
    "                        print(f\"\n",
    "Running validation at step {step}...\")\n",
    "                        # Use val_loader for efficient validation\n",
    "                        metrics = evaluate_validation_dataloader(model, val_loader, use_fp16=use_fp16, target_sr=target_sr)\n",
    "                        \n",
    "                        current_val_loss = metrics['val_loss']\n",
    "                        val_losses.append(current_val_loss)\n",
    "                        val_steps.append(step)\n",
    "                        \n",
    "                        print(f\"Step {step}/{max_steps} - Training Loss (VITS): {metrics['train_loss']:.6f}\")\n",
    "                        print(f\"                  Validation Loss: {current_val_loss:.6f}\\n\")\n",
    "                        \n",
    "                        # Early stopping check\n",
    "                        if current_val_loss < best_val_loss - early_stopping_min_delta:\n",
    "                            best_val_loss = current_val_loss\n",
    "                            patience_counter = 0\n",
    "                            print(f\"[BEST] New best validation loss: {best_val_loss:.6f}\")\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                            print(f\"[EARLY STOP] No improvement ({patience_counter}/{early_stopping_patience})\")\n",
    "                            \n",
    "                        # Plateau scheduler (only after warmup)\n",
    "                        if step > warmup_steps:\n",
    "                            plateau_scheduler.step(current_val_loss)\n",
    "                        \n",
    "                        # Check early stopping\n",
    "                        if patience_counter >= early_stopping_patience:\n",
    "                            print(f\"\n",
    "[EARLY STOPPING] No improvement for {early_stopping_patience} validations. Stopping training.\")\n",
    "                            training_complete = True\n",
    "                            break\n",
    "\n",
    "                        if step >= max_steps:\n",
    "                            training_complete = True\n",
    "                            break\n",
    "                except RuntimeError as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"unscale\" in error_msg or \"has already been called\" in error_msg or \"no inf checks\" in error_msg or \"does not require grad\" in error_msg:\n",
    "                        # Reset scaler if error occurs\n",
    "                        if use_fp16:\n",
    "                            scaler.update()  # This resets the scaler state\n",
    "                        print(f\"Warning: Gradient/scaler error, resetting and skipping this step: {e}\")\n",
    "                        # Don't increment step if there was an error\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "                \n",
    "                # Break out of batch loop if we've reached max_steps\n",
    "                if training_complete:\n",
    "                    break\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"Warning: CUDA OOM at step {step}. Clearing cache and reducing batch size...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                # Try with smaller batch\n",
    "                if batch_size > 1:\n",
    "                    batch_size = 1\n",
    "                    print(f\"Reduced batch size to {batch_size}\")\n",
    "                else:\n",
    "                    print(\"Batch size already at minimum. Skipping this batch.\")\n",
    "                    continue\n",
    "            elif \"does not require grad\" in str(e).lower():\n",
    "                print(f\"Warning: Gradient error, skipping this batch: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error in training step: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                # Clear memory on any error\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training step: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Clear memory on any error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        if training_complete:\n",
    "            break\n",
    "\n",
    "    # CRITICAL: Check before starting new epoch\n",
    "    if training_complete or step >= max_steps:\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final step count: {step}/{max_steps}\")\n",
    "print(f\"Total epochs completed: {epoch}\")\n",
    "print(f\"Total batches processed: {batch_count}\")\n",
    "print(f\"Total training loss values recorded: {len(train_losses)}\")\n",
    "print(f\"Total validation runs: {len(val_losses)}\")\n",
    "if len(train_losses) > 0:\n",
    "    print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "if len(val_losses) > 0:\n",
    "    print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Final memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "print(f\"Saving model to: {final_model_path}\")\n",
    "\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {final_model_path}\")\n",
    "print(\"\\nNOTE: This training loop uses real VITS loss calculation (waveform L1 + mel-spectrogram L1).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualizations\n",
    "\n",
    "Visualize training progress, validation metrics, and model performance.\n",
    "\n",
    "**Note**: Since MMS-TTS uses a custom training loop, losses need to be tracked manually during training. \n",
    "The visualization code below assumes losses are stored in lists: `train_losses`, `val_losses`, `train_steps`, `val_steps`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# EMA smoothing function\n",
    "def apply_ema(values, alpha=0.3):\n",
    "    \"\"\"Apply Exponential Moving Average smoothing to a list of values\"\"\"\n",
    "    if not values or len(values) == 0:\n",
    "        return []\n",
    "    smoothed = [values[0]]\n",
    "    for val in values[1:]:\n",
    "        smoothed.append(alpha * val + (1 - alpha) * smoothed[-1])\n",
    "    return smoothed\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Initialize train_losses_vits if it doesn't exist\n",
    "if 'train_losses_vits' not in globals():\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Apply EMA smoothing to VITS training loss\n",
    "train_losses_vits_smoothed = []\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    train_losses_vits_smoothed = apply_ema(train_losses_vits, alpha=0.3)\n",
    "\n",
    "# Safety check: Ensure all lists have the same length for plotting\n",
    "min_len = min(\n",
    "    len(train_steps) if train_steps else float('inf'),\n",
    "    len(train_losses) if train_losses else float('inf'),\n",
    "    len(train_losses_vits) if train_losses_vits else float('inf')\n",
    ")\n",
    "if min_len != float('inf') and min_len > 0:\n",
    "    train_steps = train_steps[:min_len]\n",
    "    train_losses = train_losses[:min_len]\n",
    "    train_losses_vits = train_losses_vits[:min_len]\n",
    "    if len(train_losses_vits_smoothed) > min_len:\n",
    "        train_losses_vits_smoothed = train_losses_vits_smoothed[:min_len]\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss Over Time (top, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if train_losses_vits and train_steps and len(train_losses_vits) == len(train_steps):\n",
    "    # Plot smoothed VITS loss as primary (thick green solid line)\n",
    "    if train_losses_vits_smoothed and len(train_losses_vits_smoothed) == len(train_steps):\n",
    "        ax1.plot(train_steps, train_losses_vits_smoothed, 'g-', linewidth=3, label='Training Loss (VITS, Smoothed)', alpha=0.9)\n",
    "    # Plot raw VITS loss (thinner dashed green line)\n",
    "    ax1.plot(train_steps, train_losses_vits, 'g--', linewidth=1.5, label='Training Loss (VITS, Raw)', alpha=0.6)\n",
    "    # Plot optimized loss (very thin dashed blue line)\n",
    "    if train_losses and len(train_losses) == len(train_steps):\n",
    "        ax1.plot(train_steps, train_losses, 'b--', linewidth=1, label='Training Loss (Optimized)', alpha=0.4)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=11, loc='best')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No training data available\\nPlease track losses during training', ha='center', va='center', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if val_losses and val_steps and len(val_losses) == len(val_steps):\n",
    "    ax2.plot(val_steps, val_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Combined Loss Plot (Training vs Validation)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if train_losses_vits_smoothed and train_steps and len(train_losses_vits_smoothed) == len(train_steps):\n",
    "    # Prioritize smoothed VITS loss for comparison with validation\n",
    "    ax3.plot(train_steps, train_losses_vits_smoothed, 'g-', linewidth=2.5, label='Training Loss (VITS, Smoothed)', alpha=0.9)\n",
    "    # Show raw VITS loss as secondary line\n",
    "    if train_losses_vits and len(train_losses_vits) == len(train_steps):\n",
    "        ax3.plot(train_steps, train_losses_vits, 'g--', linewidth=1, label='Training Loss (VITS, Raw)', alpha=0.5)\n",
    "    # Show optimized loss as secondary line\n",
    "    if train_losses and len(train_losses) == len(train_steps):\n",
    "        ax3.plot(train_steps, train_losses, 'b--', linewidth=0.8, label='Training Loss (Optimized)', alpha=0.4)\n",
    "    # Add validation loss\n",
    "    if val_losses and val_steps and len(val_losses) == len(val_steps):\n",
    "        ax3.plot(val_steps, val_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=10, loc='best')\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No training data', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Training Statistics Summary\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    summary_text = f\"Training Summary: {OUTPUT_DIR}\\n\"\n",
    "    summary_text += f\"Training Loss (VITS): Initial={train_losses_vits[0]:.6f}, Final={train_losses_vits[-1]:.6f}, Best={min(train_losses_vits):.6f}\\n\"\n",
    "    if train_losses and len(train_losses) > 0:\n",
    "        summary_text += f\"Training Loss (Optimized): Initial={train_losses[0]:.6f}, Final={train_losses[-1]:.6f}, Best={min(train_losses):.6f}\\n\"\n",
    "    if val_losses and len(val_losses) > 0:\n",
    "        summary_text += f\"Validation Loss: Initial={val_losses[0]:.6f}, Final={val_losses[-1]:.6f}, Best={min(val_losses):.6f}\"\n",
    "    ax4.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=12, family='monospace')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No training statistics available', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: For epoch-based plot, you need to track losses by epoch during training\n",
    "# Group losses by epoch if available\n",
    "if train_losses and train_steps:\n",
    "    # For simplicity, assume each step is roughly an epoch (adjust as needed)\n",
    "    # In real training, you'd group by actual epoch number\n",
    "    epochs = list(range(len(train_losses)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "    if val_losses:\n",
    "        val_epochs = list(range(len(val_losses)))\n",
    "        plt.plot(val_epochs, val_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Epoch-based loss plot saved to: {plot_path}\")\n",
    "else:\n",
    "    print(\"Warning: No training losses available. Please track losses during training.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model (Kaggle Output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory (saved to Kaggle working directory)\n",
    "zip_filename = f\"/kaggle/working/{Path(final_model_path).name}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\" Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "print(f\"File size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(\"\\nModel and zip file saved to Kaggle working directory (persistent)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Inference (After Training)\n",
    "\n",
    "Test the fine-tuned model with romanized text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for testing\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "\n",
    "print(f\"Loading fine-tuned model from: {final_model_path}\")\n",
    "test_model = VitsModel.from_pretrained(final_model_path)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    " test_model = test_model.to(\"cuda\")\n",
    " test_model.eval()\n",
    "\n",
    "# Test with sample text\n",
    "if len(test_data) > 0 and 'transcription_romanized' in test_data[0]:\n",
    " test_text_romanized = test_data[0]['transcription_romanized']\n",
    " test_output = \"test_synthesized_after_training.wav\"\n",
    "\n",
    " print(f\"\\nTesting synthesis with: {test_text_romanized}\")\n",
    " synthesize_speech(test_model, test_tokenizer, test_text_romanized, test_output)\n",
    " print(f\"Test audio saved to: {test_output}\")\n",
    "else:\n",
    " print(\"Please run romanization cells first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# EMA smoothing function (if not already defined)\n",
    "def apply_ema(values, alpha=0.3):\n",
    "    \"\"\"Apply Exponential Moving Average smoothing to a list of values\"\"\"\n",
    "    if not values or len(values) == 0:\n",
    "        return []\n",
    "    smoothed = [values[0]]\n",
    "    for val in values[1:]:\n",
    "        smoothed.append(alpha * val + (1 - alpha) * smoothed[-1])\n",
    "    return smoothed\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Initialize train_losses_vits if it doesn't exist\n",
    "if 'train_losses_vits' not in globals():\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Apply EMA smoothing to VITS training loss\n",
    "train_losses_vits_smoothed = []\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    train_losses_vits_smoothed = apply_ema(train_losses_vits, alpha=0.3)\n",
    "\n",
    "# Safety check: Ensure all lists have the same length for plotting\n",
    "min_len = min(\n",
    "    len(train_steps) if train_steps else float('inf'),\n",
    "    len(train_losses) if train_losses else float('inf'),\n",
    "    len(train_losses_vits) if train_losses_vits else float('inf')\n",
    ")\n",
    "if min_len != float('inf') and min_len > 0:\n",
    "    train_steps = train_steps[:min_len]\n",
    "    train_losses = train_losses[:min_len]\n",
    "    train_losses_vits = train_losses_vits[:min_len]\n",
    "    if len(train_losses_vits_smoothed) > min_len:\n",
    "        train_losses_vits_smoothed = train_losses_vits_smoothed[:min_len]\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if train_losses_vits and train_steps and len(train_losses_vits) == len(train_steps):\n",
    "    # Plot smoothed VITS loss as primary (thick green solid line)\n",
    "    if train_losses_vits_smoothed and len(train_losses_vits_smoothed) == len(train_steps):\n",
    "        ax1.plot(train_steps, train_losses_vits_smoothed, 'g-', linewidth=3, label='Training Loss (VITS, Smoothed)', alpha=0.9)\n",
    "    # Plot raw VITS loss (thinner dashed green line)\n",
    "    ax1.plot(train_steps, train_losses_vits, 'g--', linewidth=1.5, label='Training Loss (VITS, Raw)', alpha=0.6)\n",
    "    # Plot optimized loss (very thin dashed blue line)\n",
    "    if train_losses and len(train_losses) == len(train_steps):\n",
    "        ax1.plot(train_steps, train_losses, 'b--', linewidth=1, label='Training Loss (Optimized)', alpha=0.4)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=11, loc='best')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No training data available\\nPlease track losses during training', ha='center', va='center', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if val_losses and val_steps and len(val_losses) == len(val_steps):\n",
    "    ax2.plot(val_steps, val_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Combined Loss Plot (Training vs Validation)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if train_losses_vits_smoothed and train_steps and len(train_losses_vits_smoothed) == len(train_steps):\n",
    "    # Prioritize smoothed VITS loss for comparison with validation\n",
    "    ax3.plot(train_steps, train_losses_vits_smoothed, 'g-', linewidth=2.5, label='Training Loss (VITS, Smoothed)', alpha=0.9)\n",
    "    # Show raw VITS loss as secondary line\n",
    "    if train_losses_vits and len(train_losses_vits) == len(train_steps):\n",
    "        ax3.plot(train_steps, train_losses_vits, 'g--', linewidth=1, label='Training Loss (VITS, Raw)', alpha=0.5)\n",
    "    # Show optimized loss as secondary line\n",
    "    if train_losses and len(train_losses) == len(train_steps):\n",
    "        ax3.plot(train_steps, train_losses, 'b--', linewidth=0.8, label='Training Loss (Optimized)', alpha=0.4)\n",
    "    # Add validation loss\n",
    "    if val_losses and val_steps and len(val_losses) == len(val_steps):\n",
    "        ax3.plot(val_steps, val_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=10, loc='best')\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No training data', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Training Statistics Summary\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    summary_text = f\"Training Summary: {OUTPUT_DIR}\\n\"\n",
    "    summary_text += f\"Training Loss (VITS): Initial={train_losses_vits[0]:.6f}, Final={train_losses_vits[-1]:.6f}, Best={min(train_losses_vits):.6f}\\n\"\n",
    "    if train_losses and len(train_losses) > 0:\n",
    "        summary_text += f\"Training Loss (Optimized): Initial={train_losses[0]:.6f}, Final={train_losses[-1]:.6f}, Best={min(train_losses):.6f}\\n\"\n",
    "    if val_losses and len(val_losses) > 0:\n",
    "        summary_text += f\"Validation Loss: Initial={val_losses[0]:.6f}, Final={val_losses[-1]:.6f}, Best={min(val_losses):.6f}\"\n",
    "    ax4.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=12, family='monospace')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No training statistics available', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING SUMMARY: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    print(f\"\\nTraining Loss (VITS):\")\n",
    "    print(f\" Initial: {train_losses_vits[0]:.6f}\")\n",
    "    print(f\" Final: {train_losses_vits[-1]:.6f}\")\n",
    "    print(f\" Best: {min(train_losses_vits):.6f} (at step {train_steps[train_losses_vits.index(min(train_losses_vits))]})\")\n",
    "    print(f\" Improvement: {((train_losses_vits[0] - min(train_losses_vits)) / train_losses_vits[0] * 100):.2f}%\")\n",
    "if train_losses and len(train_losses) > 0:\n",
    "    print(f\"\\nTraining Loss (Optimized):\")\n",
    "    print(f\" Initial: {train_losses[0]:.6f}\")\n",
    "    print(f\" Final: {train_losses[-1]:.6f}\")\n",
    "    print(f\" Best: {min(train_losses):.6f} (at step {train_steps[train_losses.index(min(train_losses))]})\")\n",
    "if val_losses and len(val_losses) > 0:\n",
    "    print(f\"\\nValidation Loss:\")\n",
    "    print(f\" Initial: {val_losses[0]:.6f}\")\n",
    "    print(f\" Final: {val_losses[-1]:.6f}\")\n",
    "    print(f\" Best: {min(val_losses):.6f} (at step {val_steps[val_losses.index(min(val_losses))]})\")\n",
    "    print(f\" Improvement: {((val_losses[0] - min(val_losses)) / val_losses[0] * 100):.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# EMA smoothing function (if not already defined)\n",
    "def apply_ema(values, alpha=0.3):\n",
    "    \"\"\"Apply Exponential Moving Average smoothing to a list of values\"\"\"\n",
    "    if not values or len(values) == 0:\n",
    "        return []\n",
    "    smoothed = [values[0]]\n",
    "    for val in values[1:]:\n",
    "        smoothed.append(alpha * val + (1 - alpha) * smoothed[-1])\n",
    "    return smoothed\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Initialize train_losses_vits if it doesn't exist\n",
    "if 'train_losses_vits' not in globals():\n",
    "    train_losses_vits = []\n",
    "\n",
    "# Apply EMA smoothing to VITS training loss\n",
    "train_losses_vits_smoothed = []\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    train_losses_vits_smoothed = apply_ema(train_losses_vits, alpha=0.3)\n",
    "\n",
    "# Safety check: Ensure all lists have the same length for plotting\n",
    "min_len = min(\n",
    "    len(train_losses) if train_losses else float('inf'),\n",
    "    len(train_losses_vits) if train_losses_vits else float('inf')\n",
    ")\n",
    "if min_len != float('inf') and min_len > 0:\n",
    "    train_losses = train_losses[:min_len]\n",
    "    train_losses_vits = train_losses_vits[:min_len]\n",
    "    if len(train_losses_vits_smoothed) > min_len:\n",
    "        train_losses_vits_smoothed = train_losses_vits_smoothed[:min_len]\n",
    "\n",
    "# For epoch-based plot, we'll use steps as approximate epochs\n",
    "if train_losses_vits and len(train_losses_vits) > 0:\n",
    "    epochs = list(range(len(train_losses_vits)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Plot smoothed VITS loss as primary\n",
    "    if train_losses_vits_smoothed and len(train_losses_vits_smoothed) == len(epochs):\n",
    "        plt.plot(epochs, train_losses_vits_smoothed, 'g-', linewidth=2.5, label='Training Loss (VITS, Smoothed)', alpha=0.9)\n",
    "    # Plot raw VITS loss\n",
    "    plt.plot(epochs, train_losses_vits, 'g--', linewidth=1.5, label='Training Loss (VITS, Raw)', alpha=0.7)\n",
    "    # Plot optimized loss\n",
    "    if train_losses and len(train_losses) == len(epochs):\n",
    "        plt.plot(epochs, train_losses, 'b--', linewidth=1, label='Training Loss (Optimized)', alpha=0.5)\n",
    "    # Add validation loss\n",
    "    if val_losses and len(val_losses) > 0:\n",
    "        val_epochs = list(range(len(val_losses)))\n",
    "        plt.plot(val_epochs, val_losses, 'orange', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Steps (Epochs)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Epoch-based loss plot saved to: {plot_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"EPOCH-BASED SUMMARY: {OUTPUT_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "    if train_losses_vits and len(train_losses_vits) > 0:\n",
    "        print(f\"\\nTraining Loss (VITS):\")\n",
    "        print(f\" Initial: {train_losses_vits[0]:.6f}\")\n",
    "        print(f\" Final: {train_losses_vits[-1]:.6f}\")\n",
    "        print(f\" Best: {min(train_losses_vits):.6f}\")\n",
    "    if train_losses and len(train_losses) > 0:\n",
    "        print(f\"\\nTraining Loss (Optimized):\")\n",
    "        print(f\" Initial: {train_losses[0]:.6f}\")\n",
    "        print(f\" Final: {train_losses[-1]:.6f}\")\n",
    "    if val_losses and len(val_losses) > 0:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\" Initial: {val_losses[0]:.6f}\")\n",
    "        print(f\" Final: {val_losses[-1]:.6f}\")\n",
    "        print(f\" Best: {min(val_losses):.6f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"Warning: No training losses available. Please track losses during training.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary Table\n",
    "\n",
    "Display training metrics in a table format similar to HuggingFace Trainer output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of training metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Collect metrics at eval_steps intervals\n",
    "eval_steps_list = TRAINING_ARGS.get('eval_steps', 500)\n",
    "\n",
    "# Get training losses at eval steps\n",
    "summary_data = []\n",
    "for val_step in val_steps:\n",
    "    # Find closest training loss\n",
    "    train_loss_at_step = None\n",
    "    for i, ts in enumerate(train_steps):\n",
    "        if ts >= val_step:\n",
    "            train_loss_at_step = train_losses[i] if i < len(train_losses) else None\n",
    "            break\n",
    "    \n",
    "    if train_loss_at_step is not None and val_step in val_steps:\n",
    "        val_idx = val_steps.index(val_step)\n",
    "        val_loss = val_losses[val_idx] if val_idx < len(val_losses) else 0.0\n",
    "        summary_data.append({\n",
    "            'Step': val_step,\n",
    "            'Training Loss': train_loss_at_step,\n",
    "            'Validation Loss': val_loss\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Summary\")\n",
    "    print(\"=\"*70)\n",
    "    # Format to match HuggingFace Trainer output style\n",
    "    print(f\"{'Step':<10} {'Training Loss':<15} {'Validation Loss':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for row in summary_data:\n",
    "        print(f\"{row['Step']:<10} {row['Training Loss']:<15.6f} {row['Validation Loss']:<15.6f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No validation metrics available yet.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
