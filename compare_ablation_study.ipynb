{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Ablation Study: Model Comparison\n",
    "\n",
    "Compare all 4 ablation variants side-by-side:\n",
    "- **V0**: CTC-only baseline (no LoRA)\n",
    "- **V1**: LoRA base (r=8, alpha=16)\n",
    "- **V2**: LoRA higher capacity (r=16, alpha=32)\n",
    "- **V3**: LoRA lower capacity (r=4, alpha=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_code"
   },
   "outputs": [],
   "source": [
    "%pip install -q matplotlib numpy pandas seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## Load Training Results\n",
    "\n",
    "Load training histories from all 4 notebooks. Make sure you've run all training notebooks first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data_code"
   },
   "outputs": [],
   "source": [
    "# Define model variants\n",
    "variants = {\n",
    "    'V0_CTC_Only': {\n",
    "        'output_dir': 'wav2vec2_ctc_only_baseline',\n",
    "        'label': 'V0: CTC-Only',\n",
    "        'color': '#1f77b4'\n",
    "    },\n",
    "    'V1_Base': {\n",
    "        'output_dir': 'wav2vec2_lora_v1',\n",
    "        'label': 'V1: LoRA (r=8)',\n",
    "        'color': '#ff7f0e'\n",
    "    },\n",
    "    'V2_Higher': {\n",
    "        'output_dir': 'wav2vec2_lora_v2',\n",
    "        'label': 'V2: LoRA (r=16)',\n",
    "        'color': '#2ca02c'\n",
    "    },\n",
    "    'V3_Lower': {\n",
    "        'output_dir': 'wav2vec2_lora_v3',\n",
    "        'label': 'V3: LoRA (r=4)',\n",
    "        'color': '#d62728'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load training histories from trainer state files\n",
    "# Note: You need to save trainer.state.log_history to JSON files after training\n",
    "# Or load directly from trainer objects if available\n",
    "\n",
    "def load_training_history(output_dir):\n",
    "    \"\"\"Load training history from saved state\"\"\"\n",
    "    state_file = Path(f\"{output_dir}/trainer_state.json\")\n",
    "    if state_file.exists():\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            return state.get('log_history', [])\n",
    "    return None\n",
    "\n",
    "# Alternative: Load from trainer objects if they're still in memory\n",
    "# This assumes you've run all training notebooks and have trainer objects\n",
    "print(\"\\nTo use this notebook:\")\n",
    "print(\"1. Run all 4 training notebooks first\")\n",
    "print(\"2. Save trainer.state.log_history to JSON files, OR\")\n",
    "print(\"3. Load trainer objects directly from the training notebooks\")\n",
    "print(\"\\nFor now, this is a template. Update the data loading section based on your setup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_plots"
   },
   "source": [
    "## Comparison Visualizations\n",
    "\n",
    "Create side-by-side comparison plots for all variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_code"
   },
   "outputs": [],
   "source": [
    "# Example: Create comparison plots\n",
    "# This is a template - update with actual data\n",
    "\n",
    "def create_comparison_plots(all_histories):\n",
    "    \"\"\"Create comprehensive comparison plots\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.3)\n",
    "    \n",
    "    # Plot 1: Training Loss Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            train_logs = [log for log in history if 'loss' in log and 'eval_loss' not in log]\n",
    "            if train_logs:\n",
    "                steps = [log['step'] for log in train_logs]\n",
    "                losses = [log['loss'] for log in train_logs]\n",
    "                ax1.plot(steps, losses, label=variant_info['label'], \n",
    "                        linewidth=2.5, color=variant_info['color'], alpha=0.8)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=16, fontweight='bold', pad=15)\n",
    "    ax1.legend(fontsize=12, loc='best')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Plot 2: Validation Loss Comparison\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            eval_logs = [log for log in history if 'eval_loss' in log]\n",
    "            if eval_logs:\n",
    "                steps = [log['step'] for log in eval_logs]\n",
    "                losses = [log['eval_loss'] for log in eval_logs]\n",
    "                ax2.plot(steps, losses, label=variant_info['label'], \n",
    "                        linewidth=2.5, marker='o', markersize=4, \n",
    "                        color=variant_info['color'], alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Validation Loss', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss Comparison', fontsize=15, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Plot 3: WER Comparison\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            eval_logs = [log for log in history if 'eval_wer' in log]\n",
    "            if eval_logs:\n",
    "                steps = [log['step'] for log in eval_logs]\n",
    "                wers = [log['eval_wer'] for log in eval_logs]\n",
    "                ax3.plot(steps, wers, label=variant_info['label'], \n",
    "                        linewidth=2.5, marker='s', markersize=4, \n",
    "                        color=variant_info['color'], alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax3.set_ylabel('Word Error Rate', fontsize=13, fontweight='bold')\n",
    "    ax3.set_title('WER Comparison', fontsize=15, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.set_ylim(bottom=0)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Plot 4: CER Comparison\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            eval_logs = [log for log in history if 'eval_cer' in log]\n",
    "            if eval_logs:\n",
    "                steps = [log['step'] for log in eval_logs]\n",
    "                cers = [log['eval_cer'] for log in eval_logs]\n",
    "                ax4.plot(steps, cers, label=variant_info['label'], \n",
    "                        linewidth=2.5, marker='^', markersize=4, \n",
    "                        color=variant_info['color'], alpha=0.8)\n",
    "    ax4.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax4.set_ylabel('Character Error Rate', fontsize=13, fontweight='bold')\n",
    "    ax4.set_title('CER Comparison', fontsize=15, fontweight='bold')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax4.set_ylim(bottom=0)\n",
    "    ax4.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Plot 5: Final Metrics Bar Chart\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    final_metrics = {}\n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            eval_logs = [log for log in history if 'eval_wer' in log]\n",
    "            if eval_logs:\n",
    "                final_metrics[variant_info['label']] = eval_logs[-1]['eval_wer']\n",
    "    \n",
    "    if final_metrics:\n",
    "        labels = list(final_metrics.keys())\n",
    "        values = list(final_metrics.values())\n",
    "        colors = [variants[k]['color'] for k in variants.keys() if variants[k]['label'] in labels]\n",
    "        bars = ax5.bar(labels, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax5.set_ylabel('Final WER', fontsize=13, fontweight='bold')\n",
    "        ax5.set_title('Final WER Comparison', fontsize=15, fontweight='bold')\n",
    "        ax5.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "        ax5.set_facecolor('#f8f9fa')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    fig.suptitle('Ablation Study: Model Comparison', fontsize=20, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.show()\n",
    "    \n",
    "    # Save plot\n",
    "    fig.savefig('ablation_study_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(\"\\nComparison plot saved to: ablation_study_comparison.png\")\n",
    "\n",
    "# Example usage (update with actual data):\n",
    "# all_histories = {\n",
    "#     'V0_CTC_Only': load_training_history('wav2vec2_ctc_only_baseline'),\n",
    "#     'V1_Base': load_training_history('wav2vec2_lora_v1'),\n",
    "#     'V2_Higher': load_training_history('wav2vec2_lora_v2'),\n",
    "#     'V3_Lower': load_training_history('wav2vec2_lora_v3')\n",
    "# }\n",
    "# create_comparison_plots(all_histories)\n",
    "\n",
    "print(\"Comparison plotting function ready. Load your training histories and call create_comparison_plots()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_table"
   },
   "source": [
    "## Summary Table\n",
    "\n",
    "Create a summary table comparing final metrics across all variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary_table_code"
   },
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "def create_summary_table(all_histories):\n",
    "    \"\"\"Create a summary table of final metrics\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for variant_name, variant_info in variants.items():\n",
    "        if variant_name in all_histories:\n",
    "            history = all_histories[variant_name]\n",
    "            \n",
    "            # Get final metrics\n",
    "            train_logs = [log for log in history if 'loss' in log and 'eval_loss' not in log]\n",
    "            eval_logs = [log for log in history if 'eval_loss' in log]\n",
    "            \n",
    "            row = {\n",
    "                'Variant': variant_info['label'],\n",
    "                'Final Train Loss': train_logs[-1]['loss'] if train_logs else None,\n",
    "                'Final Val Loss': eval_logs[-1]['eval_loss'] if eval_logs else None,\n",
    "                'Final WER': eval_logs[-1].get('eval_wer', None) if eval_logs else None,\n",
    "                'Final CER': eval_logs[-1].get('eval_cer', None) if eval_logs else None,\n",
    "            }\n",
    "            \n",
    "            # Get best metrics\n",
    "            if eval_logs:\n",
    "                val_losses = [log['eval_loss'] for log in eval_logs]\n",
    "                wers = [log.get('eval_wer', float('inf')) for log in eval_logs if log.get('eval_wer', 0) > 0]\n",
    "                cers = [log.get('eval_cer', float('inf')) for log in eval_logs if log.get('eval_cer', 0) > 0]\n",
    "                \n",
    "                row['Best Val Loss'] = min(val_losses) if val_losses else None\n",
    "                row['Best WER'] = min(wers) if wers else None\n",
    "                row['Best CER'] = min(cers) if cers else None\n",
    "            \n",
    "            summary_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display formatted table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ABLATION STUDY SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('ablation_study_summary.csv', index=False)\n",
    "    print(\"\\nSummary saved to: ablation_study_summary.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# summary_df = create_summary_table(all_histories)\n",
    "\n",
    "print(\"Summary table function ready. Load your training histories and call create_summary_table()\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
