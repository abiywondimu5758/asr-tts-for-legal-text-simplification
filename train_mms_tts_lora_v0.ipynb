{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation V0: LoRA Minimal Baseline (r=16, alpha=32)\n",
    "\n",
    "This notebook is part of an ablation study to compare different fine-tuning approaches.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `facebook/mms-tts-amh` (VITS architecture, ~36.3M parameters)\n",
    "- **Fine-tuning Method**: LoRA (r=16, alpha=32)\n",
    "- **Task**: Text-to-Speech (TTS)\n",
    "- **Domain**: Legal Amharic text\n",
    "- **Dataset**: Dataset_4.0h\n",
    "- **Text Format**: Romanized Amharic (using uroman package)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa soundfile uroman scipy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    " VitsModel,\n",
    " AutoTokenizer,\n",
    " TrainingArguments,\n",
    " Trainer\n",
    ")\n",
    "\n",
    "from peft import (\n",
    " LoraConfig,\n",
    " get_peft_model,\n",
    " TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import uroman for romanization\n",
    "import uroman\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    " print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    " print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    VitsModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import uroman for romanization\n",
    "import uroman\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/mms-tts-amh\"\n",
    "\n",
    "# Dataset paths - using Dataset_4.0h\n",
    "AUDIO_DIR = \"/content/drive/MyDrive/Dataset_4.0h/audio\"\n",
    "TRAIN_CSV = \"/content/drive/MyDrive/Dataset_4.0h/train.csv\"\n",
    "VAL_CSV = \"/content/drive/MyDrive/Dataset_4.0h/val.csv\"\n",
    "TEST_CSV = \"/content/drive/MyDrive/Dataset_4.0h/test.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"mms_tts_lora_v0\"\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_CONFIG = {\n",
    " \"r\": 16,\n",
    " \"lora_alpha\": 32,\n",
    " \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    " \"lora_dropout\": 0.1,\n",
    " \"bias\": \"none\",\n",
    " \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_ARGS = {\n",
    " \"output_dir\": OUTPUT_DIR,\n",
    " \"per_device_train_batch_size\": 2, # Reduced for T4 GPU\n",
    " \"per_device_eval_batch_size\": 2,\n",
    " \"gradient_accumulation_steps\": 8, # Effective batch size 16\n",
    " \"learning_rate\": 5e-5,\n",
    " \"warmup_steps\": 100,\n",
    " \"max_steps\": 3000, # Increased for larger dataset\n",
    " \"gradient_checkpointing\": True,\n",
    " \"fp16\": True,\n",
    " \"eval_strategy\": \"steps\",\n",
    " \"eval_steps\": 500,\n",
    " \"save_strategy\": \"steps\",\n",
    " \"save_steps\": 500,\n",
    " \"save_total_limit\": 3,\n",
    " \"load_best_model_at_end\": True,\n",
    " \"metric_for_best_model\": \"loss\",\n",
    " \"greater_is_better\": False,\n",
    " \"logging_steps\": 100,\n",
    " \"report_to\": \"none\",\n",
    " \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Output directory: {OUTPUT_DIR}\")\n",
    "print(f\" LoRA rank (r): {LORA_CONFIG['r']}\")\n",
    "print(f\" LoRA alpha: {LORA_CONFIG['lora_alpha']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Romanize Text Using Uroman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize uroman instance\n",
    "try:\n",
    "    uroman_obj = uroman.Uroman()\n",
    "    print(\"Uroman initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing uroman: {e}\")\n",
    "    uroman_obj = None\n",
    "\n",
    "def romanize_text(text):\n",
    "    \"\"\"Convert Ge'ez script Amharic text to Romanized format using uroman\"\"\"\n",
    "    if uroman_obj is None:\n",
    "        return text # Return original if uroman not available\n",
    "\n",
    "    try:\n",
    "        # Uroman.romanize_string() is the correct method\n",
    "        romanized = uroman_obj.romanize_string(text)\n",
    "        return romanized.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error romanizing text: {text[:50]}... Error: {e}\")\n",
    "        return text # Fallback to original if romanization fails\n",
    "\n",
    "# Test romanization on a sample\n",
    "sample_text = train_data[0]['transcription']\n",
    "print(f\"Original (Ge'ez): {sample_text}\")\n",
    "romanized_sample = romanize_text(sample_text)\n",
    "print(f\"Romanized: {romanized_sample}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize uroman instance\n",
    "try:\n",
    "    uroman_obj = uroman.Uroman()\n",
    "    print(\"Uroman initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing uroman: {e}\")\n",
    "    uroman_obj = None\n",
    "\n",
    "def romanize_text(text):\n",
    "    \"\"\"Convert Ge'ez script Amharic text to Romanized format using uroman\"\"\"\n",
    "    if uroman_obj is None:\n",
    "        return text # Return original if uroman not available\n",
    "\n",
    "    try:\n",
    "        # Uroman.romanize_string() is the correct method\n",
    "        romanized = uroman_obj.romanize_string(text)\n",
    "        return romanized.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error romanizing text: {text[:50]}... Error: {e}\")\n",
    "        return text # Fallback to original if romanization fails\n",
    "\n",
    "# Test romanization on a sample\n",
    "sample_text = train_data[0]['transcription']\n",
    "print(f\"Original (Ge'ez): {sample_text}\")\n",
    "romanized_sample = romanize_text(sample_text)\n",
    "print(f\"Romanized: {romanized_sample}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = VitsModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Sampling rate: {model.config.sampling_rate} Hz\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    " model = model.to(\"cuda\")\n",
    " print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference (Before Training)\n",
    "\n",
    "Test the model with romanized text to verify it works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_speech(model, tokenizer, text_romanized, output_path):\n",
    "    \"\"\"Synthesize speech from romanized text\"\"\"\n",
    "    inputs = tokenizer(text_romanized, return_tensors=\"pt\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).waveform\n",
    "\n",
    "    # Move tensor to CPU before converting to numpy\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.cpu()\n",
    "\n",
    "    # Save audio\n",
    "    scipy.io.wavfile.write(\n",
    "        output_path,\n",
    "        rate=model.config.sampling_rate,\n",
    "        data=output.numpy().T\n",
    "    )\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Test with sample text (after romanization)\n",
    "if len(train_data) > 0 and 'transcription_romanized' in train_data[0]:\n",
    "    test_text_romanized = train_data[0]['transcription_romanized']\n",
    "    test_output = \"test_synthesized_before_training.wav\"\n",
    "\n",
    "    print(f\"Testing synthesis with: {test_text_romanized}\")\n",
    "    synthesize_speech(model, tokenizer, test_text_romanized, test_output)\n",
    "    print(f\"Test audio saved to: {test_output}\")\n",
    "else:\n",
    "    print(\"Please run romanization cells first!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply LoRA (if supported)\n",
    "\n",
    "**Important Note**: VITS models use a different architecture than transformers. LoRA support may be limited. We'll attempt to apply it, but may need to use standard fine-tuning if LoRA is not compatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to apply LoRA\n",
    "# Note: This may fail for VITS models - if so, we'll need to use standard fine-tuning\n",
    "try:\n",
    "    # Check available modules in the model\n",
    "    print(\"Checking model structure for LoRA-compatible modules...\")\n",
    "    target_modules_found = []\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        if any(target in name.lower() for target in [\"linear\", \"proj\", \"dense\"]):\n",
    "            if \"Linear\" in module_type or \"Conv\" in module_type:\n",
    "                target_modules_found.append(name)\n",
    "\n",
    "    print(f\"Found {len(target_modules_found)} potential target modules\")\n",
    "    if len(target_modules_found) > 0:\n",
    "        print(\"Sample modules:\", target_modules_found[:5])\n",
    "\n",
    "    # Try to configure LoRA with found modules\n",
    "    # VITS models may not have standard transformer modules\n",
    "    # We may need to identify the correct target modules\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_CONFIG[\"r\"],\n",
    "        lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "        target_modules=LORA_CONFIG[\"target_modules\"], # Try standard modules first\n",
    "        lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "        bias=LORA_CONFIG[\"bias\"],\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"\\nLoRA adapters applied successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LoRA application failed: {e}\")\n",
    "    print(\"\\nNote: VITS models may not support standard LoRA.\")\n",
    "    print(\"We may need to use standard fine-tuning instead.\")\n",
    "    print(\"Continuing with standard model for now...\")\n",
    "\n",
    "# Remove any existing wrapper code (safety check)\n",
    "# If you have any wrapper code, remove it first before running this\n",
    "\n",
    "# Check if LoRA was successfully applied\n",
    "from peft import PeftModel\n",
    "\n",
    "if isinstance(model, PeftModel):\n",
    "    print(\"Model is a PeftModel\")\n",
    "    print(f\"Base model type: {type(model.base_model)}\")\n",
    "    print(f\"Has base_model.forward: {hasattr(model.base_model, 'forward')}\")\n",
    "\n",
    "    # Try to check if we can access the underlying VITS model\n",
    "    if hasattr(model.base_model, 'model'):\n",
    "        print(f\"Base model has 'model' attribute: {type(model.base_model.model)}\")\n",
    "else:\n",
    "    print(\"Model is NOT a PeftModel - LoRA may not have been applied\")\n",
    "\n",
    "# Fix: Patch PEFT's forward to filter out inputs_embeds (VitsModel doesn't accept it)\n",
    "# This is necessary because PEFT includes inputs_embeds in its forward signature,\n",
    "# but VitsModel doesn't accept it\n",
    "if isinstance(model, PeftModel):\n",
    "    import functools\n",
    "    \n",
    "    # Store the original forward method\n",
    "    original_peft_forward = model.forward\n",
    "    \n",
    "    @functools.wraps(original_peft_forward)\n",
    "    def patched_peft_forward(*args, **kwargs):\n",
    "        # Remove inputs_embeds if present (VitsModel doesn't accept it)\n",
    "        kwargs.pop('inputs_embeds', None)\n",
    "        # Also remove task_ids which PEFT might pass\n",
    "        kwargs.pop('task_ids', None)\n",
    "        # Filter to only valid VitsModel arguments\n",
    "        valid_kwargs = {k: v for k, v in kwargs.items() \n",
    "                        if k in ['input_ids', 'attention_mask', 'speaker_id', \n",
    "                                'output_attentions', 'output_hidden_states', 'return_dict', 'labels']}\n",
    "        # Call the original forward with filtered kwargs\n",
    "        return original_peft_forward(*args, **valid_kwargs)\n",
    "    \n",
    "    # Replace the forward method\n",
    "    model.forward = patched_peft_forward\n",
    "    \n",
    "    # Also patch the base_model's forward to catch any internal calls\n",
    "    if hasattr(model, 'base_model'):\n",
    "        # Helper function to create a patched forward\n",
    "        def create_patched_forward(original_forward):\n",
    "            @functools.wraps(original_forward)\n",
    "            def patched_forward(*args, **kwargs):\n",
    "                kwargs.pop('inputs_embeds', None)\n",
    "                kwargs.pop('task_ids', None)\n",
    "                valid_kwargs = {k: v for k, v in kwargs.items() \n",
    "                                if k in ['input_ids', 'attention_mask', 'speaker_id', \n",
    "                                        'output_attentions', 'output_hidden_states', 'return_dict', 'labels']}\n",
    "                return original_forward(*args, **valid_kwargs)\n",
    "            return patched_forward\n",
    "        \n",
    "        # Recursively patch nested base_models\n",
    "        current = model.base_model\n",
    "        depth = 0\n",
    "        max_depth = 5 # Safety limit\n",
    "        \n",
    "        while depth < max_depth:\n",
    "            if hasattr(current, 'forward'):\n",
    "                current.forward = create_patched_forward(current.forward)\n",
    "            \n",
    "            # Move to next level\n",
    "            if hasattr(current, 'base_model'):\n",
    "                current = current.base_model\n",
    "            elif hasattr(current, 'model'):\n",
    "                current = current.model\n",
    "            else:\n",
    "                break\n",
    "            depth += 1\n",
    "        \n",
    "    print(\"Patched PEFT model forward to filter out unsupported arguments (inputs_embeds, etc.)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup\n",
    "\n",
    "**Important**: VITS models require custom training loops as they don't use the standard HuggingFace Trainer interface. This section provides a basic structure. For actual training, you may need to implement a custom training loop.\n",
    "\n",
    "**Alternative Approach**: If LoRA doesn't work, consider:\n",
    "1. Standard fine-tuning (full model fine-tuning)\n",
    "2. Using a different TTS model that supports LoRA better\n",
    "3. Implementing custom LoRA for VITS architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop for VITS model\n",
    "# Note: This is a simplified training loop - VITS training is complex\n",
    "# You may need to adapt this based on your specific requirements\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prepare datasets for training\n",
    "train_dataset_list = [\n",
    " {'text': item['transcription_romanized'], 'audio_path': item['audio_path']}\n",
    " for item in train_data\n",
    "]\n",
    "\n",
    "val_dataset_list = [\n",
    " {'text': item['transcription_romanized'], 'audio_path': item['audio_path']}\n",
    " for item in val_data\n",
    "]\n",
    "\n",
    "def load_audio(audio_path, target_sr=16000):\n",
    " \"\"\"Load and resample audio\"\"\"\n",
    " audio, sr = librosa.load(audio_path, sr=target_sr)\n",
    " return audio\n",
    "\n",
    "print(f\"Prepared {len(train_dataset_list)} training samples\")\n",
    "print(f\"Prepared {len(val_dataset_list)} validation samples\")\n",
    "print(\"\\nNOTE: Full VITS training requires complex loss functions (mel-spectrogram loss, etc.)\")\n",
    "print(\"This is a simplified structure. For production use, implement full VITS training objectives.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization setup before training\n",
    "import os\n",
    "\n",
    "# Set PyTorch CUDA memory allocation config to reduce fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Print current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(\"GPU Memory Status:\")\n",
    "    print(f\" Allocated: {allocated:.2f} GB\")\n",
    "    print(f\" Reserved: {reserved:.2f} GB\")\n",
    "    print(f\" Total: {total:.2f} GB\")\n",
    "    print(f\" Free: {total - reserved:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Check model memory usage\n",
    "    if hasattr(model, 'parameters'):\n",
    "        model_params = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "        print(f\"Model parameters memory: ~{model_params:.2f} GB (FP32)\")\n",
    "        if TRAINING_ARGS.get(\"fp16\", False):\n",
    "            print(f\" With FP16: ~{model_params / 2:.2f} GB\")\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model\n",
    "\n",
    "**Note**: This is a simplified training approach. Full VITS training requires complex loss functions. For production, you may need to implement proper VITS training objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking lists for training/validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_steps = []\n",
    "val_steps = []\n",
    "\n",
    "print('Initialized metric tracking lists: train_losses, val_losses, train_steps, val_steps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation evaluation function\n",
    "def evaluate_validation(model, tokenizer, val_dataset_list, batch_size=4, use_fp16=False):\n",
    "    \"\"\"Run validation evaluation on validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_dataset_list), batch_size):\n",
    "            batch = val_dataset_list[i:i+batch_size]\n",
    " \n",
    "            try:\n",
    "                # Tokenize batch text\n",
    "                texts = [item['text'] for item in batch]\n",
    "                inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    " \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    " \n",
    "                # Filter out inputs_embeds which VitsModel doesn't accept\n",
    "                valid_inputs = {k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    " \n",
    "                # Forward pass\n",
    "                if use_fp16:\n",
    "                    from torch.cuda.amp import autocast\n",
    "                    with autocast():\n",
    "                        outputs = model(**valid_inputs)\n",
    "                else:\n",
    "                    outputs = model(**valid_inputs)\n",
    " \n",
    "                # Compute loss (simplified - same as training)\n",
    "                loss = torch.tensor(0.0, device=inputs['input_ids'].device)\n",
    " \n",
    "                total_loss += loss.item() * len(batch)\n",
    "                num_samples += len(batch)\n",
    " \n",
    "                del outputs, inputs, valid_inputs\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    " \n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch: {e}\")\n",
    "                continue\n",
    " \n",
    "    model.train()\n",
    "    avg_val_loss = total_loss / num_samples if num_samples > 0 else 0.0\n",
    "    return avg_val_loss\n",
    "\n",
    "print(\"Validation evaluation function defined.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified training loop with MEMORY OPTIMIZATION\n",
    "# WARNING: This is a basic structure - full VITS training is more complex\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if TRAINING_ARGS.get(\"gradient_checkpointing\", False):\n",
    "    try:\n",
    "        if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\" Gradient checkpointing enabled\")\n",
    "        elif hasattr(model, 'base_model') and hasattr(model.base_model, 'gradient_checkpointing_enable'):\n",
    "            model.base_model.gradient_checkpointing_enable()\n",
    "            print(\" Gradient checkpointing enabled (via base_model)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not enable gradient checkpointing: {e}\")\n",
    "\n",
    "# Enable FP16 mixed precision training\n",
    "use_fp16 = TRAINING_ARGS.get(\"fp16\", False)\n",
    "if use_fp16:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    scaler = GradScaler()\n",
    "    print(\" FP16 mixed precision enabled\")\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=TRAINING_ARGS[\"learning_rate\"])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=TRAINING_ARGS[\"max_steps\"])\n",
    "\n",
    "# Reduce batch size if memory is tight (T4 15GB)\n",
    "batch_size = TRAINING_ARGS[\"per_device_train_batch_size\"]\n",
    "if batch_size > 2:\n",
    "    print(f\"Warning: Reducing batch size from {batch_size} to 2 for T4 GPU memory constraints\")\n",
    "    batch_size = 2\n",
    "\n",
    "gradient_accumulation_steps = TRAINING_ARGS[\"gradient_accumulation_steps\"]\n",
    "max_steps = TRAINING_ARGS[\"max_steps\"]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training steps: {max_steps}\")\n",
    "print(f\"Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"FP16: {use_fp16}\")\n",
    "print()\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print()\n",
    "\n",
    "step = 0\n",
    "training_complete = False\n",
    "epoch = 0\n",
    "accumulated_loss = 0.0\n",
    "\n",
    "while step < max_steps:\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Simple batching (you may want to implement proper DataLoader)\n",
    "    for i in tqdm(range(0, len(train_dataset_list), batch_size), desc=f\"Epoch {epoch+1}\"):\n",
    "        # CRITICAL: Check step before processing batch to stop at max_steps\n",
    "        if step >= max_steps:\n",
    "            training_complete = True\n",
    "            break\n",
    "\n",
    "        batch = train_dataset_list[i:i+batch_size]\n",
    "\n",
    "        # Clear gradients at the start of each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Simplified forward pass (actual VITS training needs proper loss)\n",
    "        try:\n",
    "            # Tokenize batch text\n",
    "            texts = [item['text'] for item in batch]\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "            # Forward pass with FP16 if enabled\n",
    "            # Filter out inputs_embeds which VitsModel doesn't accept\n",
    "            valid_inputs = {k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    " \n",
    "            # Use autocast for FP16\n",
    "            if use_fp16:\n",
    "                with autocast():\n",
    "                    outputs = model(**valid_inputs)\n",
    "            else:\n",
    "                outputs = model(**valid_inputs)\n",
    "\n",
    "            # PLACEHOLDER: Compute loss (VITS needs complex loss function)\n",
    "            # For now, we'll skip actual loss computation\n",
    "            # In real training, you'd compute mel-spectrogram loss, duration loss, etc.\n",
    "\n",
    "            # Dummy loss for demonstration (REPLACE with actual VITS loss)\n",
    "            # Use a simple tensor that doesn't require keeping outputs in memory\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=inputs['input_ids'].device)\n",
    "\n",
    "            # Delete outputs immediately to free memory\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            # Backward pass with FP16 scaling\n",
    "            if use_fp16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Delete loss and inputs to free memory\n",
    "            loss_value = loss.item()\n",
    "            # Track training loss and step (approximate global step)\n",
    "            train_losses.append(loss_value)\n",
    "            train_steps.append(step + 1)\n",
    "            del loss, inputs, valid_inputs\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            accumulated_loss += loss_value\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update weights after gradient accumulation\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                if use_fp16:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    " \n",
    "                scheduler.step()\n",
    "                step += 1\n",
    "\n",
    "                # CRITICAL: Check immediately after incrementing step\n",
    "                if step >= max_steps:\n",
    "                    training_complete = True\n",
    "                    break\n",
    "\n",
    "\n",
    "                if step % TRAINING_ARGS[\"logging_steps\"] == 0:\n",
    "                    avg_loss = accumulated_loss / (TRAINING_ARGS[\"logging_steps\"] * gradient_accumulation_steps)\n",
    "                    mem_used = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0\n",
    "                    mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "                    print(f\"Step {step}/{max_steps} - Loss: {avg_loss:.4f} - GPU Memory: {mem_used:.2f}/{mem_total:.2f} GB\")\n",
    "                    accumulated_loss = 0.0\n",
    " \n",
    "                # Periodic memory cleanup\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # Run actual validation at eval_steps\n",
    "                if step % TRAINING_ARGS['eval_steps'] == 0:\n",
    "                    print(f\"Running validation at step {step}...\")\n",
    "                    val_loss = evaluate_validation(model, tokenizer, val_dataset_list, batch_size=2, use_fp16=use_fp16)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_steps.append(step)\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "                    if step >= max_steps:\n",
    "                        training_complete = True\n",
    "                        break\n",
    "\n",
    "            if step >= max_steps:\n",
    "                training_complete = True\n",
    "                break\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"Warning: CUDA OOM at step {step}. Clearing cache and reducing batch size...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                # Try with smaller batch\n",
    "                if batch_size > 1:\n",
    "                    batch_size = 1\n",
    "                    print(f\"Reduced batch size to {batch_size}\")\n",
    "                else:\n",
    "                    print(\"Batch size already at minimum. Skipping this batch.\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Error in training step: {e}\")\n",
    "                # Clear memory on any error\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training step: {e}\")\n",
    "            # Clear memory on any error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    " \n",
    "        if training_complete:\n",
    "            break\n",
    "\n",
    "    # CRITICAL: Check before starting new epoch\n",
    "    if training_complete or step >= max_steps:\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Final memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "print(f\"Saving model to: {final_model_path}\")\n",
    "\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {final_model_path}\")\n",
    "print(\"\\nNOTE: This was a simplified training loop.\")\n",
    "print(\"For production use, implement proper VITS training objectives.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualizations\n",
    "\n",
    "Visualize training progress, validation metrics, and model performance.\n",
    "\n",
    "**Note**: Since MMS-TTS uses a custom training loop, losses need to be tracked manually during training. \n",
    "The visualization code below assumes losses are stored in lists: `train_losses`, `val_losses`, `train_steps`, `val_steps`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# NOTE: For MMS-TTS custom training loop, you need to track losses manually\n",
    "# During training, append to these lists:\n",
    "# train_losses.append(loss_value)\n",
    "# train_steps.append(step)\n",
    "# val_losses.append(val_loss_value) # during validation\n",
    "# val_steps.append(step) # during validation\n",
    "\n",
    "# If losses weren't tracked, create empty lists\n",
    "if 'train_losses' not in globals():\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "    print(\"Warning: Warning: Losses not tracked during training. Please modify the training loop to track losses.\")\n",
    "    print(\" Add: train_losses.append(loss_value) and train_steps.append(step) in the training loop\")\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if train_losses and train_steps:\n",
    "    ax1.plot(train_steps, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=12, loc='best')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No training data available\\nPlease track losses during training', ha='center', va='center', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if val_losses and val_steps:\n",
    "    ax2.plot(val_steps, val_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Combined Loss Plot\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if train_losses and train_steps:\n",
    "    ax3.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    if val_losses and val_steps:\n",
    "        ax3.plot(val_steps, val_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.7)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No training data', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Training Statistics Summary\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "if train_losses:\n",
    "    summary_text = f\"Training Summary: {OUTPUT_DIR}\\n\"\n",
    "    summary_text += f\"Training Loss: Initial={train_losses[0]:.4f}, Final={train_losses[-1]:.4f}, Best={min(train_losses):.4f}\\n\"\n",
    "    if val_losses:\n",
    "        summary_text += f\"Validation Loss: Initial={val_losses[0]:.4f}, Final={val_losses[-1]:.4f}, Best={min(val_losses):.4f}\"\n",
    "    ax4.text(0.5, 0.5, summary_text, ha='center', va='center', fontsize=12, family='monospace')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No training statistics available', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: For epoch-based plot, you need to track losses by epoch during training\n",
    "# Group losses by epoch if available\n",
    "if train_losses and train_steps:\n",
    "    # For simplicity, assume each step is roughly an epoch (adjust as needed)\n",
    "    # In real training, you'd group by actual epoch number\n",
    "    epochs = list(range(len(train_losses)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "    if val_losses:\n",
    "        val_epochs = list(range(len(val_losses)))\n",
    "        plt.plot(val_epochs, val_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Epoch-based loss plot saved to: {plot_path}\")\n",
    "else:\n",
    "    print(\"Warning: No training losses available. Please track losses during training.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Zip Model and Copy to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory (saved to Kaggle working directory)\n",
    "zip_filename = f\"{Path(final_model_path).name}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\" Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "print(f\"File size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(\"\\nModel and zip file saved to Kaggle working directory (persistent)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Inference (After Training)\n",
    "\n",
    "Test the fine-tuned model with romanized text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for testing\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "\n",
    "print(f\"Loading fine-tuned model from: {final_model_path}\")\n",
    "test_model = VitsModel.from_pretrained(final_model_path)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    " test_model = test_model.to(\"cuda\")\n",
    " test_model.eval()\n",
    "\n",
    "# Test with sample text\n",
    "if len(test_data) > 0 and 'transcription_romanized' in test_data[0]:\n",
    " test_text_romanized = test_data[0]['transcription_romanized']\n",
    " test_output = \"test_synthesized_after_training.wav\"\n",
    "\n",
    " print(f\"\\nTesting synthesis with: {test_text_romanized}\")\n",
    " synthesize_speech(test_model, test_tokenizer, test_text_romanized, test_output)\n",
    " print(f\"Test audio saved to: {test_output}\")\n",
    "else:\n",
    " print(\"Please run romanization cells first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# NOTE: MMS-TTS uses custom training loop, so we use manually tracked losses\n",
    "# These should be populated during training: train_losses, val_losses, train_steps, val_steps\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "\n",
    "# For WER and CER, these are not tracked in MMS-TTS custom loop\n",
    "# Set to empty lists (they would need to be added to the training loop if needed)\n",
    "eval_wers = []\n",
    "eval_cers = []\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if train_losses and train_steps:\n",
    "    ax1.plot(train_steps, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=12, loc='best')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No training data available\\nPlease track losses during training', ha='center', va='center', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if val_losses and val_steps:\n",
    "    ax2.plot(val_steps, val_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Word Error Rate (WER) - Not available for MMS-TTS\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    ax3.plot(val_steps, eval_wers, 'g-', linewidth=2.5, marker='s', markersize=5, label='WER', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Word Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.set_ylim(bottom=0)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'WER not tracked\\n(Not available for MMS-TTS)', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Character Error Rate (CER) - Not available for MMS-TTS\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    ax4.plot(val_steps, eval_cers, 'm-', linewidth=2.5, marker='^', markersize=5, label='CER', alpha=0.8)\n",
    "    ax4.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Character Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.set_ylim(bottom=0)\n",
    "    ax4.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'CER not tracked\\n(Not available for MMS-TTS)', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Combined Loss Plot (Training vs Validation)\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "if train_losses and train_steps:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    if val_losses and val_steps:\n",
    "        ax5.plot(val_steps, val_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'No training data', ha='center', va='center', fontsize=12)\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING SUMMARY: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "if train_losses:\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\" Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\" Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\" Best: {min(train_losses):.4f} (at step {train_steps[train_losses.index(min(train_losses))]})\")\n",
    "    print(f\" Improvement: {((train_losses[0] - min(train_losses)) / train_losses[0] * 100):.2f}%\")\n",
    "if val_losses:\n",
    "    print(f\"\\nValidation Loss:\")\n",
    "    print(f\" Initial: {val_losses[0]:.4f}\")\n",
    "    print(f\" Final: {val_losses[-1]:.4f}\")\n",
    "    print(f\" Best: {min(val_losses):.4f} (at step {val_steps[val_losses.index(min(val_losses))]})\")\n",
    "    print(f\" Improvement: {((val_losses[0] - min(val_losses)) / val_losses[0] * 100):.2f}%\")\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    valid_wers = [w for w in eval_wers if w > 0]\n",
    "    valid_steps = [val_steps[i] for i, w in enumerate(eval_wers) if w > 0]\n",
    "    print(f\"\\nWord Error Rate (WER):\")\n",
    "    print(f\" Initial: {valid_wers[0]:.4f}\")\n",
    "    print(f\" Final: {valid_wers[-1]:.4f}\")\n",
    "    print(f\" Best: {min(valid_wers):.4f} (at step {valid_steps[valid_wers.index(min(valid_wers))]})\")\n",
    "    print(f\" Improvement: {((valid_wers[0] - min(valid_wers)) / valid_wers[0] * 100):.2f}%\")\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    valid_cers = [c for c in eval_cers if c > 0]\n",
    "    valid_steps = [val_steps[i] for i, c in enumerate(eval_cers) if c > 0]\n",
    "    print(f\"\\nCharacter Error Rate (CER):\")\n",
    "    print(f\" Initial: {valid_cers[0]:.4f}\")\n",
    "    print(f\" Final: {valid_cers[-1]:.4f}\")\n",
    "    print(f\" Best: {min(valid_cers):.4f} (at step {valid_steps[valid_cers.index(min(valid_cers))]})\")\n",
    "    print(f\" Improvement: {((valid_cers[0] - min(valid_cers)) / valid_cers[0] * 100):.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: MMS-TTS uses custom training loop, so we use manually tracked losses\n",
    "# These should be populated during training: train_losses, val_losses, train_steps, val_steps\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "\n",
    "# For epoch-based plot, we'll use steps as approximate epochs\n",
    "# In a real scenario, you'd track actual epoch numbers during training\n",
    "if train_losses and train_steps:\n",
    "    # For simplicity, assume each step is roughly an epoch (adjust as needed)\n",
    "    # In real training, you'd group by actual epoch number\n",
    "    epochs = list(range(len(train_losses)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "    if val_losses:\n",
    "        val_epochs = list(range(len(val_losses)))\n",
    "        plt.plot(val_epochs, val_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Epoch-based loss plot saved to: {plot_path}\")\n",
    "else:\n",
    "    print(\"Warning: No training losses available. Please track losses during training.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract training history from trainer state\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in train_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in train_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract epochs and losses\n",
    "# Group training logs by epoch\n",
    "train_by_epoch = {}\n",
    "for log in train_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in train_by_epoch:\n",
    "        train_by_epoch[epoch] = []\n",
    "    train_by_epoch[epoch].append(log.get('loss', 0))\n",
    "\n",
    "# Average losses per epoch\n",
    "epochs = sorted(train_by_epoch.keys())\n",
    "train_losses = [np.mean(train_by_epoch[epoch]) for epoch in epochs]\n",
    "\n",
    "# Extract validation losses by epoch\n",
    "eval_by_epoch = {}\n",
    "for log in eval_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in eval_by_epoch:\n",
    "        eval_by_epoch[epoch] = []\n",
    "    eval_by_epoch[epoch].append(log.get('eval_loss', 0))\n",
    "\n",
    "# Average validation losses per epoch\n",
    "eval_epochs = sorted(eval_by_epoch.keys())\n",
    "eval_losses = [np.mean(eval_by_epoch[epoch]) for epoch in eval_epochs]\n",
    "\n",
    "# Create the plot matching the image style\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "if eval_epochs:\n",
    "    plt.plot(eval_epochs, eval_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"Epoch-based loss plot saved to: {plot_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# NOTE: MMS-TTS uses custom training loop, so we use manually tracked losses\n",
    "# These should be populated during training: train_losses, val_losses, train_steps, val_steps\n",
    "\n",
    "# Check if losses were tracked\n",
    "if 'train_losses' not in globals() or not train_losses:\n",
    "    print(\"Warning: Training losses not tracked. Please ensure the training loop tracks losses.\")\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "\n",
    "# For WER and CER, these are not tracked in MMS-TTS custom loop\n",
    "# Set to empty lists (they would need to be added to the training loop if needed)\n",
    "eval_wers = []\n",
    "eval_cers = []\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if train_losses and train_steps:\n",
    "    ax1.plot(train_steps, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
    "    ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=12, loc='best')\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No training data available\\nPlease track losses during training', ha='center', va='center', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if val_losses and val_steps:\n",
    "    ax2.plot(val_steps, val_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Word Error Rate (WER) - Not available for MMS-TTS\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    ax3.plot(val_steps, eval_wers, 'g-', linewidth=2.5, marker='s', markersize=5, label='WER', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Word Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.set_ylim(bottom=0)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'WER not tracked\\n(Not available for MMS-TTS)', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Character Error Rate (CER) - Not available for MMS-TTS\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    ax4.plot(val_steps, eval_cers, 'm-', linewidth=2.5, marker='^', markersize=5, label='CER', alpha=0.8)\n",
    "    ax4.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Character Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.set_ylim(bottom=0)\n",
    "    ax4.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'CER not tracked\\n(Not available for MMS-TTS)', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Combined Loss Plot (Training vs Validation)\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "if train_losses and train_steps:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    if val_losses and val_steps:\n",
    "        ax5.plot(val_steps, val_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'No training data', ha='center', va='center', fontsize=12)\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING SUMMARY: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "if train_losses:\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\" Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\" Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\" Best: {min(train_losses):.4f} (at step {train_steps[train_losses.index(min(train_losses))]})\")\n",
    "    print(f\" Improvement: {((train_losses[0] - min(train_losses)) / train_losses[0] * 100):.2f}%\")\n",
    "if val_losses:\n",
    "    print(f\"\\nValidation Loss:\")\n",
    "    print(f\" Initial: {val_losses[0]:.4f}\")\n",
    "    print(f\" Final: {val_losses[-1]:.4f}\")\n",
    "    print(f\" Best: {min(val_losses):.4f} (at step {val_steps[val_losses.index(min(val_losses))]})\")\n",
    "    print(f\" Improvement: {((val_losses[0] - min(val_losses)) / val_losses[0] * 100):.2f}%\")\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    valid_wers = [w for w in eval_wers if w > 0]\n",
    "    valid_steps = [val_steps[i] for i, w in enumerate(eval_wers) if w > 0]\n",
    "    print(f\"\\nWord Error Rate (WER):\")\n",
    "    print(f\" Initial: {valid_wers[0]:.4f}\")\n",
    "    print(f\" Final: {valid_wers[-1]:.4f}\")\n",
    "    print(f\" Best: {min(valid_wers):.4f} (at step {valid_steps[valid_wers.index(min(valid_wers))]})\")\n",
    "    print(f\" Improvement: {((valid_wers[0] - min(valid_wers)) / valid_wers[0] * 100):.2f}%\")\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    valid_cers = [c for c in eval_cers if c > 0]\n",
    "    valid_steps = [val_steps[i] for i, c in enumerate(eval_cers) if c > 0]\n",
    "    print(f\"\\nCharacter Error Rate (CER):\")\n",
    "    print(f\" Initial: {valid_cers[0]:.4f}\")\n",
    "    print(f\" Final: {valid_cers[-1]:.4f}\")\n",
    "    print(f\" Best: {min(valid_cers):.4f} (at step {valid_steps[valid_cers.index(min(valid_cers))]})\")\n",
    "    print(f\" Improvement: {((valid_cers[0] - min(valid_cers)) / valid_cers[0] * 100):.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}