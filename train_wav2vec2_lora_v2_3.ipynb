{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAf3OpB3RM-h"
   },
   "source": [
    "# Fine-tuning Wav2Vec2 for Amharic Legal ASR with LoRA\n",
    "\n",
    "This notebook fine-tunes `agkphysics/wav2vec2-large-xlsr-53-amharic` for legal domain Amharic speech recognition using LoRA (Low-Rank Adaptation) for efficient training on Colab free tier.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `agkphysics/wav2vec2-large-xlsr-53-amharic`\n",
    "- **Fine-tuning Method**: LoRA (via PEFT library)\n",
    "- **Task**: Automatic Speech Recognition (ASR)\n",
    "- **Domain**: Legal Amharic text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTBFzv27RM-j"
   },
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mbjUkoRzRM-k"
   },
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa jiwer soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvGj3opKRM-l",
    "outputId": "b893707c-5378-4b80-f5bf-470a59e1f9df"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "CUDA memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import jiwer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-rvtu_ASGI1",
    "outputId": "d02448c4-00dc-413f-adfc-d953882c8bef"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB0JZBxORM-l"
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UI5rQADTRM-l",
    "outputId": "b9b39221-2b41-487b-9d8f-de763bcd4b5d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration:\n",
      "  Model: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "  Output directory: wav2vec2_lora_amharic_legal_v2\n",
      "  LoRA rank (r): 8\n",
      "  LoRA alpha: 32\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"agkphysics/wav2vec2-large-xlsr-53-amharic\"\n",
    "\n",
    "AUDIO_DIR = \"/content/drive/MyDrive/Dataset_4.0h/audio\"\n",
    "TRAIN_CSV = \"/content/drive/MyDrive/Dataset_4.0h/train.csv\"\n",
    "VAL_CSV = \"/content/drive/MyDrive/Dataset_4.0h/val.csv\"\n",
    "TEST_CSV = \"/content/drive/MyDrive/Dataset_4.0h/test.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"wav2vec2_lora_amharic_legal_v2\"\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,  # Lowered from 3e-4 for more stable training\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 2000,  # Increased steps for larger dataset\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 400,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 400,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"wer\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"none\",\n",
    "    \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  LoRA rank (r): {LORA_CONFIG['r']}\")\n",
    "print(f\"  LoRA alpha: {LORA_CONFIG['lora_alpha']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU1SGlmWRM-l"
   },
   "source": [
    "## 3. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsI2197MRM-m",
    "outputId": "a7fbb6e7-c833-407c-fb6a-e65246f66dec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train samples: 302\n",
      "Validation samples: 37\n",
      "Test samples: 39\n",
      "\n",
      "Total samples: 378\n"
     ]
    }
   ],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2D2ulFARM-m"
   },
   "source": [
    "## 4. Create Vocabulary and Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7dkdOsTRM-m",
    "outputId": "b6a6a4b4-6318-4695-da6a-091e00c0daba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 170\n",
      "First 20 characters: [' ', 'ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ']\n"
     ]
    }
   ],
   "source": [
    "# Use the original model's processor instead of creating a new vocabulary\n",
    "# This preserves the pre-trained CTC head weights\n",
    "print(\"Loading original model processor to preserve vocabulary and CTC head...\")\n",
    "original_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Original vocabulary size: {len(original_processor.tokenizer)}\")\n",
    "print(f\"First 20 characters: {list(original_processor.tokenizer.get_vocab().keys())[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QcROwz4RM-m",
    "outputId": "aaad3024-90ff-4858-b7f5-4b56f411abb0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processor created successfully\n"
     ]
    }
   ],
   "source": [
    "# Use the original processor to maintain vocabulary compatibility\n",
    "processor = original_processor\n",
    "print(\"Using original model processor to preserve CTC head weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp68_vI9RM-n"
   },
   "source": [
    "## 5. Load Model and Apply LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuKL9WXjRM-n",
    "outputId": "612967ac-529a-44cf-fce7-a95f211c5936"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at agkphysics/wav2vec2-large-xlsr-53-amharic and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([234]) in the checkpoint and torch.Size([172]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([234, 1024]) in the checkpoint and torch.Size([172, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Base model loaded: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "Vocabulary size: 172\n",
      "Model parameters: 315.62M\n",
      "Model moved to CUDA\n"
     ]
    }
   ],
   "source": [
    "# Load model WITHOUT reinitializing CTC head - use original vocabulary\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    "    # No vocab_size or ignore_mismatched_sizes - preserves original CTC head\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {len(processor.tokenizer)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Ensure CTC head (lm_head) is trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "print(f\"CTC head (lm_head) parameters: {sum(p.numel() for p in model.lm_head.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"CTC head trainable: {all(p.requires_grad for p in model.lm_head.parameters())}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4T3_GKzRM-n",
    "outputId": "f30c21e8-622c-425b-ae6b-93a01b11f9df"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 1,572,864 || all params: 317,187,884 || trainable%: 0.4959\n",
      "\n",
      "LoRA adapters applied successfully\n",
      "Model base forward method wrapped to filter input_ids and inputs_embeds\n",
      "Applied comprehensive PEFT compatibility patches for Wav2Vec2\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_CONFIG[\"r\"],\n",
    "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "    bias=LORA_CONFIG[\"bias\"],\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure CTC head remains trainable after LoRA application\n",
    "for param in model.base_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Count trainable parameters including CTC head\n",
    "lm_head_params = sum(p.numel() for p in model.base_model.lm_head.parameters() if p.requires_grad)\n",
    "print(f\"\\nCTC head (lm_head) trainable parameters: {lm_head_params / 1e6:.2f}M\")\n",
    "\n",
    "print(\"\\nLoRA adapters applied successfully\")\n",
    "\n",
    "# Wrap forward method to filter out input_ids and inputs_embeds (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "original_base_forward = model.base_model.forward\n",
    "\n",
    "def filtered_base_forward(*args, **kwargs):\n",
    "    # Remove input_ids and inputs_embeds if present (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "    # Wav2Vec2ForCTC only expects input_values, not input_ids or inputs_embeds\n",
    "    if 'input_ids' in kwargs:\n",
    "        del kwargs['input_ids']\n",
    "    if 'inputs_embeds' in kwargs:\n",
    "        del kwargs['inputs_embeds']\n",
    "    return original_base_forward(*args, **kwargs)\n",
    "\n",
    "model.base_model.forward = filtered_base_forward\n",
    "print(\"Model base forward method wrapped to filter input_ids and inputs_embeds\")\n",
    "\n",
    "# RIGHT AFTER applying LoRA and before creating Trainer\n",
    "# Solution 1: Patch PEFT's save function\n",
    "from peft.utils.save_and_load import get_peft_model_state_dict\n",
    "import peft.utils.save_and_load\n",
    "\n",
    "_original_get_peft_state_dict = get_peft_model_state_dict\n",
    "\n",
    "def safe_get_peft_state_dict(model, state_dict=None, adapter_name=\"default\",\n",
    "                              unwrap_compiled=False, save_embedding_layers=False, **kwargs):\n",
    "    \"\"\"Skip embedding layers for Wav2Vec2 models\"\"\"\n",
    "    if hasattr(model, 'base_model'):\n",
    "        base_class = model.base_model.__class__.__name__\n",
    "        if 'Wav2Vec2' in base_class:\n",
    "            save_embedding_layers = False\n",
    "    elif 'Wav2Vec2' in model.__class__.__name__:\n",
    "        save_embedding_layers = False\n",
    "\n",
    "    return _original_get_peft_state_dict(\n",
    "        model, state_dict=state_dict, adapter_name=adapter_name,\n",
    "        unwrap_compiled=unwrap_compiled,\n",
    "        save_embedding_layers=save_embedding_layers, **kwargs\n",
    "    )\n",
    "\n",
    "peft.utils.save_and_load.get_peft_model_state_dict = safe_get_peft_state_dict\n",
    "\n",
    "# Solution 2: Patch the Wav2Vec2Model class\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2ForCTC\n",
    "\n",
    "Wav2Vec2Model.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2Model.get_output_embeddings = lambda self: None\n",
    "Wav2Vec2Model.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2Model.set_output_embeddings = lambda self, x: None\n",
    "\n",
    "Wav2Vec2ForCTC.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2ForCTC.get_output_embeddings = lambda self: (self.lm_head if hasattr(self, 'lm_head') else None)\n",
    "Wav2Vec2ForCTC.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2ForCTC.set_output_embeddings = lambda self, x: (setattr(self, 'lm_head', x) if hasattr(self, 'lm_head') else None)\n",
    "\n",
    "print(\"Applied comprehensive PEFT compatibility patches for Wav2Vec2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOgioHVBRM-n"
   },
   "source": [
    "## 6. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184,
     "referenced_widgets": [
      "0ed4cb3de54e4db08b4625d2cf420688",
      "ced3c3e4e3ba4b0db2e77c0b8bdb2149",
      "86a9ebbcc5be424da7797d905b9be59d",
      "f221a38aeb56487ba52a3a211e224655",
      "69970d5c869f4751887f055a07516182",
      "904837f4797e48518a7afd4dae0520f5",
      "e7937e0d796a4d8c8fef27b8be205a44",
      "b8b72ac0e47240e388ccc38672191645",
      "0a30b418428a465d8a13e23d83888543",
      "2db835ece6564cc49d4915a97e16f871",
      "bb060dd30d164857b1c5b2bb55e4584f",
      "332324bf1c4d4d64956c6cb62401ad1d",
      "76481d23c9524750ba66ffb2c17b3672",
      "244493af556040d29d64cee0c4ebb7e4",
      "d81f2b9c21c149698c79d1a6b2eb7e40",
      "39b2a72aab084ec986ec1df3ea910898",
      "59cb32e978494d1d8ad7066dd05a64df",
      "b596d56219c247c2a44750906c4f579e",
      "6e872f6f827e49c59b3925f0a39d7b11",
      "24bbb8c0c2664a43857bfc9ad904f39d",
      "5f4455b416a74694aa034e89d27eb8b2",
      "d980bc6f4a9240ecad09d3ec8646d05b",
      "384c64917a774a5eb85bd08a9b045ec5",
      "362edb2fa859496496452df80e4da7b0",
      "86cb43ba57a14bf28c789ce8d91658ec",
      "1dc6d8ef17e041a899714ec8171f0914",
      "221ede75ae7e4be6951646707361b05c",
      "0dc4599754a34c7b952e2fffb1bea17a",
      "fa8979ba0ee6475fad16a4faf477b185",
      "daaaf80e51a246cfb1de53cee79bfef5",
      "698c4022c67f477692a6586b8c2a4b2a",
      "7db8d00cb83b43768b800a7eb693533b",
      "8e5b0ac573dd48fb98ddac2a0b4f7ba6"
     ]
    },
    "id": "74r42C3ZRM-n",
    "outputId": "0be57c52-e9ad-4d47-fe9c-ea1d8930c375"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/302 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ed4cb3de54e4db08b4625d2cf420688"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "332324bf1c4d4d64956c6cb62401ad1d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "384c64917a774a5eb85bd08a9b045ec5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets prepared:\n",
      "  Train: 302 samples\n",
      "  Validation: 37 samples\n",
      "  Test: 39 samples\n"
     ]
    }
   ],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    \"\"\"Load audio file and resample to 16kHz\"\"\"\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=16000)\n",
    "    return speech_array\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Process a batch of audio and transcriptions\"\"\"\n",
    "    audio = [speech_file_to_array_fn(path) for path in batch[\"audio_path\"]]\n",
    "\n",
    "    # Process audio - return as lists (no return_tensors)\n",
    "    audio_features = processor.feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000\n",
    "        # No padding, no return_tensors here - collator handles it\n",
    "    )\n",
    "    # Only store input_values, not attention_mask\n",
    "    batch[\"input_values\"] = audio_features.input_values  # This will be a list of arrays\n",
    "\n",
    "    # Process text using tokenizer directly - return as lists\n",
    "    batch[\"labels\"] = [tokenizer(transcription, add_special_tokens=False)[\"input_ids\"] for transcription in batch[\"transcription\"]]\n",
    "\n",
    "    # Make sure we're not accidentally storing attention_mask\n",
    "    if \"attention_mask\" in batch:\n",
    "        del batch[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjV1TF1ERM-n"
   },
   "source": [
    "## 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "R8Q631KmRM-n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d82f6cc-aa3f-411f-de0d-02069a1b0f5e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data collator created\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        import torch\n",
    "\n",
    "        # Extract input_values and labels - explicitly ignore attention_mask if present\n",
    "        input_values_list = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # Convert audio to tensors and pad manually\n",
    "        input_values_tensors = []\n",
    "        for iv in input_values_list:\n",
    "            if isinstance(iv, torch.Tensor):\n",
    "                input_values_tensors.append(iv)\n",
    "            elif isinstance(iv, np.ndarray):\n",
    "                input_values_tensors.append(torch.tensor(iv, dtype=torch.float32))\n",
    "            else:\n",
    "                input_values_tensors.append(torch.tensor(np.array(iv), dtype=torch.float32))\n",
    "\n",
    "        # Pad audio sequences\n",
    "        input_values = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_values_tensors,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0\n",
    "        )\n",
    "\n",
    "        # Pad labels manually\n",
    "        max_label_len = max(len(labels) for labels in label_features)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        padded_labels = []\n",
    "        for labels in label_features:\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                labels = labels.tolist()\n",
    "\n",
    "            padding_length = max_label_len - len(labels)\n",
    "            padded = labels + [pad_token_id] * padding_length\n",
    "            padded_labels.append(padded)\n",
    "\n",
    "        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        # Mask padded labels with -100\n",
    "        attention_mask_labels = (labels_tensor != pad_token_id).long()\n",
    "        labels_tensor = labels_tensor.masked_fill(attention_mask_labels.ne(1), -100)\n",
    "\n",
    "        # Return ONLY input_values and labels - explicitly create a new dict\n",
    "        batch = {}\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = labels_tensor\n",
    "\n",
    "        # Explicitly ensure no other keys are present\n",
    "        return batch\n",
    "\n",
    "# Add these lines:\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "print(\"Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9Fcp29eRM-n"
   },
   "source": [
    "## 8. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOmpPIlFRM-n",
    "outputId": "0c3fc0d0-fdb5-4571-fcec-838c9c710dfd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation metrics function created\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER (Word Error Rate) and CER (Character Error Rate)\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIxIZJMkRM-o"
   },
   "source": [
    "## 9. Training Arguments and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R939GiN-RM-o",
    "outputId": "c834adad-e3f3-4fc7-b02f-6ea830339dda"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainer initialized\n",
      "\n",
      "Training configuration:\n",
      "  Max steps: 800\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0003\n",
      "  Evaluation steps: 400\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(**TRAINING_ARGS)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Evaluation steps: {training_args.eval_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if data collator is working correctly\n",
    "sample_features = [train_dataset[0], train_dataset[1]]\n",
    "sample_batch = data_collator(sample_features)\n",
    "print(\"Batch keys:\", list(sample_batch.keys()))\n",
    "print(\"Batch has input_ids:\", \"input_ids\" in sample_batch)\n",
    "print(\"Batch has input_values:\", \"input_values\" in sample_batch)\n",
    "print(\"Batch has labels:\", \"labels\" in sample_batch)\n",
    "\n",
    "# Try to manually call the model to see what happens\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        output = model(**sample_batch)\n",
    "        print(\"Model forward works!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model forward error: {e}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npH0SemDXsxD",
    "outputId": "49be1f0f-91ec-415d-e928-bfe61805c402"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch keys: ['input_values', 'labels']\n",
      "Batch has input_ids: False\n",
      "Batch has input_values: True\n",
      "Batch has labels: True\n",
      "Model forward error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jx8djbjRM-o"
   },
   "source": [
    "## 10. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "aQvmn4KiRM-o",
    "outputId": "8e7d1475-a9a8-4264-cfea-318baddcf158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/800 27:39 < 1:11:19, 0.13 it/s, Epoch 11.79/43]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 1:38:34, Epoch 42/43]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.174500</td>\n",
       "      <td>5.653153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.907200</td>\n",
       "      <td>4.723449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 11.4706\n",
      "Final model saved to: wav2vec2_lora_amharic_legal_v2_final\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nko9i712RM-o"
   },
   "source": [
    "## 11. Zip and Copy Model to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7p9kY_huRM-o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22546f46-27ae-4626-f4a4-be181661d196"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating zip file: wav2vec2_lora_amharic_legal_v2_final.zip...\n",
      "  Added: added_tokens.json\n",
      "  Added: preprocessor_config.json\n",
      "  Added: special_tokens_map.json\n",
      "  Added: tokenizer_config.json\n",
      "  Added: adapter_config.json\n",
      "  Added: training_args.bin\n",
      "  Added: adapter_model.safetensors\n",
      "  Added: README.md\n",
      "  Added: vocab.json\n",
      "\n",
      "Zip file created: wav2vec2_lora_amharic_legal_v2_final.zip\n",
      "Model zip file copied to Google Drive: /content/drive/MyDrive/wav2vec2_lora_amharic_legal_v2_final.zip\n",
      "\n",
      "File size: 6.21 MB\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory\n",
    "zip_filename = f\"{final_model_path}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            # Get relative path for archive\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"  Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "\n",
    "# Copy to Google Drive\n",
    "drive_dest = f\"/content/drive/MyDrive/{zip_filename}\"\n",
    "shutil.copy2(zip_filename, drive_dest)\n",
    "\n",
    "print(f\"Model zip file copied to Google Drive: {drive_dest}\")\n",
    "print(f\"\\nFile size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQpWO7FeRM-o"
   },
   "source": [
    "## 12. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0vTOqf4gRM-o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "db746c94-7e85-4214-bd2c-05684180a000"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final model saved to: wav2vec2_lora_amharic_legal_v2_final\n",
      "\n",
      "Files saved:\n",
      "  - LoRA adapters (adapter_model.bin, adapter_config.json)\n",
      "  - Processor (tokenizer, feature_extractor)\n",
      "  - Training configuration\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - LoRA adapters (adapter_model.bin, adapter_config.json)\")\n",
    "print(\"  - Processor (tokenizer, feature_extractor)\")\n",
    "print(\"  - Training configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SJA1iNJRM-o"
   },
   "source": [
    "## 13. Inference Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4UUnRGSXRM-o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4763032e-2782-4423-96f4-b0795974fea1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference function created\n",
      "\n",
      "Example usage:\n",
      "  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\n",
      "  print(transcription)\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(model, processor, audio_path):\n",
    "    \"\"\"Transcribe a single audio file\"\"\"\n",
    "    speech, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "print(\"Inference function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\")\n",
    "print(\"  print(transcription)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1GL3RybRM-o"
   },
   "source": [
    "## 14. Load Model for Inference (After Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yX18oDGaRM-o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3575bd86-4cc0-4895-cc37-a0559d2a5eca"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loading function created\n",
      "\n",
      "Example usage:\n",
      "  model, processor = load_trained_model(\n",
      "      MODEL_NAME,\n",
      "      'wav2vec2_lora_amharic_legal_v2_final',\n",
      "      'wav2vec2_lora_amharic_legal_v2_final'\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_trained_model(base_model_name, adapter_path, processor_path):\n",
    "    \"\"\"Load base model and LoRA adapters\"\"\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "\n",
    "    base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        base_model_name,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "print(\"Model loading function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  model, processor = load_trained_model(\")\n",
    "print(\"      MODEL_NAME,\")\n",
    "print(f\"      '{OUTPUT_DIR}_final',\")\n",
    "print(f\"      '{OUTPUT_DIR}_final'\")\n",
    "print(\"  )\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed4cb3de54e4db08b4625d2cf420688": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ced3c3e4e3ba4b0db2e77c0b8bdb2149",
       "IPY_MODEL_86a9ebbcc5be424da7797d905b9be59d",
       "IPY_MODEL_f221a38aeb56487ba52a3a211e224655"
      ],
      "layout": "IPY_MODEL_69970d5c869f4751887f055a07516182"
     }
    },
    "ced3c3e4e3ba4b0db2e77c0b8bdb2149": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_904837f4797e48518a7afd4dae0520f5",
      "placeholder": "​",
      "style": "IPY_MODEL_e7937e0d796a4d8c8fef27b8be205a44",
      "value": "Map: 100%"
     }
    },
    "86a9ebbcc5be424da7797d905b9be59d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8b72ac0e47240e388ccc38672191645",
      "max": 302,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a30b418428a465d8a13e23d83888543",
      "value": 302
     }
    },
    "f221a38aeb56487ba52a3a211e224655": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2db835ece6564cc49d4915a97e16f871",
      "placeholder": "​",
      "style": "IPY_MODEL_bb060dd30d164857b1c5b2bb55e4584f",
      "value": " 302/302 [00:04&lt;00:00, 71.33 examples/s]"
     }
    },
    "69970d5c869f4751887f055a07516182": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "904837f4797e48518a7afd4dae0520f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7937e0d796a4d8c8fef27b8be205a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8b72ac0e47240e388ccc38672191645": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a30b418428a465d8a13e23d83888543": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2db835ece6564cc49d4915a97e16f871": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb060dd30d164857b1c5b2bb55e4584f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "332324bf1c4d4d64956c6cb62401ad1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76481d23c9524750ba66ffb2c17b3672",
       "IPY_MODEL_244493af556040d29d64cee0c4ebb7e4",
       "IPY_MODEL_d81f2b9c21c149698c79d1a6b2eb7e40"
      ],
      "layout": "IPY_MODEL_39b2a72aab084ec986ec1df3ea910898"
     }
    },
    "76481d23c9524750ba66ffb2c17b3672": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59cb32e978494d1d8ad7066dd05a64df",
      "placeholder": "​",
      "style": "IPY_MODEL_b596d56219c247c2a44750906c4f579e",
      "value": "Map: 100%"
     }
    },
    "244493af556040d29d64cee0c4ebb7e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e872f6f827e49c59b3925f0a39d7b11",
      "max": 37,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24bbb8c0c2664a43857bfc9ad904f39d",
      "value": 37
     }
    },
    "d81f2b9c21c149698c79d1a6b2eb7e40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f4455b416a74694aa034e89d27eb8b2",
      "placeholder": "​",
      "style": "IPY_MODEL_d980bc6f4a9240ecad09d3ec8646d05b",
      "value": " 37/37 [00:00&lt;00:00, 78.18 examples/s]"
     }
    },
    "39b2a72aab084ec986ec1df3ea910898": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59cb32e978494d1d8ad7066dd05a64df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b596d56219c247c2a44750906c4f579e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e872f6f827e49c59b3925f0a39d7b11": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24bbb8c0c2664a43857bfc9ad904f39d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f4455b416a74694aa034e89d27eb8b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d980bc6f4a9240ecad09d3ec8646d05b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "384c64917a774a5eb85bd08a9b045ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_362edb2fa859496496452df80e4da7b0",
       "IPY_MODEL_86cb43ba57a14bf28c789ce8d91658ec",
       "IPY_MODEL_1dc6d8ef17e041a899714ec8171f0914"
      ],
      "layout": "IPY_MODEL_221ede75ae7e4be6951646707361b05c"
     }
    },
    "362edb2fa859496496452df80e4da7b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dc4599754a34c7b952e2fffb1bea17a",
      "placeholder": "​",
      "style": "IPY_MODEL_fa8979ba0ee6475fad16a4faf477b185",
      "value": "Map: 100%"
     }
    },
    "86cb43ba57a14bf28c789ce8d91658ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daaaf80e51a246cfb1de53cee79bfef5",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_698c4022c67f477692a6586b8c2a4b2a",
      "value": 39
     }
    },
    "1dc6d8ef17e041a899714ec8171f0914": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7db8d00cb83b43768b800a7eb693533b",
      "placeholder": "​",
      "style": "IPY_MODEL_8e5b0ac573dd48fb98ddac2a0b4f7ba6",
      "value": " 39/39 [00:00&lt;00:00, 66.75 examples/s]"
     }
    },
    "221ede75ae7e4be6951646707361b05c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dc4599754a34c7b952e2fffb1bea17a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa8979ba0ee6475fad16a4faf477b185": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "daaaf80e51a246cfb1de53cee79bfef5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "698c4022c67f477692a6586b8c2a4b2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7db8d00cb83b43768b800a7eb693533b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e5b0ac573dd48fb98ddac2a0b4f7ba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}