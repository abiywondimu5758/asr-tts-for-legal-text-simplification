{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Fine-tuning MMS-TTS Amharic Model (Ablation Study)\n",
        "\n",
        "This notebook fine-tunes `facebook/mms-tts-amh` for legal domain Amharic text-to-speech using **full fine-tuning** (no LoRA) as an ablation study.\n",
        "\n",
        "## Model and Approach\n",
        "- **Base Model**: `facebook/mms-tts-amh` (VITS architecture, ~36.3M parameters)\n",
        "- **Fine-tuning Method**: Full fine-tuning (all parameters trainable)\n",
        "- **Task**: Text-to-Speech (TTS)\n",
        "- **Domain**: Legal Amharic text\n",
        "- **Text Format**: Romanized Amharic (using uroman package)\n",
        "- **Optimization**: Memory-optimized for T4 15GB GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers datasets accelerate torchaudio librosa soundfile uroman scipy torch-audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Union, Optional, Tuple\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import scipy.io.wavfile\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import (\n",
        "    VitsModel,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import uroman for romanization\n",
        "import uroman\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration (Optimized for T4 15GB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"facebook/mms-tts-amh\"\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/Dataset_1.5h/audio\"\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/Dataset_1.5h/train.csv\"\n",
        "VAL_CSV = \"/content/drive/MyDrive/Dataset_1.5h/val.csv\"\n",
        "TEST_CSV = \"/content/drive/MyDrive/Dataset_1.5h/test.csv\"\n",
        "\n",
        "OUTPUT_DIR = \"mms_tts_full_finetune_amharic_legal\"\n",
        "\n",
        "# Memory-optimized training configuration for T4 15GB\n",
        "TRAINING_ARGS = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"per_device_train_batch_size\": 2,  # Small batch size for memory efficiency\n",
        "    \"per_device_eval_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 8,  # Effective batch size = 16\n",
        "    \"learning_rate\": 5e-5,  # Slightly lower LR for full fine-tuning\n",
        "    \"warmup_steps\": 100,\n",
        "    \"max_steps\": 1200,  # ~2h15min training time\n",
        "    \"gradient_checkpointing\": True,  # CRITICAL for memory savings\n",
        "    \"fp16\": True,  # Mixed precision for memory efficiency\n",
        "    \"eval_strategy\": \"steps\",\n",
        "    \"eval_steps\": 300,\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"save_steps\": 300,\n",
        "    \"save_total_limit\": 3,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"metric_for_best_model\": \"loss\",\n",
        "    \"greater_is_better\": False,\n",
        "    \"logging_steps\": 50,\n",
        "    \"report_to\": \"none\",\n",
        "    \"push_to_hub\": False,\n",
        "    \"dataloader_num_workers\": 2,  # Reduce workers to save memory\n",
        "    \"dataloader_pin_memory\": False,  # Disable pinning to save memory\n",
        "    \"remove_unused_columns\": False,\n",
        "}\n",
        "\n",
        "# Audio processing configuration\n",
        "TARGET_SAMPLE_RATE = 16000  # MMS-TTS uses 16kHz\n",
        "MAX_AUDIO_LENGTH = 10.0  # Maximum audio length in seconds (for memory efficiency)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Batch size: {TRAINING_ARGS['per_device_train_batch_size']}\")\n",
        "print(f\"  Gradient accumulation: {TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch size: {TRAINING_ARGS['per_device_train_batch_size'] * TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
        "print(f\"  Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
        "print(f\"  Gradient checkpointing: {TRAINING_ARGS['gradient_checkpointing']}\")\n",
        "print(f\"  FP16: {TRAINING_ARGS['fp16']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_split(csv_path, audio_dir):\n",
        "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    data = []\n",
        "    for _, row in df.iterrows():\n",
        "        audio_path = Path(audio_dir) / row['file_name']\n",
        "        transcription = str(row['transcription']).strip()\n",
        "\n",
        "        if audio_path.exists():\n",
        "            data.append({\n",
        "                'audio_path': str(audio_path),\n",
        "                'transcription': transcription\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
        "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
        "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization setup\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "try:\n",
        "    matplotlib.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        matplotlib.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('default')\n",
        "        \n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Visualization libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Romanize Text Using Uroman\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize uroman instance\n",
        "try:\n",
        "    uroman_obj = uroman.Uroman()\n",
        "    print(\"Uroman initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing uroman: {e}\")\n",
        "    uroman_obj = None\n",
        "\n",
        "def romanize_text(text):\n",
        "    \"\"\"Convert Ge'ez script Amharic text to Romanized format using uroman\"\"\"\n",
        "    if uroman_obj is None:\n",
        "        return text  # Return original if uroman not available\n",
        "\n",
        "    try:\n",
        "        romanized = uroman_obj.romanize_string(text)\n",
        "        return romanized.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error romanizing text: {text[:50]}... Error: {e}\")\n",
        "        return text  # Fallback to original if romanization fails\n",
        "\n",
        "# Test romanization on a sample\n",
        "sample_text = train_data[0]['transcription']\n",
        "print(f\"Original (Ge'ez): {sample_text}\")\n",
        "romanized_sample = romanize_text(sample_text)\n",
        "print(f\"Romanized: {romanized_sample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tracking lists if not already done\n",
        "if 'learning_rates' not in locals():\n",
        "    learning_rates = []\n",
        "if 'gpu_memory_usage' not in locals():\n",
        "    gpu_memory_usage = []\n",
        "if 'train_steps' not in locals():\n",
        "    train_steps = []\n",
        "if 'val_steps' not in locals():\n",
        "    val_steps = []\n",
        "\n",
        "print(\"Metrics tracking initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training metrics to JSON file for later analysis\n",
        "import json\n",
        "\n",
        "metrics_data = {\n",
        "    'training_config': {\n",
        "        'max_steps': TRAINING_ARGS['max_steps'],\n",
        "        'batch_size': TRAINING_ARGS['per_device_train_batch_size'],\n",
        "        'gradient_accumulation_steps': TRAINING_ARGS['gradient_accumulation_steps'],\n",
        "        'learning_rate': TRAINING_ARGS['learning_rate'],\n",
        "        'fp16': TRAINING_ARGS['fp16'],\n",
        "        'gradient_checkpointing': TRAINING_ARGS['gradient_checkpointing'],\n",
        "        'logging_steps': TRAINING_ARGS['logging_steps'],\n",
        "        'eval_steps': TRAINING_ARGS['eval_steps'],\n",
        "    },\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'learning_rates': learning_rates,\n",
        "    'gpu_memory_usage': gpu_memory_usage,\n",
        "    'train_steps': train_steps if len(train_steps) > 0 else [i * TRAINING_ARGS['logging_steps'] for i in range(len(train_losses))],\n",
        "    'val_steps': val_steps if len(val_steps) > 0 else [i * TRAINING_ARGS['eval_steps'] for i in range(len(val_losses))],\n",
        "    'final_step': global_step,\n",
        "    'best_val_loss': best_val_loss if 'best_val_loss' in locals() else None,\n",
        "}\n",
        "\n",
        "metrics_file = f'{OUTPUT_DIR}/training_metrics.json'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Enhanced metrics data with statistics and metadata\n",
        "from datetime import datetime\n",
        "\n",
        "# Safely handle potentially missing variables\n",
        "train_losses_safe = train_losses if 'train_losses' in locals() and len(train_losses) > 0 else []\n",
        "val_losses_safe = val_losses if 'val_losses' in locals() and len(val_losses) > 0 else []\n",
        "learning_rates_safe = learning_rates if 'learning_rates' in locals() and len(learning_rates) > 0 else []\n",
        "gpu_memory_usage_safe = gpu_memory_usage if 'gpu_memory_usage' in locals() and len(gpu_memory_usage) > 0 else []\n",
        "train_steps_safe = train_steps if 'train_steps' in locals() and len(train_steps) > 0 else []\n",
        "val_steps_safe = val_steps if 'val_steps' in locals() and len(val_steps) > 0 else []\n",
        "global_step_safe = global_step if 'global_step' in locals() else 0\n",
        "best_val_loss_safe = best_val_loss if 'best_val_loss' in locals() and best_val_loss != float('inf') else None\n",
        "\n",
        "# Calculate step values if not explicitly tracked\n",
        "if len(train_steps_safe) == 0 and len(train_losses_safe) > 0:\n",
        "    train_steps_safe = [i * TRAINING_ARGS['logging_steps'] for i in range(len(train_losses_safe))]\n",
        "\n",
        "if len(val_steps_safe) == 0 and len(val_losses_safe) > 0:\n",
        "    val_steps_safe = [i * TRAINING_ARGS['eval_steps'] for i in range(len(val_losses_safe))]\n",
        "\n",
        "# Calculate statistics\n",
        "train_stats = {}\n",
        "if len(train_losses_safe) > 0:\n",
        "    train_stats = {\n",
        "        'initial': float(train_losses_safe[0]),\n",
        "        'final': float(train_losses_safe[-1]),\n",
        "        'best': float(min(train_losses_safe)),\n",
        "        'worst': float(max(train_losses_safe)),\n",
        "        'mean': float(np.mean(train_losses_safe)),\n",
        "        'std': float(np.std(train_losses_safe)),\n",
        "    }\n",
        "\n",
        "val_stats = {}\n",
        "if len(val_losses_safe) > 0:\n",
        "    val_stats = {\n",
        "        'initial': float(val_losses_safe[0]),\n",
        "        'final': float(val_losses_safe[-1]),\n",
        "        'best': float(min(val_losses_safe)),\n",
        "        'worst': float(max(val_losses_safe)),\n",
        "        'mean': float(np.mean(val_losses_safe)),\n",
        "        'std': float(np.std(val_losses_safe)),\n",
        "    }\n",
        "\n",
        "# Enhanced metrics data structure\n",
        "enhanced_metrics_data = {\n",
        "    'metadata': {\n",
        "        'saved_at': datetime.now().isoformat(),\n",
        "        'model_name': MODEL_NAME,\n",
        "        'output_dir': OUTPUT_DIR,\n",
        "    },\n",
        "    'training_config': {\n",
        "        'max_steps': TRAINING_ARGS['max_steps'],\n",
        "        'batch_size': TRAINING_ARGS['per_device_train_batch_size'],\n",
        "        'gradient_accumulation_steps': TRAINING_ARGS['gradient_accumulation_steps'],\n",
        "        'effective_batch_size': TRAINING_ARGS['per_device_train_batch_size'] * TRAINING_ARGS['gradient_accumulation_steps'],\n",
        "        'learning_rate': TRAINING_ARGS['learning_rate'],\n",
        "        'fp16': TRAINING_ARGS['fp16'],\n",
        "        'gradient_checkpointing': TRAINING_ARGS['gradient_checkpointing'],\n",
        "        'logging_steps': TRAINING_ARGS['logging_steps'],\n",
        "        'eval_steps': TRAINING_ARGS['eval_steps'],\n",
        "    },\n",
        "    'training_progress': {\n",
        "        'final_step': global_step_safe,\n",
        "        'total_steps_completed': global_step_safe,\n",
        "        'best_val_loss': best_val_loss_safe,\n",
        "    },\n",
        "    'metrics': {\n",
        "        'train_losses': [float(x) for x in train_losses_safe],\n",
        "        'val_losses': [float(x) for x in val_losses_safe],\n",
        "        'learning_rates': [float(x) for x in learning_rates_safe],\n",
        "        'gpu_memory_usage': [float(x) for x in gpu_memory_usage_safe],\n",
        "        'train_steps': [int(x) for x in train_steps_safe],\n",
        "        'val_steps': [int(x) for x in val_steps_safe],\n",
        "    },\n",
        "    'statistics': {\n",
        "        'training_loss': train_stats,\n",
        "        'validation_loss': val_stats,\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(enhanced_metrics_data, f, indent=2)\n",
        "    \n",
        "    print(f\"✓ Training metrics saved to: {metrics_file}\")\n",
        "    print(f\"\\nMetrics Summary:\")\n",
        "    print(f\"  - Training loss points: {len(train_losses_safe)}\")\n",
        "    print(f\"  - Validation loss points: {len(val_losses_safe)}\")\n",
        "    print(f\"  - Learning rate points: {len(learning_rates_safe)}\")\n",
        "    print(f\"  - GPU memory points: {len(gpu_memory_usage_safe)}\")\n",
        "    print(f\"  - Final step: {global_step_safe}\")\n",
        "    \n",
        "    if train_stats:\n",
        "        print(f\"\\nTraining Loss Statistics:\")\n",
        "        print(f\"  - Initial: {train_stats['initial']:.4f}\")\n",
        "        print(f\"  - Final: {train_stats['final']:.4f}\")\n",
        "        print(f\"  - Best: {train_stats['best']:.4f}\")\n",
        "        print(f\"  - Mean: {train_stats['mean']:.4f} ± {train_stats['std']:.4f}\")\n",
        "    \n",
        "    if val_stats:\n",
        "        print(f\"\\nValidation Loss Statistics:\")\n",
        "        print(f\"  - Initial: {val_stats['initial']:.4f}\")\n",
        "        print(f\"  - Final: {val_stats['final']:.4f}\")\n",
        "        print(f\"  - Best: {val_stats['best']:.4f}\")\n",
        "        print(f\"  - Mean: {val_stats['mean']:.4f} ± {val_stats['std']:.4f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error saving enhanced metrics: {e}\")\n",
        "    print(\"Falling back to original metrics structure...\")\n",
        "    try:\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(metrics_data, f, indent=2)\n",
        "        print(f\"✓ Basic metrics saved to: {metrics_file}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"✗ Failed to save metrics: {e2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-time Training Progress (Run this cell during training to see live updates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-time training progress visualization\n",
        "# Run this cell during training to see live updates\n",
        "\n",
        "# Check if metrics exist\n",
        "if 'train_losses' in locals() and len(train_losses) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Training Loss\n",
        "    ax1 = axes[0]\n",
        "    train_step_values = [i * TRAINING_ARGS['logging_steps'] for i in range(len(train_losses))]\n",
        "    ax1.plot(train_step_values, train_losses, 'b-', linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
        "    ax1.set_xlabel('Training Step', fontsize=11)\n",
        "    ax1.set_ylabel('Training Loss', fontsize=11)\n",
        "    ax1.set_title('Training Loss (Live)', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim(left=0)\n",
        "    \n",
        "    # Add current loss annotation\n",
        "    if len(train_losses) > 0:\n",
        "        current_loss = train_losses[-1]\n",
        "        current_step = train_step_values[-1]\n",
        "        ax1.annotate(f'Current: {current_loss:.4f}', \n",
        "                     xy=(current_step, current_loss),\n",
        "                     xytext=(10, 10), textcoords='offset points',\n",
        "                     bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
        "                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "    \n",
        "    # Plot 2: Training and Validation Loss Comparison\n",
        "    ax2 = axes[1]\n",
        "    if len(train_losses) > 0:\n",
        "        ax2.plot(train_step_values, train_losses, 'b-', label='Training', linewidth=2, alpha=0.7)\n",
        "    \n",
        "    if 'val_losses' in locals() and len(val_losses) > 0:\n",
        "        val_step_values = [i * TRAINING_ARGS['eval_steps'] for i in range(len(val_losses))]\n",
        "        ax2.plot(val_step_values, val_losses, 'r-', label='Validation', linewidth=2, marker='s', markersize=3, alpha=0.7)\n",
        "    \n",
        "    ax2.set_xlabel('Training Step', fontsize=11)\n",
        "    ax2.set_ylabel('Loss', fontsize=11)\n",
        "    ax2.set_title('Training vs Validation Loss (Live)', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(left=0)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print current statistics\n",
        "    print(f\"\\nCurrent Training Status:\")\n",
        "    print(f\"  Steps completed: {len(train_losses) * TRAINING_ARGS['logging_steps']}\")\n",
        "    print(f\"  Latest training loss: {train_losses[-1]:.4f}\")\n",
        "    if 'val_losses' in locals() and len(val_losses) > 0:\n",
        "        print(f\"  Latest validation loss: {val_losses[-1]:.4f}\")\n",
        "    if 'learning_rates' in locals() and len(learning_rates) > 0:\n",
        "        print(f\"  Current learning rate: {learning_rates[-1]:.2e}\")\n",
        "else:\n",
        "    print(\"Training hasn't started yet or no metrics available.\")\n",
        "    print(\"Run this cell after training has logged some steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training visualizations\n",
        "print(\"Generating training visualizations...\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# 1. Training and Validation Loss\n",
        "ax1 = plt.subplot(2, 2, 1)\n",
        "if len(train_losses) > 0:\n",
        "    # Calculate steps for training losses (assuming logged every logging_steps)\n",
        "    train_step_values = [i * TRAINING_ARGS['logging_steps'] for i in range(len(train_losses))]\n",
        "    ax1.plot(train_step_values, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.7)\n",
        "    \n",
        "if len(val_losses) > 0:\n",
        "    # Calculate steps for validation losses (assuming logged every eval_steps)\n",
        "    val_step_values = [i * TRAINING_ARGS['eval_steps'] for i in range(len(val_losses))]\n",
        "    ax1.plot(val_step_values, val_losses, 'r-', label='Validation Loss', linewidth=2, alpha=0.7)\n",
        "\n",
        "ax1.set_xlabel('Training Step', fontsize=12)\n",
        "ax1.set_ylabel('Loss', fontsize=12)\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim(left=0)\n",
        "\n",
        "# 2. Learning Rate Schedule\n",
        "ax2 = plt.subplot(2, 2, 2)\n",
        "if len(learning_rates) > 0:\n",
        "    lr_steps = [i * TRAINING_ARGS['logging_steps'] for i in range(len(learning_rates))]\n",
        "    ax2.plot(lr_steps, learning_rates, 'g-', linewidth=2, alpha=0.7)\n",
        "    ax2.set_xlabel('Training Step', fontsize=12)\n",
        "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
        "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.set_xlim(left=0)\n",
        "else:\n",
        "    # Plot theoretical LR schedule\n",
        "    max_steps = TRAINING_ARGS['max_steps']\n",
        "    initial_lr = TRAINING_ARGS['learning_rate']\n",
        "    steps = np.arange(0, max_steps, 50)\n",
        "    # Cosine annealing schedule\n",
        "    lrs = [initial_lr * (1 + np.cos(np.pi * s / max_steps)) / 2 for s in steps]\n",
        "    ax2.plot(steps, lrs, 'g-', linewidth=2, alpha=0.7)\n",
        "    ax2.set_xlabel('Training Step', fontsize=12)\n",
        "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
        "    ax2.set_title('Learning Rate Schedule (Theoretical)', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "\n",
        "# 3. Loss Comparison (Training vs Validation)\n",
        "ax3 = plt.subplot(2, 2, 3)\n",
        "if len(train_losses) > 0 and len(val_losses) > 0:\n",
        "    # Normalize to same step range for comparison\n",
        "    min_steps = min(len(train_losses), len(val_losses))\n",
        "    train_norm = train_losses[:min_steps] if len(train_losses) >= min_steps else train_losses\n",
        "    val_norm = val_losses[:min_steps] if len(val_losses) >= min_steps else val_losses\n",
        "    \n",
        "    x_range = range(min(len(train_norm), len(val_norm)))\n",
        "    ax3.plot(x_range, train_norm[:len(x_range)], 'b-', label='Training', linewidth=2, alpha=0.7)\n",
        "    ax3.plot(x_range, val_norm[:len(x_range)], 'r-', label='Validation', linewidth=2, alpha=0.7)\n",
        "    ax3.set_xlabel('Logging Interval', fontsize=12)\n",
        "    ax3.set_ylabel('Loss', fontsize=12)\n",
        "    ax3.set_title('Loss Comparison (Normalized)', fontsize=14, fontweight='bold')\n",
        "    ax3.legend(fontsize=11)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax3.text(0.5, 0.5, 'Insufficient data for comparison', \n",
        "             ha='center', va='center', transform=ax3.transAxes, fontsize=12)\n",
        "    ax3.set_title('Loss Comparison', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 4. Training Statistics Summary\n",
        "ax4 = plt.subplot(2, 2, 4)\n",
        "ax4.axis('off')\n",
        "\n",
        "# Calculate statistics\n",
        "stats_text = []\n",
        "stats_text.append(\"Training Statistics Summary\")\n",
        "stats_text.append(\"=\" * 40)\n",
        "stats_text.append(f\"Total Training Steps: {global_step}\")\n",
        "stats_text.append(f\"Max Steps Configured: {TRAINING_ARGS['max_steps']}\")\n",
        "\n",
        "if len(train_losses) > 0:\n",
        "    stats_text.append(f\"\\nTraining Loss:\")\n",
        "    stats_text.append(f\"  Initial: {train_losses[0]:.4f}\")\n",
        "    stats_text.append(f\"  Final: {train_losses[-1]:.4f}\")\n",
        "    stats_text.append(f\"  Best: {min(train_losses):.4f}\")\n",
        "    stats_text.append(f\"  Average: {np.mean(train_losses):.4f}\")\n",
        "\n",
        "if len(val_losses) > 0:\n",
        "    stats_text.append(f\"\\nValidation Loss:\")\n",
        "    stats_text.append(f\"  Initial: {val_losses[0]:.4f}\")\n",
        "    stats_text.append(f\"  Final: {val_losses[-1]:.4f}\")\n",
        "    stats_text.append(f\"  Best: {min(val_losses):.4f}\")\n",
        "    stats_text.append(f\"  Average: {np.mean(val_losses):.4f}\")\n",
        "\n",
        "stats_text.append(f\"\\nConfiguration:\")\n",
        "stats_text.append(f\"  Batch Size: {TRAINING_ARGS['per_device_train_batch_size']}\")\n",
        "stats_text.append(f\"  Gradient Accumulation: {TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
        "stats_text.append(f\"  Effective Batch Size: {TRAINING_ARGS['per_device_train_batch_size'] * TRAINING_ARGS['gradient_accumulation_steps']}\")\n",
        "stats_text.append(f\"  Learning Rate: {TRAINING_ARGS['learning_rate']}\")\n",
        "stats_text.append(f\"  FP16: {TRAINING_ARGS['fp16']}\")\n",
        "stats_text.append(f\"  Gradient Checkpointing: {TRAINING_ARGS['gradient_checkpointing']}\")\n",
        "\n",
        "stats_str = \"\\n\".join(stats_text)\n",
        "ax4.text(0.1, 0.95, stats_str, transform=ax4.transAxes, \n",
        "         fontsize=10, verticalalignment='top', family='monospace',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/training_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"Visualizations saved to: {OUTPUT_DIR}/training_visualizations.png\")\n",
        "plt.show()\n",
        "\n",
        "# Also create individual high-quality plots for report\n",
        "print(\"\\nGenerating individual plots for report...\")\n",
        "\n",
        "# Individual plot 1: Training and Validation Loss\n",
        "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "if len(train_losses) > 0:\n",
        "    train_step_values = [i * TRAINING_ARGS['logging_steps'] for i in range(len(train_losses))]\n",
        "    ax1.plot(train_step_values, train_losses, 'b-', label='Training Loss', linewidth=2.5, marker='o', markersize=4, alpha=0.8)\n",
        "if len(val_losses) > 0:\n",
        "    val_step_values = [i * TRAINING_ARGS['eval_steps'] for i in range(len(val_losses))]\n",
        "    ax1.plot(val_step_values, val_losses, 'r-', label='Validation Loss', linewidth=2.5, marker='s', markersize=4, alpha=0.8)\n",
        "ax1.set_xlabel('Training Step', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('Training and Validation Loss Over Time', fontsize=15, fontweight='bold')\n",
        "ax1.legend(fontsize=12, loc='best')\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.set_xlim(left=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/loss_plot.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"  Saved: {OUTPUT_DIR}/loss_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "# Individual plot 2: Learning Rate Schedule\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "if len(learning_rates) > 0:\n",
        "    lr_steps = [i * TRAINING_ARGS['logging_steps'] for i in range(len(learning_rates))]\n",
        "    ax2.plot(lr_steps, learning_rates, 'g-', linewidth=2.5, marker='o', markersize=4, alpha=0.8)\n",
        "else:\n",
        "    # Theoretical schedule\n",
        "    max_steps = TRAINING_ARGS['max_steps']\n",
        "    initial_lr = TRAINING_ARGS['learning_rate']\n",
        "    steps = np.arange(0, max_steps, 50)\n",
        "    lrs = [initial_lr * (1 + np.cos(np.pi * s / max_steps)) / 2 for s in steps]\n",
        "    ax2.plot(steps, lrs, 'g-', linewidth=2.5, alpha=0.8)\n",
        "ax2.set_xlabel('Training Step', fontsize=13, fontweight='bold')\n",
        "ax2.set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "ax2.set_title('Learning Rate Schedule (Cosine Annealing)', fontsize=15, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_xlim(left=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/learning_rate_plot.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"  Saved: {OUTPUT_DIR}/learning_rate_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAll visualizations generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Romanize all transcriptions\n",
        "print(\"Romanizing training data...\")\n",
        "for item in tqdm(train_data, desc=\"Train\"):\n",
        "    item['transcription_romanized'] = romanize_text(item['transcription'])\n",
        "\n",
        "print(\"Romanizing validation data...\")\n",
        "for item in tqdm(val_data, desc=\"Val\"):\n",
        "    item['transcription_romanized'] = romanize_text(item['transcription'])\n",
        "\n",
        "print(\"Romanizing test data...\")\n",
        "for item in tqdm(test_data, desc=\"Test\"):\n",
        "    item['transcription_romanized'] = romanize_text(item['transcription'])\n",
        "\n",
        "print(\"\\nRomanization complete!\")\n",
        "print(f\"Sample - Original: {train_data[0]['transcription']}\")\n",
        "print(f\"Sample - Romanized: {train_data[0]['transcription_romanized']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = VitsModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Sampling rate: {model.config.sampling_rate} Hz\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "    print(\"Model moved to CUDA\")\n",
        "    print(f\"Memory after model load: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Inference (Before Training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synthesize_speech(model, tokenizer, text_romanized, output_path):\n",
        "    \"\"\"Synthesize speech from romanized text\"\"\"\n",
        "    inputs = tokenizer(text_romanized, return_tensors=\"pt\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs).waveform\n",
        "\n",
        "    # Move tensor to CPU before converting to numpy\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        output = output.cpu()\n",
        "\n",
        "    # Save audio\n",
        "    scipy.io.wavfile.write(\n",
        "        output_path,\n",
        "        rate=model.config.sampling_rate,\n",
        "        data=output.numpy().T\n",
        "    )\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Test with sample text (after romanization)\n",
        "if len(train_data) > 0 and 'transcription_romanized' in train_data[0]:\n",
        "    test_text_romanized = train_data[0]['transcription_romanized']\n",
        "    test_output = \"test_synthesized_before_training.wav\"\n",
        "\n",
        "    print(f\"Testing synthesis with: {test_text_romanized}\")\n",
        "    synthesize_speech(model, tokenizer, test_text_romanized, test_output)\n",
        "    print(f\"Test audio saved to: {test_output}\")\n",
        "else:\n",
        "    print(\"Please run romanization cells first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Implement VITS Training Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mel_spectrogram(waveform, sample_rate=16000, n_mels=80, n_fft=1024, hop_length=256):\n",
        "    \"\"\"Compute mel-spectrogram from waveform\"\"\"\n",
        "    # Convert to numpy if tensor\n",
        "    if isinstance(waveform, torch.Tensor):\n",
        "        waveform_np = waveform.cpu().numpy()\n",
        "    else:\n",
        "        waveform_np = waveform\n",
        "    \n",
        "    # Handle multi-channel audio\n",
        "    if len(waveform_np.shape) > 1:\n",
        "        waveform_np = waveform_np[0] if waveform_np.shape[0] == 1 else waveform_np.mean(axis=0)\n",
        "    \n",
        "    # Compute mel-spectrogram using librosa\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=waveform_np,\n",
        "        sr=sample_rate,\n",
        "        n_mels=n_mels,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        fmin=0,\n",
        "        fmax=sample_rate // 2\n",
        "    )\n",
        "    \n",
        "    # Convert to log scale\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    \n",
        "    return torch.tensor(mel_spec_db, dtype=torch.float32)\n",
        "\n",
        "def vits_training_loss(predicted_waveform, target_waveform, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Compute training loss for VITS model.\n",
        "    Uses a combination of waveform L1 loss and mel-spectrogram loss.\n",
        "    \"\"\"\n",
        "    # Ensure same length (truncate or pad)\n",
        "    min_len = min(predicted_waveform.shape[-1], target_waveform.shape[-1])\n",
        "    pred = predicted_waveform[..., :min_len]\n",
        "    target = target_waveform[..., :min_len]\n",
        "    \n",
        "    # Waveform L1 loss\n",
        "    waveform_loss = F.l1_loss(pred, target)\n",
        "    \n",
        "    # Mel-spectrogram loss\n",
        "    try:\n",
        "        pred_mel = compute_mel_spectrogram(pred, sample_rate)\n",
        "        target_mel = compute_mel_spectrogram(target, sample_rate)\n",
        "        \n",
        "        # Ensure same dimensions\n",
        "        min_time = min(pred_mel.shape[-1], target_mel.shape[-1])\n",
        "        pred_mel = pred_mel[..., :min_time]\n",
        "        target_mel = target_mel[..., :min_time]\n",
        "        \n",
        "        mel_loss = F.l1_loss(pred_mel, target_mel)\n",
        "    except Exception as e:\n",
        "        # Fallback to waveform loss only if mel computation fails\n",
        "        print(f\"Warning: Mel-spectrogram computation failed: {e}\")\n",
        "        mel_loss = torch.tensor(0.0, device=waveform_loss.device)\n",
        "    \n",
        "    # Combined loss (weighted)\n",
        "    total_loss = waveform_loss + 0.5 * mel_loss\n",
        "    \n",
        "    return total_loss, waveform_loss, mel_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VITSDataset:\n",
        "    \"\"\"Dataset for VITS training\"\"\"\n",
        "    \n",
        "    def __init__(self, data_list, tokenizer, target_sr=16000, max_length=10.0):\n",
        "        self.data_list = data_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.target_sr = target_sr\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        \n",
        "        # Load and preprocess audio\n",
        "        audio_path = item['audio_path']\n",
        "        audio, sr = librosa.load(audio_path, sr=self.target_sr, duration=self.max_length)\n",
        "        \n",
        "        # Convert to tensor and ensure mono\n",
        "        if len(audio.shape) > 1:\n",
        "            audio = audio[0] if audio.shape[0] == 1 else audio.mean(axis=0)\n",
        "        \n",
        "        audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
        "        \n",
        "        # Tokenize text\n",
        "        text_romanized = item['transcription_romanized']\n",
        "        tokenized = self.tokenizer(\n",
        "            text_romanized,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
        "            'audio': audio_tensor,\n",
        "            'text': text_romanized\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VITSDataset(train_data, tokenizer, target_sr=TARGET_SAMPLE_RATE, max_length=MAX_AUDIO_LENGTH)\n",
        "val_dataset = VITSDataset(val_data, tokenizer, target_sr=TARGET_SAMPLE_RATE, max_length=MAX_AUDIO_LENGTH)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"\\nSample item keys: {list(train_dataset[0].keys())}\")\n",
        "print(f\"Sample audio shape: {train_dataset[0]['audio'].shape}\")\n",
        "print(f\"Sample input_ids shape: {train_dataset[0]['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop (Memory-Optimized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training\n",
        "model.train()\n",
        "\n",
        "# Use FP16 scaler for mixed precision\n",
        "scaler = torch.cuda.amp.GradScaler() if TRAINING_ARGS['fp16'] else None\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=TRAINING_ARGS['learning_rate'],\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=TRAINING_ARGS['max_steps']\n",
        ")\n",
        "\n",
        "# Data loaders with memory-efficient settings\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAINING_ARGS['per_device_train_batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=TRAINING_ARGS['dataloader_num_workers'],\n",
        "    pin_memory=TRAINING_ARGS['dataloader_pin_memory'],\n",
        "    drop_last=True  # Drop last incomplete batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=TRAINING_ARGS['per_device_eval_batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=TRAINING_ARGS['dataloader_num_workers'],\n",
        "    pin_memory=TRAINING_ARGS['dataloader_pin_memory']\n",
        ")\n",
        "\n",
        "print(\"Training setup complete!\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
        "print(f\"Scheduler: {type(scheduler).__name__}\")\n",
        "print(f\"FP16 scaler: {scaler is not None}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "max_steps = TRAINING_ARGS['max_steps']\n",
        "gradient_accumulation_steps = TRAINING_ARGS['gradient_accumulation_steps']\n",
        "logging_steps = TRAINING_ARGS['logging_steps']\n",
        "save_steps = TRAINING_ARGS['save_steps']\n",
        "eval_steps = TRAINING_ARGS['eval_steps']\n",
        "\n",
        "step = 0\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "accumulated_loss = 0.0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Max steps: {max_steps}\")\n",
        "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {TRAINING_ARGS['per_device_train_batch_size'] * gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {TRAINING_ARGS['learning_rate']}\")\n",
        "print()\n",
        "\n",
        "while global_step < max_steps:\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {step // len(train_loader) + 1}\")):\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        target_audio = batch['audio'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Forward pass with mixed precision\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if TRAINING_ARGS['fp16'] and scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Generate waveform from model\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                predicted_waveform = outputs.waveform\n",
        "                \n",
        "                # Compute loss\n",
        "                loss, waveform_loss, mel_loss = vits_training_loss(\n",
        "                    predicted_waveform,\n",
        "                    target_audio.unsqueeze(1),  # Add channel dimension\n",
        "                    sample_rate=TARGET_SAMPLE_RATE\n",
        "                )\n",
        "                \n",
        "                # Scale loss for gradient accumulation\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                # Gradient clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                \n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                \n",
        "                global_step += 1\n",
        "                accumulated_loss += loss.item() * gradient_accumulation_steps\n",
        "                \n",
        "                # Logging\n",
        "                if global_step % logging_steps == 0:\n",
        "                    avg_loss = accumulated_loss / logging_steps\n",
        "                    current_lr = scheduler.get_last_lr()[0]\n",
        "                    print(f\"\\nStep {global_step}/{max_steps}\")\n",
        "                    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "                    print(f\"  Learning rate: {current_lr:.2e}\")\n",
        "                    print(f\"  GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "                    train_losses.append(avg_loss)\n",
        "                    accumulated_loss = 0.0\n",
        "                \n",
        "                # Evaluation\n",
        "                if global_step % eval_steps == 0:\n",
        "                    model.eval()\n",
        "                    val_loss = 0.0\n",
        "                    val_batches = 0\n",
        "                    \n",
        "                    with torch.no_grad():\n",
        "                        for val_batch in val_loader:\n",
        "                            val_input_ids = val_batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                            val_attention_mask = val_batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                            val_target_audio = val_batch['audio'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "                            \n",
        "                            with torch.cuda.amp.autocast():\n",
        "                                val_outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask)\n",
        "                                val_predicted = val_outputs.waveform\n",
        "                                val_loss_batch, _, _ = vits_training_loss(\n",
        "                                    val_predicted,\n",
        "                                    val_target_audio.unsqueeze(1),\n",
        "                                    sample_rate=TARGET_SAMPLE_RATE\n",
        "                                )\n",
        "                                val_loss += val_loss_batch.item()\n",
        "                                val_batches += 1\n",
        "                    \n",
        "                    avg_val_loss = val_loss / val_batches if val_batches > 0 else 0.0\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
        "                    \n",
        "                    # Save best model\n",
        "                    if avg_val_loss < best_val_loss:\n",
        "                        best_val_loss = avg_val_loss\n",
        "                        checkpoint_path = f\"{OUTPUT_DIR}/checkpoint-{global_step}\"\n",
        "                        os.makedirs(checkpoint_path, exist_ok=True)\n",
        "                        model.save_pretrained(checkpoint_path)\n",
        "                        tokenizer.save_pretrained(checkpoint_path)\n",
        "                        print(f\"  Saved best model to {checkpoint_path}\")\n",
        "                    \n",
        "                    model.train()\n",
        "                \n",
        "                # Save checkpoint\n",
        "                if global_step % save_steps == 0:\n",
        "                    checkpoint_path = f\"{OUTPUT_DIR}/checkpoint-{global_step}\"\n",
        "                    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "                    model.save_pretrained(checkpoint_path)\n",
        "                    tokenizer.save_pretrained(checkpoint_path)\n",
        "                    print(f\"  Saved checkpoint to {checkpoint_path}\")\n",
        "                \n",
        "                if global_step >= max_steps:\n",
        "                    break\n",
        "        else:\n",
        "            # FP32 training (fallback)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predicted_waveform = outputs.waveform\n",
        "            \n",
        "            loss, waveform_loss, mel_loss = vits_training_loss(\n",
        "                predicted_waveform,\n",
        "                target_audio.unsqueeze(1),\n",
        "                sample_rate=TARGET_SAMPLE_RATE\n",
        "            )\n",
        "            \n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            \n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                \n",
        "                global_step += 1\n",
        "                accumulated_loss += loss.item() * gradient_accumulation_steps\n",
        "                \n",
        "                if global_step % logging_steps == 0:\n",
        "                    avg_loss = accumulated_loss / logging_steps\n",
        "                    print(f\"\\nStep {global_step}/{max_steps} - Loss: {avg_loss:.4f}\")\n",
        "                    train_losses.append(avg_loss)\n",
        "                    accumulated_loss = 0.0\n",
        "                \n",
        "                if global_step >= max_steps:\n",
        "                    break\n",
        "    \n",
        "    if global_step >= max_steps:\n",
        "        break\n",
        "    \n",
        "    step += 1\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
        "print(f\"\\nSaving final model to: {final_model_path}\")\n",
        "os.makedirs(final_model_path, exist_ok=True)\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"Final model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Zip Model and Copy to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Zip the final model directory\n",
        "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
        "zip_filename = f\"{final_model_path}.zip\"\n",
        "\n",
        "print(f\"Creating zip file: {zip_filename}...\")\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
        "        if file_path.is_file():\n",
        "            arcname = file_path.relative_to(final_model_path)\n",
        "            zipf.write(file_path, arcname)\n",
        "            print(f\"  Added: {arcname}\")\n",
        "\n",
        "print(f\"\\nZip file created: {zip_filename}\")\n",
        "\n",
        "# Copy to Google Drive\n",
        "drive_dest = f\"/content/drive/MyDrive/{zip_filename}\"\n",
        "shutil.copy2(zip_filename, drive_dest)\n",
        "\n",
        "print(f\"Model zip file copied to Google Drive: {drive_dest}\")\n",
        "print(f\"\\nFile size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Test Inference (After Training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model for testing\n",
        "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
        "\n",
        "print(f\"Loading fine-tuned model from: {final_model_path}\")\n",
        "test_model = VitsModel.from_pretrained(final_model_path)\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    test_model = test_model.to(\"cuda\")\n",
        "    test_model.eval()\n",
        "\n",
        "# Test with sample text\n",
        "if len(test_data) > 0 and 'transcription_romanized' in test_data[0]:\n",
        "    test_text_romanized = test_data[0]['transcription_romanized']\n",
        "    test_output = \"test_synthesized_after_training.wav\"\n",
        "\n",
        "    print(f\"\\nTesting synthesis with: {test_text_romanized}\")\n",
        "    synthesize_speech(test_model, test_tokenizer, test_text_romanized, test_output)\n",
        "    print(f\"Test audio saved to: {test_output}\")\n",
        "else:\n",
        "    print(\"Please run romanization cells first!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
